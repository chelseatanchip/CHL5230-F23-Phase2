author,title,abstract,discussion,link
Lebret 2023,Allocating organs through algorithms and equitable access to transplantation—a European human rights law approach,"Digitization in transplantation is not a new phenomenon. Algorithms are being used, for example, to allocate organs based on medical compatibility and priority criteria. However, digitization is accelerating as computer scientists and physicians increasingly develop and use machine learning (ML) models to obtain better predictions on the chances of a successful transplant. The objective of the article is to shed light on the potential threats to equitable access to organs allocated through algorithms, whether these are the consequence of political choices made upstream of digitization or of the algorithmic design, or are produced by self-learning algorithms. The article shows that achieving equitable access requires an overall vision of the algorithmic development process and that European legal norms only partially contribute to preventing harm and addressing equality in access to organs.","The article has shown some of the risks of unfair outcomes in the design and use of algorithms for organ transplantation and the interplay with the European legal definition of equitable access to health resources. Such threats are rarely born with digitization, but digitization can amplify and systematize inequitable outputs.

The article completed more general research on algorithmic discrimination by concretely examining the difficulties to apply non-discrimination standards since on the one hand, current categories of discrimination do not fully reflect algorithmic reality and, on the other hand, access to biomedical resources inevitably rests on a balance of interests and values. Algorithms may perpetuate or produce disadvantageous effects for certain groups of people, but the associated risk of indirect discrimination will almost always meet the test of necessity and proportionality and is unlikely to exceed the wide state margin of appreciation in the allocation of scarce resources. In addition to the opacity of algorithmic systems, the lack of uniform approach in European legal instruments and jurisprudence may affect legal certainty and make litigation more complex.

In this regard, the clarification and adaptation of European non-discrimination standards would be desirable.197 Other frameworks such as privacy, briefly mentioned in this paper, can enrich the interpretation of fairness in law and contribute to the reaching of a more equitable access to organ transplantation. Although the EU Proposal of AI Act addresses certain of these issues, it is part of several EU initiatives based on market issues198 and should be complemented by an appropriate human rights-based instrument. The project to develop a proper international convention on AI and human rights at the level of the Council of Europe may fill the gap: it is the occasion to adopt a dedicated instrument and adjust the definition of equality and non-discrimination to the algorithmic context.199

The article also emphasized the importance of looking at the use of algorithms in organ transplantation in their context, rather than only focusing on ML discriminatory outputs. The above-mentioned threats to equitable access may emerge at different stages and involve different actors. Addressing such risks requires a preventive approach and will not be solved at the algorithmic level only. It will involve the development of a chain of duties, including long-term structural obligations and concrete immediate actions. International and European Human Rights Law, including the UN Guiding principles on the responsibilities of transnational corporations, can provide an internationally coherent framework to share and define responsibilities from the planning, to the development and selling/buying of algorithms.200 The future AI Act, which will apply to providers and users of medical devices in the EU,201 should fit into this transnational procedural framework. Its preventive approach can complete the redress-based approach of European non-discrimination law.

It seems important to delineate the scope of state obligations and the need of developers’ due diligence in order to achieve an equitable access to transplantation. In particular, due diligence involves human-rights impact assessments by private sector actors developing algorithms to be used for organ transplantation.202 States have obligations to identify and address barriers to equitable access to organ transplantation and can achieve this objective with the help of algorithms. In the ECtHR’ typology, such positive obligations are of a substantive and procedural nature. They may be of a structural nature.203

First, States need to encourage upstream medicine in order to assess the social determinants of health that may affect organ transplantation. Although the article principally discussed organ allocation algorithms, numerous persons in need of a transplant are not registered on national waiting lists. States need to address the reasons for the disparity in access to the waiting lists in the first place, the death rate while on a waiting list, and the disparities in access to organ transplants in a later phase. Algorithms may help to address the socio-economic determinants of health in the case of transplantation, by assessing risks of developing disease and need for transplant, looking at different communities’ living circumstances. As stated above, a gender perspective should be integrated by the disaggregation of health and socio-economic data according to sex.204

Second, and on the basis of the above, States should address disparities by determining the scope of positive action as normative constraints playing on the allocation algorithms while avoiding the creation of new imbalances. An example is the issue of algorithmic compensation to address disparities in access. Although non-discrimination law is not providing clear answers on the scope and extent of positive measures in that regard, a health and human rights suggests to further down what established policies already do in the allocation of organs from deceased donors. Such type of positive measures or ‘special measures’ to address inequalities have the advantage of being transparent and can be temporary as required by European law; the time for more structural reforms to produce effect. Indeed, adjustments at the allocation stage need to be constantly revaluated in order not to result in new inequalities, and a fair balance should be reached in addressing different sources of vulnerability. Besides, and as already mentioned, redistribution resulting in algorithmic compensation needs to account for studies on social determinants explaining disparities in access in a broader context.205 Regarding the use of the planning and development of ML, States should facilitate the collection and use of appropriate data by adapting their legislation to achieve equivalent quality predictions between patients. The EU AI Act will partly contribute to the achievement of this mission since it will be directly applicable in the EU Member States. Since developing ML with new datasets is costly, there is a compelling interest in ensuring the inclusiveness and appropriateness of data itself and collection methods in the first place. In the meantime, mitigating biases can limit disadvantage on certain groups.

Third, States need to foster education on ethics and human rights for data scientists as well as in medical schools.206 Interdisciplinary dialogue must be strengthened in order to effectively prevent the risk of potentially discriminatory algorithmic outputs.",https://academic.oup.com/jlb/article/10/1/lsad004/7097871
Zarrinpar et al. 2016,Individualizing liver transplant immunosuppression using a phenotypic personalized medicine platform,"After organ transplant, patients are on a merry-go-round of medicines and procedures to make sure that the graft is not rejected. Currently, physicians use dosing guidelines for drugs meant to suppress the immune system, but also use educated guesses in choosing dose, to account for variability in patient response to the drugs and drug-drug interactions. Now, Zarrinpar and colleagues have come up with a mathematical approach to remove the guesswork. Their approach, called parabolic personalized dosing (PPD), relies on algebraic equations to relate phenotype (in this case, trough level of an immunosuppressant, tacrolimus) to input (tacrolimus concentration). By mapping patient response over the course of treatment, the equation produces a two-dimensional (2D) parabola that indicates the next dose that the patient should receive. The parabola shifts as drugs are added or taken away, or as the patient undergoes additional clinical procedures, such as hemodialysis, which can interfere with drug distribution within the body. The PPD approach was tested in four patients and compared to the standard of care, physician guidance. The PPD patients were out of trough range less frequently and for shorter periods of time than controls, suggesting that the equation was predicting next doses accurately. Future studies will involve more patients and will expand the PPD equation to represent a 3D parabolic surface, which will factor in drug combinations. The PPD approach will have broad applicability beyond transplant medicine, because it is independent of disease mechanism or drug of choice and could thus personalize regimens for many types of patients.","Transplant patients undergo combination therapy with a substantial number of drugs and procedures. This regimen is changed constantly to account for infection, inflammation, rejection, and kidney function, among other factors, and patients respond uniquely to their constantly changing regimens. We therefore developed a clinical approach to personalizing drug dosing and demonstrated proof of concept in eight patients in a prospective randomized controlled study administering tacrolimus for posttransplant liver immunosuppression. Clinical data generated by standard-of-care dosing during the initial treatment period were used to calibrate patient-specific coefficients to construct a parabolic map, called PPD, to guide the immunosuppression-dosing process. Patients treated using PPD had significantly less variability in tacrolimus trough levels compared with control patients with physician-guided standard-of-care dosing. In addition, retrospective PPD for control patients indicated the possibility of better maintaining trough levels within the target range compared to the standard of care that patients received. Regardless of the dosing approach, all eight patients revealed intra- and interpatient variability as a result of drug-drug interactions and procedures, further supporting the need for such phenotypic platforms in personalized medicine.
Our PPD approach is broadly applicable, as we have demonstrated previously in vitro and in vivo in animals (20–25). This clinical study has shown that this parabolic surface represents patient phenotypic responses to monotherapy and combination therapies well, serving as a powerful foundation for expanding the PPD process toward other indications. The PPD 3D drug interaction map may provide further insight into the effects of patient-specific drug additivity, synergism, or antagonism on drug levels to assist with clinical decision-making. Notably, PPD is not a pharmacogenomic or pharmacokinetic predictive modeling approach. Instead, PPD uses phenotypic outputs such as clinical efficacy and/or safety to plot the parabolic surface. The phenotype innately accounts for molecular and pharmacokinetic determinants, serving as the foundation for the disease mechanism–independent and indication-agnostic nature of PPD. This also differentiates PPD from systems biology approaches (26–28).
With PPD, we can visualize the phenotypic effects of drug-drug interactions and procedural changes on trough levels. For example, hemodialysis alters total body water and can therefore affect tacrolimus redistribution during and after dialysis (29). There is also an unpredictable correlation between MMF dose and tacrolimus levels (change in absorption levels and in intestinal transit time). Both tacrolimus and MMF absorption and metabolism are affected by cytochrome P450 and P-glycoprotein. Such interactions could be further evaluated in humans using PPD to optimize dosing regimens. This pilot study guided tacrolimus dosing only; therefore, a recalibration process addressed regimen changes to maintain trough levels within target ranges. Although the recalibration process managed trough levels, retrospective PPD modulated multidrug dosing to eliminate recalibration altogether (which would reduce the incidence of target range deviations). Specifically, when a regimen change—change in drug dose, hemodialysis, etc.—is anticipated, deviations from target ranges are typically imminent because modifying tacrolimus dosing alone is not sufficient to account for changes in dosages of other drugs or the introduction of additional drugs into the regimen.
This preliminary study was based on four patients, where mean analysis of the outcomes was not statistically significant but the reduction of interpatient variance was statistically significant. Continued PPD scale-up for widespread clinical application should focus on enhancing preemptive management of trough levels or novel immunosuppression markers with personalized multidrug dosing. Scale-up would also include integrating PPD with outpatient immunosuppression to assess long-term patient response, because high levels of calcineurin trough level variability adversely affect graft outcomes (30). For this initial validation of PPD, we chose to limit our study to the inpatient setting to facilitate data collection, to ensure that the patients are receiving the drugs that they are prescribed, and to allow for the incorporation of as broad a range of clinical data as possible.
This study also raises the question of whether a second-order polynomial is more effective in dosage guidance compared to a line. The clinical drug titration standard is based on the linear approach. This is evident for patient C1, where for most of the treatment period, the tacrolimus trough level was not in the target range; with the parabolic response surface, as shown by retrospective PPD optimization, the target trough level would have been reached much more quickly. The parabola may also intersect the tacrolimus target range at two points to identify two possible dose suggestions; a line cannot accomplish this and therefore may prevent optimized dosing. The linear approach is, at best, an approximation of one side of a parabola, whereas Eq. 2 (PPD) shows that the efficacy-dose response surface is inherently parabolic (20–25).
The current clinical practice is to personalize immunosuppression for each recipient by setting the tacrolimus target range according to the clinical scenario. An alternative indicator could be CD4+ T cell activation, which the Cylex ImmuKnow assay was designed to measure. This assay was never embraced clinically, because its utility was never definitively demonstrated (31, 32). Therefore, in the absence of a clinically useful and validated measure of immunosuppression, the tacrolimus trough level has become the standard. Factors such as kidney function, comorbidities, race, and disease severity are therefore taken into account in this determination. However, PPD could be adapted to any novel indicator of immunosuppression that may be more effective than trough levels (for instance, T cell alloreactivity or donor-specific anti–human leukocyte antigen antibodies) (33, 34).
The PPD platform implemented in this study is markedly different from the clinical standard of care that relies on titration or incremental dosing using educated guesses. PPD has thus far not been automated so that the clinician is given the final say in approving dosing orders, to minimize patient risk. PPD is embedded with upper and lower dosing limits to prevent over- and underdosing, so automation is possible depending on the indication. PPD implementation could be completed within minutes, allowing one person to manage many patients. PPD can also be implemented in an outpatient setting where tacrolimus levels are recorded every few days, and dosing prescriptions can be given to patients through their outpatient care provider. Therefore, PPD could maximize patient benefit and turnaround time, as well as financial considerations, such as reimbursement, associated with reducing treatment complications and duration of postoperative hospitalization. In sum, this parameter was used to demonstrate that PPD implementation did not result in apparent adverse events, complications in administrating PPD, or other barriers that required prolonged hospitalization. Our preliminary clinical study of a phenotypic medicine approach will serve as a foundation for the expansion of PPD toward other disease indications, such as cancer, infectious diseases, and cardiovascular disorders, where dosing could be better controlled and personalized.",https://www.science.org/doi/full/10.1126/scitranslmed.aac5954?casa_token=uV0NMAfJmj8AAAAA%3AVmy8aqONcmLfF_vT4efZkVY5cSc5HOajfKad7lYSyhtceJm_TNp482YXcoXNxN6CGk1esrCUYneP
Ding et al. 2023,Fairly Predicting Graft Failure in Liver Transplant for Organ Assigning,"Liver transplant is an essential therapy performed for severe liver diseases. The fact of scarce liver resources makes
the organ assigning crucial. Model for End-stage Liver Disease (MELD) score is a widely adopted criterion when
making organ distribution decisions. However, it ignores post-transplant outcomes and organ/donor features. These
limitations motivate the emergence of machine learning (ML) models. Unfortunately, ML models could be unfair and
trigger bias against certain groups of people. To tackle this problem, this work proposes a fair machine learning
framework targeting graft failure prediction in liver transplant. Specifically, knowledge distillation is employed to
handle dense and sparse features by combining the advantages of tree models and neural networks. A two-step
debiasing method is tailored for this framework to enhance fairness. Experiments are conducted to analyze unfairness
issues in existing models and demonstrate the superiority of our method in both prediction and fairness performance.","We conduct experiments to compare the prediction and fairness performance of the proposed method with multiple
baseline methods (Table 3). The key observation is that the proposed model can provide competitive prediction
performance with less bias across subgroups.
Compared with the MELD score, we observe that machine learning models show much stronger prediction capability
of graft failure. The poor graft failure prediction performance of MELD score aligns with the weak correlations
between MELD score and graft failure rate from statistic analysis in Table 2. The tree model has better and less biased
prediction performance than linear model. This may be caused by the tree model’s internal selection of features, which
could implicitly omit some features with bias.
Compared with baseline machine learning methods, when the sensitive attribute is race, the proposed method can
significantly debias the prediction with only 2.1% decreases of ROC AUC, while the two fairness metrics decrease by
5.5% averagely. As for gender, the ROC AUC decreases only 2.0%, however, the two fairness metrics decrease by
32.2% on average. Recall that the parity loss we applied is based on the demographic parity. In Table 3, we observe
improvement not only on DPD but also on EOD. This can validate the effectiveness of our debiasing method, which
can generally mitigate the unfairness issues. To validate the effectiveness of our two-step debiasing strategy, we conduct ablation study to investigate the contribution of each component. From Table 3, we observe that by only adding the debiasing method in knowledge distillation
step (first step), the proposed model can only improve the DPD metrics. When only debiasing the end-to-end training
step, both fairness metrics improve to some extent. The model achieves the best debiasing performance when the
two debiasing steps are combined. This is because the knowledge-distilled embedding and end-to-end training are
interleaved, which verifies the necessity of the two-step debiasing strategy.",https://arxiv.org/pdf/2302.09400.pdf
Delen et al. 2010,A machine learning-based approach to prognostic analysis of thoracic transplantations,"Objective
The prediction of survival time after organ transplantations and prognosis analysis of different risk groups of transplant patients are not only clinically important but also technically challenging. The current studies, which are mostly linear modeling-based statistical analyses, have focused on small sets of disparate predictive factors where many potentially important variables are neglected in their analyses. Data mining methods, such as machine learning-based approaches, are capable of providing an effective way of overcoming these limitations by utilizing sufficiently large data sets with many predictive factors to identify not only linear associations but also highly complex, non-linear relationships. Therefore, this study is aimed at exploring risk groups of thoracic recipients through machine learning-based methods.

Methods and material
A large, feature-rich, nation-wide thoracic transplantation dataset (obtained from the United Network for Organ Sharing—UNOS) is used to develop predictive models for the survival time estimation. The predictive factors that are most relevant to the survival time identified via, (1) conducting sensitivity analysis on models developed by the machine learning methods, (2) extraction of variables from the published literature, and (3) eliciting variables from the medical experts and other domain specific knowledge bases. A unified set of predictors is then used to develop a Cox regression model and the related prognosis indices. A comparison of clustering algorithm-based and conventional risk grouping techniques is conducted based on the outcome of the Cox regression model in order to identify optimal number of risk groups of thoracic recipients. Finally, the Kaplan–Meier survival analysis is performed to validate the discrimination among the identified various risk groups.

Results
The machine learning models performed very effectively in predicting the survival time: the support vector machine model with a radial basis Kernel function produced the best fit with an R2 value of 0.879, the artificial neural network (multilayer perceptron-MLP-model) came the second with an R2 value of 0.847, and the M5 algorithm-based regression tree model came last with an R2 value of 0.785. Following the proposed method, a consolidated set of predictive variables are determined and used to build the Cox survival model. Using the prognosis indices revealed by the Cox survival model along with a k-means clustering algorithm, an optimal number of “three” risk groups is identified. The significance of differences among these risk groups are also validated using the Kaplan–Meier survival analysis.

Conclusions
This study demonstrated that the integrated machine learning method to select the predictor variables is more effective in developing the Cox survival models than the traditional methods commonly found in the literature. The significant distinction among the risk groups of thoracic patients also validates the effectiveness of the methodology proposed herein. We anticipate that this study (and other AI based analytic studies like this one) will lead to more effective analyses of thoracic transplant procedures to better understand the prognosis of thoracic organ recipients. It would potentially lead to new medical and biological advances and more effective allocation policies in the field of organ transplantation.","This study demonstrates that machine learning-based methodology for selecting predictor variables in survivability and prognostic modeling of thoracic organ transplantation is superior to the approaches adopting only expert-selected variables. The study showed that of the comprehensive list of predictors, some have been included in the previous studies (such as gender and age of the recipient, his/her medical condition at registration) while some others (which are found to be critical) have been absent from the related literature. These variables (e.g. such as recipient length of stay post-transplant and the interaction of gender and ethnicity between the recipient and the donor) should be combined with the factors identified in previous studies to better understand and improve the organ transplantation process.

The study revealed that based on k-means clustering algorithm the thoracic organ recipients should be allocated into an optimal number of “three” risk groups, namely low, medium, and high. This finding confirms the conventional medical discrimination commonly used in this field of study. However, it also proves that this grouping should be better performed through a data mining perspective rather than a heuristics-based approach because the latter one gives more skewed distribution of patients for our US nation-wide dataset. This is the point where the medical professionals should be advised to handle the problem in the future.

Some of the research extensions to the study reported in this article includes analysis of other organ types as well as the analysis of multiorgan scenarios where the correlations among the organs coming from the same donor are also included in the formulation of the problem. Another potential further research direction of this study is to validate the patterns obtained from the data mining models with a comprehensive simulation model of the organ transplantation process. Using actual cases, a comprehensive discrete-event simulation model can be developed and used as a test-bed where the potential benefits and limitations of these novel patterns are tested and validated for a sufficiently long period of time in the computer simulation environment.",https://www.sciencedirect.com/science/article/pii/S0933365710000035
Yoon et al. 2017,Personalized Donor-Recipient Matching for Organ Transplantation,"Organ transplants can improve the life expectancy and quality of life for the recipient but carry the risk of serious postoperative complications, such as septic shock and organ rejection. The probability of a successful transplant depends in
a very subtle fashion on compatibility between the donor and
the recipient - but current medical practice is short of domain knowledge regarding the complex nature of recipientdonor compatibility. Hence a data-driven approach for learning compatibility has the potential for significant improvements in match quality. This paper proposes a novel system
(ConfidentMatch) that is trained using data from electronic
health records. ConfidentMatch predicts the success of an organ transplant (in terms of the 3-year survival rates) on the
basis of clinical and demographic traits of the donor and
recipient. ConfidentMatch captures the heterogeneity of the
donor and recipient traits by optimally dividing the feature
space into clusters and constructing different optimal predictive models to each cluster. The system controls the complexity of the learned predictive model in a way that allows for
assuring more granular and accurate predictions for a larger
number of potential recipient-donor pairs, thereby ensuring
that predictions are “personalized” and tailored to individual characteristics to the finest possible granularity. Experiments conducted on the UNOS heart transplant dataset show
the superiority of the prognostic value of ConfidentMatch to
other competing benchmarks; ConfidentMatch can provide
predictions of success with 95% accuracy for 5,489 patients
of a total population of 9,620 patients, which corresponds to
410 more patients than the most competitive benchmark algorithm (DeepBoost).","Experiments were conducted using the UNOS database for
patients who underwent a heart transplant over the years
from 1987 to 2015 (Cecka 1996). We use the “Thoracic DATA” dataset in the UNOS database as our root dataset.
In this dataset, all patients were followed-up until death, i.e.
the post-transplant survival times for all patients are available in the dataset. Of the 148,512 patients in the “Thoracic
DATA” who underwent either heart or lung transplant, we
extract 60,516 patients who underwent a heart transplant.
Of the 60,516 patients who underwent a heart transplant,
we exclude 3,800 patients (6.28%) who are still alive (rightcensored), and we only use the 56,716 patients for whom we
have the exact survival (lifetime) information.
For each patient in the dataset, a total of 504 features are
provided; these include a combination of both the patient’s
and the donor’s information. We discard 12 features that are
normally obtained after the transplant. Of the remaining 492
features, we extract 70 features for which we have less than
10% missing information in order to reduce the noise of imputation. We use the k-nearest-neighbor (KNN) imputation
method to impute the missing data (Hastie 1999).
We compared the performance of ConfidentMatch in predicting the success of transplants with the following benchmark algorithms: logistic regression (Logit), Lasso regularized logistic regression (Lasso), decision tree (DTree), Random Forest (RForest), AdaBoost (ABoost), and DeepBoost
(DBoost). We use the correlation feature selection (CFS)
method to discover the relevant features for the predictive models of both ConfidentMatch and benchmarks (Hall
1999). The validation set calibrated all parameters of ConfidentMatch (α) and the benchmark algorithms. We adopt
the following metric for quantifying the performance of the
different algorithms. The transplant’s success probability is
quantified via the 3-year post-transplant survival rate (longterm survival rate). We say that an algorithm provides a prediction for the transplant’s success (probability of 3-year
post-transplant survival) for a certain recipient-donor pair
with an accuracy level X% if the algorithm’s probability of
correct prediction is X% for that recipient-donor pair. Based
on this definition, we define the gain of ConfidentMatch at
an accuracy level of X% as the number of recipient-donor
instances in the testing dataset for which ConfidentMatch
provides an accurate prediction of the transplant success,
whereas the best-competing benchmark does not.
We split the dataset into a training/validation set comprising the recipient-donor instances in which the recipient
underwent the transplant before the year 2010 (former patients), and a testing set that comprises recipients who underwent the transplant after 2010 (current patients). Of the
56,716 recipient-donor pairs, 37,677 pairs (66.43%) were
used for training, 9,419 pairs (16.61%) were used for validating and 9,620 pairs (16.96%) were used for testing. We
varied the accuracy level from 80% to 95% and evaluated
the performance within this range of the accuracy levels. The
execution time of the ConfidentMatch on this dataset is less
than 5 hours on MATLAB R2015a with Intel i5 (1.5GHz)
processor with 4 GB RAM.
Table 2 and Fig 2 show that ConfidentMatch consistently outperforms the benchmarks to predict the success of
heart transplant regarding 3-year mortality prediction. For
instance, ConfidentMatch boosts the number of recipientdonor pairs who get 95% accurate predictions by 410 as compared to the best performing benchmark (DBoost); this
means that ConfidentMatch can allow an additional number
of 410 recipient-donor pairs to make better pre-transplant
decisions such as whether or not the transplant should be
conducted, and whether the recipient should be matched
with another donor.
The performance gains achieved by ConfidentMatch can
be attributed to the improved phenotypic characterization
of the recipient-donor pairs that the algorithm achieves by stratifying the recipient-donor feature space. The fine and
granular phenotypic characterization achieved by ConfidentMatch is restricted by the size of the training data; the more
recipient-donor instances are available in the training set, the
larger is the number of partitions that ConfidentMatch can
construct and cast a specialized predictive model. Fig 3 illustrates the trade-off associated with increasing the complexity of ConfidentMatch’s predictive model by increasing the
number of partitions; if the number of partitions increases,
the gain of ConfidentMatch also increases as it copes with
the underlying complexity of the recipient-donor compatibility patterns, until a certain number of partitions when the
gain starts to decrease due to over-fitting.
ConfidentMatch does not only improve the quality of
prognosis, but can also draw clinical insights on the patterns
of recipient-donor compatibility. To illustrate this, we list the
first two partitions through which ConfidentMatch stratifies
the recipient feature space: ConfidentMatch forms two recipient groups, group A comprises patients whose length of
stay in status 1A (urgent transplant wait-list) is shorter than
10 days, whereas group B comprises the remaining patients. Table 1 lists the most relevant features that are predictive
of the transplant outcome for each group. It can be seen
that group B patients are more sensitive to the donor characteristics; the donor’s Venereal disease research laboratory
(VDRL) result and blood type are relevant to the transplant
outcome, whereas group A patients appear to be less sensitive to these features. Thus, the more the patient waits in status 1A, the more it becomes essential to consider the extent
of her compatibility with the donors. As more training data
becomes available, ConfidentMatch can reveal finer partitions and identify the relevant features for more granular recipient subgroups.",https://ojs.aaai.org/index.php/AAAI/article/view/10711
Berrevoets et al. 2020,OrganITE: Optimal transplant donor organ offering using an individual treatment effect,"Transplant-organs are a scarce medical resource. The uniqueness of each organ and the patients' heterogeneous responses to the organs present a unique and challenging machine learning problem. In this problem there are two key challenges: (i) assigning each organ ""optimally"" to a patient in the queue; (ii) accurately estimating the potential outcomes associated with each patient and each possible organ. In this paper, we introduce OrganITE, an organ-to-patient assignment methodology that assigns organs based not only on its own estimates of the potential outcomes but also on organ scarcity. By modelling and accounting for organ scarcity we significantly increase total life years across the population, compared to the existing greedy approaches that simply optimise life years for the current organ available. Moreover, we propose an individualised treatment effect model capable of addressing the high dimensionality of the organ space. We test our method on real and simulated data, resulting in as much as an additional year of life expectancy as compared to existing organ-to-patient policies.","It is very clear that machine learning has the potential to transform healthcare. Its success both in
other domains and already within healthcare is very promising. OrganITE has the potential to extend
lives. However, as with almost any other machine learning method, there are risks associated with its
deployment in a real healthcare setting. We would stress that to mitigate these risks, we envisage
OrganITE as a decision support tool, with the clinicians and their patients making the final decision
on any suggested organ-recipient pairing.
A match made in high dimensions. A persistent problem in all transplantation programs is the
shortage of suitable donor organs of appropriate quality, resulting in a significant waiting list mortality.
That issue is compounded by the importance of matching high-dimensional donor characteristics to
high-dimensional recipient characteristics to achieve the best outcome for that donor and recipient
pair. Outcome after transplantation is dependent on a wide range of factors including the cause of the
disease, clinical status, and laboratory parameters of the recipient (describing how sick they are) as
well as characteristics related to the donor quality including age, body mass index, donor diabetes,
and other factors. The interaction of all these parameters is complex and makes the problem ripe for
a machine learning driven solution [52].
Machine learning can fail. It is very important to note that machine learning models can fail, and
OrganITE is no exception. In Section 3 we outline two assumptions that need to hold to make correct
estimations of the potential outcomes used in OrganITE: overlap, and unconfoundedness. Failing to
satisfy these assumptions can lead to OrganITE failing to learn the necessary individualised treatment
effect, and thus assigning an organ sub-optimally with respect to its objective, resulting in the ""proper""
recipient potentially failing to receive an organ they should have. This has the very serious potential
for mortality that would/could have otherwise been avoided. Having domain knowledge of the data is
therefore crucially important [3, 53, 28] to ensure it satisfies the assumptions.
The past is not the present. Learning from historical data (the data used in the experiments of this
paper spanned 26 years), presents further possible concerns. Surgical techniques, immunosuppressive
protocols, allocation and management policies have all materially changed over the years. Biases
will present differently over time. Historical data will not only reflect biases produced by medical
9
policy but also social policy, which has implications regarding the fairness of the learned model with
respect to previously disadvantaged subgroups of the population - if access to healthcare in the past
was difficult for certain subgroups, they may not be well represented in the data, and OrganITE may
fail to learn their potential outcomes well.
A population of equals. Agreeing on the best objective in organ-transplantation is incredibly
difficult [54]. The total life-years objective used here puts equal value in each year of each person’s
life. On the surface this may seem ""fair"", but the moral question of whether, for example, 2 people
surviving for 10 additional years is better than 1 surviving for 21 additional years is deep and one
that society needs to answer for itself. Does the answer change if you learn that one of the people
involved is 85 while the other is 15? OrganITE, and moreover this paper, does not attempt to
answer this question, nor is it tied to total life-years. OrganITE can be adapted to other objectives
straightforwardly by replacing the value function associated with total life-years with another that
places different weighting on different recipients’ lives.
There are several different organ allocation schemes that have been considered in health systems
[55–57], where the issue of the definition of equitable allocation continues to be debated. Factors
such as equity concerning aetiology of organ disease [58], demographic, and deprivation status need
to be considered [59], as well as geography and distance from a transplant centre [60, 61]. Whilst
methodologies such as OrganITE, including its emphasis on future donor organ density, may show
theoretic benefits, ensuring equity of access in those parameters will be important and will need to be
considered before actual implementation.
The National Liver Offering Scheme, based on net life-years gained, transplant benefit, currently used
in the UK, was introduced in March 2018 after a transplant benefit score was shown to maximise
population life years in a simulation comparing transplant benefit with prior unit based allocation,
a sickest first policy or a utility policy [3]. Transplant benefit policies, as compared to a simple
needs-based policy also have some ethical benefits [62] as they attempt to balance both justice
(prioritizing high need) and utility. Although concerns have been raised about the imprecision in
calculating net benefit using Cox models, which was acknowledged by Schaubel et al. [54], our
results demonstrates OrganITE’s superiority compared to other policies in our simulations.",https://proceedings.neurips.cc/paper/2020/file/e7c573c14a09b84f6b7782ce3965f335-Paper.pdf
Bertsimas et al. 2020,Balancing Efficiency and Fairness in Liver Transplant Access: Tradeoff Curves for the Assessment of Organ Distribution Policies,"Background. 
Current distribution policies have resulted in persistent geographic disparity in access to donated livers across the country for waitlisted candidates.

Methods. 
Using mathematical optimization,
Using mathematical optimization, and subsequently the Liver Simulation Allocation Model, the following organ distribution concepts were assessed: (1) current policy, (2) proposed alternative models, and (3) a novel continuous distribution model. A number of different scenarios for each policy distribution concept were generated and analyzed through efficiency-fairness tradeoff curves.

Results. 
The continuous distribution concept allowed both for the greatest reduction in patient deaths and for the most equitable geographic distribution across comparable organ transportation burden. When applied with an Optimized Prediction of Mortality allocation scheme, continuous distribution allowed for a significant reduction in number of deaths—on the order of 500 lives saved annually (https://livervis.github.io/).

Conclusions. 
Tradeoff curves allow for a visualized understanding on the efficiency/fairness balance, and have demonstrated that liver candidates awaiting transplant would benefit from a model employing continuous distribution as this holds the greatest advantage for mortality reduction. Development and implementation of continuous distribution models for all solid organ transplants may allow for minimization of the geographic disparity in organ distribution, and allow for efficient and fair access to a limited national resource for all candidates.","To promote access and efficiency, the Final Rule mandated that organ allocation not to be based on the transplant candidate’s place of residence or listing—except as required by sound medical judgment and best use of donated organs to avoid wasting organs and futile transplants.1 Yet, a geographic disparity in access to organs exists. For liver transplantation, this persistent discrepancy has manifested in differences in median MELD scores at transplant, rates of waitlist mortality, and ultimately has resulted in differential patterns of clinical practice by transplant professionals and candidates awaiting transplantation. The use of living-donor liver transplantation, a high-risk surgical endeavor, has largely been relegated to those areas of the country where the discrepancy between supply and demand is the greatest.11 In addition, those socioeconomically privileged candidates who can travel, choose to do so by “migrating” to areas with lower MELD scores at transplant in order to achieve liver transplant in an expedited fashion, and subsequently return with their new liver graft to their home.12

The liver transplant community has remained divided over the discussions surrounding broader distribution of deceased-donor liver grafts. Indeed, the geographic disparity in access to organs remains a contentious topic that has progressed from a debate among medical professionals to now resulting in a litigious intersection with law, politics, and policy. Although in December of 2018, the UNOS Board of Directors voted to support an AC approach to liver distribution with the hopes of achieving a more consistent and equitable approach to distribution, this by no means represents a mathematically optimized approach. Indeed, in December of 2018, the UNOS Board of Directors also approved the recommendation from the UNOS Ad Hoc Geography Committee to use continuous distribution as a model for developing future organ distribution policies9—a direction supported herein by the first demonstration of the potential application of a boundaryless CS in liver distribution.

Paramount to achieving consensus on an approach to address geographic disparity is the need for a balance between efficiency and fairness. We have previously extensively analyzed this dilemma as it relates to resource allocation through the use of tradeoffs with assigned objectives7 and herein applied this framework to comprehensively analyze outcomes of various liver distribution concepts. This analysis has allowed for an in-depth, mathematically optimized, data-driven analysis of tradeoffs underlying each distribution concept. Tradeoff analyses of this kind have allowed for a methodical comparison of different distribution frameworks in terms of achievable outcomes of interest. The data in Figure 1, for example, revealed how many extra lives each framework can save per additional transport mile incurred, while Figure 2 reveals how much “fairer” distribution can be by minimizing differences in disease severity scores at transplantation per extra mile.

The generated tradeoff curves have demonstrated that a continuous scoring model achieved the greatest benefits both in terms of efficiency and fairness. Indeed, for any amount of transport distance incurred, there exists a CS-style policy that achieves both the lowest number of deaths, and the lowest SD of MMaT, among all other suggested policies. Tradeoff curves have allowed for a complete assessment of all policies simultaneously to provide the framework for an informed decision—a decision that the transplant community will have to pursue in selecting the single policy that achieves their desired outcome. Although the OPTN’s Liver Intestine Committee accepted MMaT as a metric of geographic disparity in liver allocation in March of 2013, it is important to note that tradeoff curves assessing additional metrics of community interest can be generated to aid the decision making process.

Although the issues of liver distribution have remained in the forefront of discussions, it is important to note that there is significantly higher potential impact in lives saved through a more accurate and objective prioritization of candidate disease severity in liver allocation. An OPOM (http://www.opom.online) was recently developed utilizing machine learning models trained to predict any adult candidate’s 3-month waitlist mortality based on 28 variables.10 Indeed, OPOM allocation, when compared to MELD, reduced mortality on average by 418 deaths every year in LSAM analysis. An examination of liver distribution policies as applied through a MELD- versus OPOM-based allocation score not only reaffirmed that OPOM results in a significant number of additional lives saved every year, but also that OPOM allocation combined with a continuous scoring distribution policy maximized this potential (556 lives saved annually).

Limitations of this study include that estimating number of deaths averted using LSAM may represent an overestimation, and that LSAM cannot account for changes in practitioner listing or acceptance behavior; however, it is important to note the ability of LSAM to predict the overall directionality of change in assisting in organ policy development.13 In addition, the approach herein was only 1 possible implementation of continuous distribution, utilizing a simple linear function of distance with disease severity. Snyder et al eloquently delineated other continuous distribution models that entail nonlinear functions of distance.14 Although such more advanced models might not be as simple to communicate to patients and transplant professionals as the linear model considered here, they represent more general models that can only potentially further improve outcomes. To alleviate disparity concerns of a linear continuous distribution model on candidates awaiting liver transplant separated by small distances (ie, 2 hospitals on opposite sides of town), a continuous scoring distribution model with rounding of distances at different granularity levels was created and in simulation demonstrated the same potential improvements in transplantation outcomes independent of the distance measurement granularity.

In summary, the transplant community has now accepted the concept of continuous scoring distribution policies to allow for a more equitable and boundaryless organ distribution. We now demonstrate application of this concept utilizing the model of liver distribution. This first application of a continuous distribution score for liver transplantation demonstrates superiority to all other policies currently employed or considered, and warrants similar consideration for other forms of solid organ transplants.",https://journals.lww.com/transplantjournal/Fulltext/2020/05000/Balancing_Efficiency_and_Fairness_in_Liver.19.aspx
Bishara et al. 2021,Machine Learning Prediction of Liver Allograft Utilization From Deceased Organ Donors Using the National Donor Management Goals Registry,"Early prediction of whether a liver allograft will be utilized for transplantation may allow better resource deployment during donor management and improve organ allocation. The national donor management goals (DMG) registry contains critical care data collected during donor management. We developed a machine learning model to predict transplantation of a liver graft based on data from the DMG registry.

Methods: Several machine learning classifiers were trained to predict transplantation of a liver graft. We utilized 127 variables available in the DMG dataset. We included data from potential deceased organ donors between April 2012 and January 2019. The outcome was defined as liver recovery for transplantation in the operating room. The prediction was made based on data available 12-18 h after the time of authorization for transplantation. The data were randomly separated into training (60%), validation (20%), and test sets (20%). We compared the performance of our models to the Liver Discard Risk Index.

Results: Of 13 629 donors in the dataset, 9255 (68%) livers were recovered and transplanted, 1519 recovered but used for research or discarded, 2855 were not recovered. The optimized gradient boosting machine classifier achieved an area under the curve of the receiver operator characteristic of 0.84 on the test set, outperforming all other classifiers.

Conclusions: This model predicts successful liver recovery for transplantation in the operating room, using data available early during donor management. It performs favorably when compared to existing models. It may provide real-time decision support during organ donor management and transplant logistics.","We developed a
We developed a machine learning model that can accurately predict liver recovery for transplantation in the operating room from a deceased donor 12–18 h after the OPO had begun management of the organ donor. Our study marks the first use of machine learning on detailed data on donor physiology to predict utilization of a donor liver for transplantation. In addition to physiologic and demographic data of the donor, our model also takes into account OPO, region, and hospital information for each prediction. Our GBM model with a ROC-AUC of 0.84 compares favorably when compared to the recently published DSRI,6 and as well the most recently available result of the Scientific Registry of Transplant Recipients yield calculator.7 Our model predicts utilization of a donor liver for transplantation at an early time point during donor management (Figure ​(Figure10)10) and is independent of OPO allocation/organ offer protocols and their postings on DonorNet. Because of the granularity of the DMG dataset, we achieved a favorable AUC when compared to the DSRI model but with only 12.4 % of donors and based on data available earlier during the donor management process. Criteria to determine if a liver is suitable for transplantation are imprecisely defined, and several scoring systems have been developed.13-17 Many of the existing scoring systems are based on unmodifiable, donor demographic data. One study revealed that donor factors associated with delayed or primary nonfunction included increasing age, hypernatremia (sodium levels >155 mEq/L), macrovesicular steatosis (greater than 40%), cold ischemia time longer than 12 h, partial-liver allografts, donor race, extended criteria donor status, and DCD-grafts.18

Meeting DMGs in brain dead organ donors has been associated with improved organ utilization and outcomes for multiple organs. Achieving these critical care endpoints has been associated with an increased number of organs transplanted per donor10,19-21 as well as graft survival rates for specific organs.22-24

The relationship between DBD-donor demographics, meeting DMGs, utilization of a donor liver for transplantation, and graft survival rates of livers has been studied by Bloom et al.22 The authors used prospectively collected data from 8 OPOs and after controlling for known predictors, donor BMI, male sex, normal glucose levels, the use of dopamine at the time of authorization for donation, and the use of vasopressin at time of allocation were associated with improved liver utilization. However, at follow-up, only donor BMI and serum sodium level at the time of allocation of organs were associated with improved graft survival.22 This finding is consistent with several published reports of hypernatremia contributing to graft loss.25-27

We identified several modifiable risk factors to be important (Figure ​(Figure2)2) predicting liver utilization, such as urine output, glucose levels, insulin dose, ratio of arterial blood concentration of oxygen over fraction of inspired oxygen, and mean arterial pressure. These risk factors may allow for additional interventions beyond the generally accepted DMG guideline.

Our model also identified nonmodifiable risk factors such as BMI and age. Overall, our model might allow to improve resource allocation during the donor management process. A recent study demonstrated that even livers refused by 5 or more surgical teams can be transplanted successfully via a rescue allocation processes.4 Our model allows timely identification of whether a liver will be used for transplantation, or conversely is at high risk for discard. Two potential interventions aimed at improving an organ’s probability of utilization for transplantation are a reduction of cold ischemia time through local allocation and machine perfusion. Behavioral changes regarding organ acceptance by OPOs and individual transplant centers as well as efficiencies gained in transportation can all affect cold ischemia times. Finally, newer technologies, such as normothermic perfusion, may be able to ameliorate the effects of ischemia time and rehabilitate organs to render them more useable for transplantation. It can facilitate the development and refinement of organ allocation policies to most efficiently manage organ donors and place these organs to maximize recipient utility.

In our GBM model, total number of DMGs met at authorization was one of the findings highly associated with utilization of a donor liver for transplantation. The following DMG’s were among the top fifty most important variables in the GBM model: target CVP (4–12 mm Hg) met at 12–18 h after authorization, target for number of vasopressors (≤1) met at authorization and target pH (7.3–7.5) met at 12–18 h after authorization.

The association between the achievement of DMGs and organ donation outcomes in DCDs has not been previously examined, mainly because DMGs are used by OPO staff to guide the bedside management of DBDs after declaration of death and authorization for organ donation. In contrast, OPO staff are not primarily responsible for the critical care of DCDs, as that remains under the purview of donor hospital staff until the withdrawal of life-sustaining treatment. However, the critical care parameters of a potential DCD can still be evaluated and considered during the allocation process, when OPOs decide which donation opportunities to pursue and transplant programs make organ acceptance decisions.

This study is an example of how machine learning can be utilized to examine a large data set. Machine learning may be useful in identifying important variables among several hundred candidates to help identify donors and donor organs that could benefit from improved donor management.

Given simplicity of the tabular data, GBM and other methods perform better than the neural network. Neural networks excel at analyzing and learning nonlinear associations and patterns within the data that interrelate and form multi-layered groupings of findings that combine to help the model make a prediction.28 The current dataset likely does not hold enough data and not enough dimensions to benefit from a neural network and may even cause overfitting by the neural network. A GBM on the other hand uses residual trees to attempt to capture the left-over components of the data that ascribes itself well to tabular data without overfitting.

The registry does not contain data of potential DBDs who are never taken to the operating room for organ recovery and therefore our model has not been validated in this group of donors. As a result, our model is most useful early during donor management and allows optimized resource allocation well before the donor is taken to the operating room for procurement. Also, we chose not to include any recipient-related factors or outcomes such as findings suggesting graft failure as this allows our model to predict earlier in the donation process but limits the insights into which donor-recipient matches are optimal. Our model does not address recipient outcomes or whether discarded livers should have been used for transplant. Our model cannot address differences between countries as it only includes data from the United States.

Like many other machine learning models, our model provides the probability for each prediction, which can be used as the confidence of the model in the prediction it has just made. These values are used to draw the receiver operator characteristic curves as seen in Figure ​Figure1.1. Also, as our model is a GBM, it can provide the variable importance for each prediction made. These 2 components will allow ease of use and interpretability of the model by providers and allow providers additional information in instances when the model makes a different prediction than their own clinical expertise. Although these interpretability tools can be helpful in making decisions, they do not describe a causation relationship between these important input variables and the outcome, they only provide explanation of the model’s prediction.

In summary, a GBM machine learning model was used to accurately classify donor livers as likely to be utilized for transplantation or not. Our model was able to predict utilization earlier in the donor management process than previously published models. Such a model will be crucial in efforts to define and improve a rescue allocation system for liver transplantation.",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8478404/
Papalexopoulos et al. 2022,"Ethics-by-design: efficient, fair and inclusive resource allocation using machine learning","The distribution of crucial medical goods and services in conditions of scarcity is among the most important, albeit contested, areas of public policy development. Policymakers must strike a balance between multiple efficiency and fairness objectives, while reconciling disparate value judgments from a diverse set of stakeholders. We present a general framework for combining ethical theory, data modeling, and stakeholder input in this process and illustrate through a case study on designing organ transplant allocation policies. We develop a novel analytical tool, based on machine learning and optimization, designed to facilitate efficient and wide-ranging exploration of policy outcomes across multiple objectives. Such a tool enables all stakeholders, regardless of their technical expertise, to more effectively engage in the policymaking process by developing evidence-based value judgments based on relevant tradeoffs.","As the OPTN case study shows, leveraging the predictive power of machine learning to replace the bottleneck policy evaluation step with near-instantaneous model-based evaluations opens up exciting possibilities for refining ethicists’ and other stakeholders’ input to data-driven policymaking. One of the chief advantages of the proposed approach is that it enables ethicists and communities to offer input at precisely the points at which their views have the most value. To illustrate, consider the following example of lung allocation policymaking. Asking ethicists or community members whether a lung candidate attribute estimating posttransplant survival and another attribute measuring waitlist survival should both be weighted at 34% of the composite allocation score, or one reduced by 2%, would likely elicit blank stares or weak intuitions at best. Asking instead ‘would you be willing to tolerate a 10% decrease in average years of post-transplant survival if it meant a 5% reduction in waitlist mortality,’ and showing how the tradeoff changes as a ‘slider’ moves, poses a set of considerations for which ethical theory and/or community sentiment is likely to generate more useful feedback. We view this tool as an important way to improve the ability of key stakeholders without medical expertise, including donor and recipient populations, to influence policymaking. This approach also enables ethicists and community groups to clearly prespecify ‘guardrails,’ for example, that disparities between blood type or racial groups not increase at all, or more than a specified amount, and show in near-real time how including, removing, or modifying those guardrails will change results. While we view this approach as a major step forward for ethically informed policymaking, it does not solve every problem. First among the difficulties that persist is that modeling is only as good as the data on which it is based and the assumptions upon which it relies. In general, our approach works best when policymakers have access to high-quality data and analytical tools to accurately predict outcomes, e.g. counterfactual simulation in the case of organ allocation. Even then, an ethicist or a community stakeholder may raise a question about the effect of a policy change—for example, effects on patients with intellectual disabilities—that is not captured in the input data and cannot be accurately modeled in a tradeoff curve. Similarly, predictions in applications areas where practitioners have little historical experience (e.g., COVID-19 vaccine allocation), are likely to rely on assumptions that should be closely examined. Recent work has shown how particular modeling choices in, say, curation of the training data or selection of objective metric, can result in biased model recommendations. Fortunately, the machine learning and optimization communities have made significant headway in developing methods to identify and mitigate such model-based biases.26– 28 In practice, data and model limitations should also be clearly communicated to stakeholders during the design process, to avoid overreliance on imperfect predictions and to allow clinical and subject-matter expertise to influence conclusions drawn from the analysis. The ideal solution is to use representative, high-quality data sets, when that is not possible to be transparent about the limitations of the data sets being used, and when those limitations are serious enough to reconsider whether the model can do the work we want out of it. That being said, we note that such limitations also beset older, simpler approaches to policymaking, and one advantage of our framework is that it streamlines determining where current modeling practices fall short in addressing ethically important questions; this in turns sets up the possibility of changing those practices to fill the gaps. Second, when done right, the approach discussed here seeks input from a multiplicity of stakeholders including ethicists, transplant professionals, donor communities, potential recipients, and sub-stratifications such as racial and ethnic minorities. The introduction of new analytical tools does not obviate the need for wide-ranging, cross-cutting deliberation to reach consensus, particularly as engaging with these stakeholders also means engaging with their biases. We believe our approach makes it easier for stakeholders to more meaningfully assess and weigh relevant tradeoffs, but it does not guarantee more convergence between the groups. How should a policymaker respond when various communities champion different tradeoffs? Should all views be treated equally? Many ethicists will chafe at the idea of ‘ethics by headcount’ in the sense of aggregating, without exploring the reasons behind, preferences for tradeoffs and potentially dismissing some as inconsistent or problematic. Though the federal regulation governing U.S. organ allocation policy provides a final safeguard to rule out legally inconsistent policy options, this Final Rule does not precisely dictate which tradeoffs are out of bounds. Again, this problem is not new; in making tradeoffs more visible and accessible, our approach may make the problem more common—though some may view this more a feature (by inviting greater engagement with the tradeoffs) than a weakness. Finally, there often exist significant cultural and institutional hurdles that must be overcome for the adoption of advanced analytical tools like the one we propose. In the case of organ allocation, US policymakers over the past two decades have most often used trial-and-error simulation to explore policy options. The shift to include advanced machine learning and optimization methods for helping develop a lung CD policy challenged the community’s flexibility to accommodate a new approach. Despite the introduction of new methodology, many of the core components of the age-old policy-making process remained in place including, committee discussion, public comment feedback from the community, and SRTR simulation modeling. The additional analyses, particularly optimized tradeoff curves (Figure 3), provided extra scrutiny of the proposal and helped stakeholders home-in on a final set of policy options worth consideration. Leveraging these tradeoff curves also provided a gentle introduction to the use of advanced mathematical methods that could pave the way for broader community acceptance of even greater reliance on goal-driven optimization in future allocation policy development.

Although some in the transplantation community might struggle with more sophisticated analytical tools, others see their adoption as an opportunity to improve the historically time-consuming policy development process. For years, committee discussion and retrospective data analysis informed policy proposals that were then modeled by the SRTR. However, with these new tools, the community and committee can feel more confident about their chosen allocation policy options before the final, confirmatory simulation modeling is conducted. Over time, these more complex models can gain public confidence as new-and-improved policies are implemented and demonstrated to meaningfully improve outcomes for patients awaiting organ transplantation. More generally, one
More generally, one might worry that, given relative unfamiliarity with machine learning among ethicists and stakeholder communities of interest, some may find it difficult to understand precisely what the approach we describe here ‘does’ or be concerned about ‘not seeing the whole picture.’ To remedy this gap will require thoughtful attempts at scientific communication that meets stakeholders ‘where they live,’ rather than a one-size-fits all strategy. Policymakers need to be sensitive to algorithmic aversion and key opportunities to manage it including the use of interactive tools like ‘sliders’ that enable stakeholders to see how AI/ML works even if they will never get ‘under the hood.’29 For healthcare policymakers focused on allocation controversies, it might also be worth considering, depending on the setting, combining the machine learning methods discussed here with deliberative democracy techniques, such as deliberative polling, consensus conferences, citizen juries, etc.30,31,32
 ",https://academic.oup.com/jlb/article/9/1/lsac012/6575420#351573490
Thongprayoon et al. 2022,Use of Machine Learning Consensus Clustering to Identify Distinct Subtypes of Black Kidney Transplant Recipients and Associated Outcomes,"Importance  Among kidney transplant recipients, Black patients continue to have worse graft function and reduced patient and graft survival. Better understanding of different phenotypes and subgroups of Black kidney transplant recipients may help the transplant community to identify individualized strategies to improve outcomes among these vulnerable groups.

Objective  To cluster Black kidney transplant recipients in the US using an unsupervised machine learning approach.

Design, Setting, and Participants  This cohort study performed consensus cluster analysis based on recipient-, donor-, and transplant-related characteristics in Black kidney transplant recipients in the US from January 1, 2015, to December 31, 2019, in the Organ Procurement and Transplantation Network/United Network for Organ Sharing database. Each cluster’s key characteristics were identified using the standardized mean difference, and subsequently the posttransplant outcomes were compared among the clusters. Data were analyzed from June 9 to July 17, 2021.

Exposure  Machine learning consensus clustering approach.

Main Outcomes and Measures  Death-censored graft failure, patient death within 3 years after kidney transplant, and allograft rejection within 1 year after kidney transplant.

Results  Consensus cluster analysis was performed for 22 687 Black kidney transplant recipients (mean [SD] age, 51.4 [12.6] years; 13 635 men [60%]), and 4 distinct clusters that best represented their clinical characteristics were identified. Cluster 1 was characterized by highly sensitized recipients of deceased donor kidney retransplants; cluster 2, by recipients of living donor kidney transplants with no or short prior dialysis; cluster 3, by young recipients with hypertension and without diabetes who received young deceased donor transplants with low kidney donor profile index scores; and cluster 4, by older recipients with diabetes who received kidneys from older donors with high kidney donor profile index scores and extended criteria donors. Cluster 2 had the most favorable outcomes in terms of death-censored graft failure, patient death, and allograft rejection. Compared with cluster 2, all other clusters had a higher risk of death-censored graft failure and death. Higher risk for rejection was found in clusters 1 and 3, but not cluster 4.

Conclusions and Relevance  In this cohort study using an unsupervised machine learning approach, the identification of clinically distinct clusters among Black kidney transplant recipients underscores the need for individualized care strategies to improve outcomes among vulnerable patient groups.","Black recipients of kidney transplants in the US have inferior outcomes. Several variables have been implicated, including higher risk for rejection and socioeconomic factors. These variables are often universally applied to all Black kidney transplant recipients. In this study, an unsupervised machine learning consensus clustering approach was successfully used to categorize Black kidney transplant recipients in the OPTN/UNOS database into 4 distinct phenotypes with high-stability clusters. The characteristics of transplant recipients in these 4 distinct groups include (1) highly sensitized, deceased donor kidney retransplant in cluster 1; (2) preemptive and/or short dialysis time with living donor kidney transplant in cluster 2; (3) younger, with hypertension, and without diabetes receiving low-KDPI kidneys from young deceased donors in cluster 3; and (4) older with diabetes receiving high-KDPI kidneys from ECDs in cluster 4. These distinct subgroups of Black kidney transplant recipients are associated with different clinical outcomes, including mortality, acute rejection, and death-censored graft loss. Cluster 2 represented the lowest number of Black kidney transplant recipients (3096 [14%]). Patients in this cluster had superior patient survival and the lowest risk for rejection. Most patients in this cluster had preemptive transplants (25%) or had a shorter dialysis duration (53%) and received a living donor kidney transplant (94%). Recipients were less likely to have had a prior transplant or to have diabetes. Compared with other clusters, patients in cluster 2 had excellent functional status, were more likely to carry private insurance, and had a higher level of educational attainment. Given these favorable characteristics, patients in cluster 2 demonstrated the best patient and graft survival and had the lowest observed incidence of acute rejection. The excellent outcomes observed in cluster 2 align with known data supporting better and earlier access to health care, preemptive kidney transplant, and improved survival benefits.7,41-48 Recipients in cluster 4 were older and more likely to have diabetes and have lower functional status. Cluster 4 represented the largest number of patients (38%). Although this group of recipients was not sensitized (median PRA, 0%) and the risk for rejection was lower (4.8%), most patients received thymoglobulin for induction. Recipients in cluster 4 were more likely to receive ECD kidney transplants and/or have high-KDPI donors with higher cold ischemia times and increased incidence of machine perfusion (59%). Recipients in cluster 4 had the highest rates of delayed graft function (42%) and death-censored graft loss at 3 years (10.2%). In addition to having higher cardiovascular risk, medical comorbidities, and reduced functional status, recipients in cluster 4 also had the lowest number of recipients who worked for income. Given these findings, recipients in cluster 4 may have had increased difficulty with access to posttransplant health care,26 resulting in increased mortality and graft loss.5,43,49,50 However, cluster 4 also had lower rates of rejection, likely because of a lower immune response due to being older and having a low PRA.51,52. The findings unique to cluster 4 recipients raise opportunities for directed improvements in outcomes. Emphasis on earlier access to transplant, improvements in diabetes care and functional status, and optimization of immunosuppressant regimens are areas of future investigations. Physiological changes associated with senescence can affect drug metabolism and increase the risk of posttransplant infection and malignant neoplasms in older recipients.52 A recent study using the US National Transplant Registry data (2005-2016)51 suggested that lower-intensity immunosuppression regimens, such as corticosteroid-sparing treatment, are beneficial for older kidney transplant recipients.51 Given that patients in cluster 4 had the highest mortality but the lowest rate of acute rejection, future studies are needed to identify whether lower-intensity immunosuppression regimens can reduce posttransplant complications, including infection and cancer,53-61 and ultimately improve patient survival. Recipients in cluster 1 were almost exclusively patients with a high PRA (IQR, 87%-100%). More than half of the patients had prior transplants and were women. Recipients in cluster 1 were more likely to receive a kidney from outside the local organ procurement organization (59%), with longer cold ischemia times and higher rates of delayed graft function. Most recipients received thymoglobulin induction along with triple-drug maintenance immunosuppression; however, these patients experienced higher rejection at 1 year and lower allograft survival when compared with recipients in cluster 2. Patients in this cluster had longer dialysis duration and lower functional status compared with recipients in cluster 2. Recipients in cluster 1 had the highest risk for rejection (8.1%). Despite an increased rejection risk, 3-year death-censored graft loss remained superior compared with rates in clusters 3 and 4. Additional strategies for earlier detection of subclinical rejection, including cell-free DNA and access to for-cause and protocol kidney biopsies, may be unique opportunities for improvement in cluster 1. Recipients in cluster 3 accounted for the second largest group (34%) among these Black kidney transplant recipients. Despite being younger and having fewer comorbidities, including diabetes and peripheral vascular disease, recipients in cluster 3 had a higher risk of mortality when compared with those in cluster 2. Among all clusters, patients in cluster 3 had the highest proportion of patients who received dialysis longer than 3 years (76%). The most common cause of ESKD was hypertension (53%). Most patients received non-ECD deceased donor kidneys with KDPI scores less than 85%. More than half of the donors were young, White male donors from a local organ procurement organization. Although patients in cluster 3 had good functional status, they had lower educational attainment when compared with recipients in cluster 2. Most patients in cluster 3 had public insurance and did not have a working income. In addition to increased mortality risk, patients in cluster 3 also had increased risk of allograft rejection (6.6%) and allograft loss compared with cluster 2. Although these patients were young and nonsensitized and had fewer comorbidities, these patients still had the longest dialysis duration, representing health inequities and disparities in access to kidney transplant among Black patients in the US.7,23 Compared with White patients, it is well known that Black patients experienced greater delays in referral to transplant centers, longer waiting times for transplant, and longer duration of dialysis before transplant.7,12,62-65 Although the 2014 Kidney Allocation System implementation has helped decrease racial disparities in wait-listing, inequalities in wait-listing remain in the US, suggesting the need for additional interventions.62 Indeed, many factors contribute to racial disparities in kidney transplant, including lower socioeconomic status, limited transplant education, geography, use of the Black race coefficient in estimated glomerular filtration rate formulas, and physician bias.7,13,26,62 Furthermore, although racial disparities have decreased in deceased donor transplant, ongoing disparities remain in living donor transplant among Black patients.66",https://jamanetwork.com/journals/jamasurgery/article-abstract/2791955
Ivanics et al. 2023,Machine learning–based mortality prediction models using national liver transplantation registries are feasible but have limited utility across countries,"Many countries curate national registries of liver transplant (LT) data. These registries are often used to generate predictive models; however, potential performance and transferability of these models remain unclear. We used data from 3 national registries and developed machine learning algorithm (MLA)–based models to predict 90-day post-LT mortality within and across countries. Predictive performance and external validity of each model were assessed. Prospectively collected data of adult patients (aged ≥18 years) who underwent primary LTs between January 2008 and December 2018 from the Canadian Organ Replacement Registry (Canada), National Health Service Blood and Transplantation (United Kingdom), and United Network for Organ Sharing (United States) were used to develop MLA models to predict 90-day post-LT mortality. Models were developed using each registry individually (based on variables inherent to the individual databases) and using all 3 registries combined (variables in common between the registries [harmonized]). The model performance was evaluated using area under the receiver operating characteristic (AUROC) curve. The number of patients included was as follows: Canada, n = 1214; the United Kingdom, n = 5287; and the United States, n = 59,558. The best performing MLA-based model was ridge regression across both individual registries and harmonized data sets. Model performance diminished from individualized to the harmonized registries, especially in Canada (individualized ridge: AUROC, 0.74; range, 0.73-0.74; harmonized: AUROC, 0.68; range, 0.50-0.73) and US (individualized ridge: AUROC, 0.71; range, 0.70-0.71; harmonized: AUROC, 0.66; range, 0.66-0.66) data sets. External model performance across countries was poor overall. MLA-based models yield a fair discriminatory potential when used within individual databases. However, the external validity of these models is poor when applied across countries. Standardization of registry-based variables could facilitate the added value of MLA-based models in informing decision making in future LTs.","This is the first study evaluating the internal and external performance of MLA-based models for predicting 90-day post-LT mortality across 3 international registries. The development of MLA-based prediction models is feasible using an objective and reproducible data cleaning method, and these models have internal validity. Such MLA-based models can be constructed using independent and harmonized international LT registries, with the ridge regression model yielding the best predictive performance. Although feasible, the model performance using such registries and an MLA-based approach yields fair discriminatory performance at best. As expected, a diminishing number of variables and granularity (independent to harmonized registries) yields decreased model performance. Importantly, the external validity of MLA-based models (built on 1 or more countries and evaluated in other countries) is poor.

Several groups have applied MLA-based models to predict post-LT outcomes (including graft and patient survival) using single-institutional and multi-institutional national data sets.5,12,25 Generally, the optimal discriminatory performance has been achieved with smaller, often single-institutional data sets.5,12,25,26 This discrepancy in performance is likely because of population-specific predictions, which show promising validation performance in patient data that are very similar to those in the training set, resulting in overly optimistic results for model performance beyond these specific populations.11 This problem is particularly challenging in smaller data sets. However, it is also likely that smaller registries, such as those using single-institutional data, may exhibit less variability in data recording and a higher degree of variable granularity, allowing them to derive the maximum benefit of MLA-based modeling approaches. The criticism raised with such smaller, often single-institutional–based data to develop MLA has been the unclear predictive accuracy of the models when applied to external data sets because they typically evaluate model performance on a split validation set from the same data set used to develop the original model.27

Despite the appeal of MLA-based modeling for big data, previous studies have demonstrated the predictive ability of machine learning techniques to be limited by data set characteristics, including limited accuracy in variable definitions and granularity.28,29 Notably, given these limitations, MLA models have been unable to reliably outperform standard non-MLA models (such as logistic regression) when using national transplant registries.28,29 Likely, to leverage the full potential of MLA and achieve maximum predictive performance in the future using big data, the use of high-quality, well-curated registries, with more in-depth clinical detail is required.30, 31, 32 Minimizing the use of subjective variables is imperative, given that model performance and reproducibility are highly contingent on the quality and objective nature of the data input.28 Within this context, it is critical to ensure that investigators do not introduce subjectivity by arbitrary groupings of categorical variables. The current study used an approach to maximize objectivity and reproducibility by grouping variables to a hierarchical decision tree according to the proportion prevalence of any given level of a multilevel categorical variable. This was performed to avoid grouping variables into bins that are likely to be investigator dependent and subject to bias (eg, grouping ascites as absent/mild vs moderate/severe or absent vs mild vs moderate/severe). Furthermore, prediction model building may be enhanced by including other types of increased granularity data beyond only clinical characteristics, such as imaging, pathology, and genomic information.33

Of the 3 national LT registries evaluated in the current report, MLA-based prediction models have been built and published using only the UNOS database.12 Recently, Ershoff et al12 used a deep neural network model on an extensive feature set (202 features) from the UNOS registry across 57,544 patients over 10 years, obtaining an AUROC of 0.703 for 90-day post-LT mortality. This predictive performance was similar to that obtained in this study using the ridge regression model with UNOS data (AUROC, 0.71). Notably, in the study by Ershoff et al,12 the MLA-based model did not achieve a significantly higher discriminatory performance than the survival outcomes following liver transplantation (SOFT) score (a non-MLA model) despite its complexity. In this study, a time-based modeling approach was used to better reflect the real world, where a model is trained on past data and validated and tested on future data. This was not performed in the study by Ershoff et al,12 which randomly split the entire data set in an 80:20 fashion. Therefore, MLA models are unlikely to add significantly to the prognostic performance compared with standard models using the current state of large-scale transplant registries. Beside this UNOS study, other national registry studies include a study from 2014, wherein Briceño et al6 built an artificial neural network model to predict 3-month graft survival and graft loss using data from 1003 donor-recipient pairs from 11 Spanish centers. When comparing the performance for graft survival and graft loss, both models outperformed traditional models such as SOFT, balance of risk, pre-allocation SOFT, donor MELD, MELD, and donor risk index, with an AUROC of 0.806 and 0.822, respectively.6 The authors subsequently sought to externally validate their findings of graft survival and graft loss prediction using an English cohort based on transplant data from King’s College in London.10 This study highlighted that MLA models function best when used for prediction in populations similar to those in which the models were developed. In addition, this study also highlighted that with more granular registries with potentially less variation in variable recording, MLA models are likely to be of added utility in model building compared with traditional statistical models.

Several aspects of this study differ from previous studies. This study evaluated data available by request and contains prospectively collected data. As such, it provides a real-life evaluation of MLA performance because no new data according to a common data dictionary were collected. To allow comparability between model performance harmonization, a process of variable coding ensured that variables were comparable between the registries. Moreover, although previous studies have evaluated the internal validity of MLA models, this study aimed to address an aspect that has often been lacking—the external validity of these models. The registries afforded opportunities to build various MLA models within each and evaluate their performance when applied to data from another country. External validation can be performed in several ways, and the current study was unique in that it sought to evaluate the impact of various cross-country validations on model performance. Specifically, models trained in 1 country were validated in another country and overall demonstrated poor discriminatory potential (AUROC ≤ 0.70). This underscores the poor utility of using MLA-based models built on 1 country’s data for application in a different context, such as another country. This is likely because of inherent limitations and variability within each data set. It is conceivable that these limitations may be overcome in the future by increasing the granularity of data sets and placing an increased emphasis on consensus for variable standardization, such as with increased collaboration across transplant data collecting organizations. In light of these limitations, although MLA-based models may have some applicability for use within each country, their transferability to other countries and settings is poor, and the use of conventional non–MLA-based statistical models may be preferable.34 One reason is the potential variability in data interpretation and entry across centers and countries. This has been previously demonstrated in nontransplant transatlantic registry studies.35 Although this variability can be mitigated with serial audits to ensure that the quality of data reporting is maintained within each country, it may be amplified when different country registries are compared with one another. Moreover, 1 criticial aspect related to the failure of artificial intelligence and machine learning systems is a phenomenon known as “data set shift.” This occurs when there is a mismatch between the data set on which the prediction model is developed and the data to which it is deployed.36,37 The downstream effect of this is an underperforming prediction model. The central theme related to mitigation strategies include first and foremost recognition and subsequent model retraining.36 There are several examples where external validation and model reappraisals have helped identify model deficiencies and help make the models more useful and relevant. Adamson and Smith38 outlined the risk of emergence of health disparities when certain demographic characteristics were excluded from the training data set of an MLA model. Other examples include a decrease in performance of prediction models with deployment to new clinical practice settings.39,40 Finally, as new treatments or standards of care are implemented for patients and diseases on which a previous prediction model is built, model training using new data after the implementation of such therapies will be required. An example of this was demonstrated by Pate et al41 who demonstrated significant miscalibrations over time secondary to shifting trends in cardiovascular diseases, most likely believed to be attributable to statin therapies. A mitigation strategy of this would be model retraining with data available after the introduction of such new therapies.36,41 Although many common LT variables are available in the transplant registries of the present study, there exists differential variable missingness, granularity, and availability across registries. For example, ascites is coded as “not reported,” “absent,” “slight,” “moderate,” “N/A,” and “unknown” in the US registry and as dichotomous (present/absent) in the UK registry. A variable denoting recipient ascites is presently unavailable in the Canadian data set. For any prediction model to be applied externally, it requires that the variables on which predictions are built exist in the external data set. For this reason, harmonization of variables was required across data sets to allow external validation. This inherently reduces model performance in 2 steps, the harmonization process by itself, and subsequently external validation (which by its nature is expected to yield inferior performance to internal validation). There are 2 main implications of this study. One is that external validation should be obligatory with the development of any prediction model that is meant for use in a different setting (country or institution). Second, it is critical to establish consensus on which variables to include in any given LT registry and how variables should be interpreted and recorded. Overcoming the abovementioned limitations is paramount to allowing international comparisons leveraging big data techniques such as MLA and identifying LT practices that could be shared based across practice environments in efforts to optimize patient outcomes.
This study is limited by the potential for selection and misclassification bias. The study cohort did not include individuals who dropped off the waitlist; therefore, inferences cannot be extended to this group of patients. Although the number of missing variables was low in the NHSBT and UNOS registries, it was relatively high in the CORR. As a result, certain variables closely associated with end-stage liver disease, such as serum albumin and recipient vasopressor status, could not be included in the harmonized data set. MELD exception points were not included as a variable because they are awarded differently between the countries. Instead, we used the biologic MELD scores and the etiology of liver disease (including hepatocellular carcinoma) in the modeling. Moreover, data validity within each registry depends on data entry by clinical reviewers, which is subject to nonquantifiable differences in measurement bias, and variable interpretation of definitions between countries. Regarding Canadian data, the Quebec organ transplant data set is not included in the CORR. Moreover, although CORR collects data from transplant programs, organ procurement organizations, and independent health facilities, data reporting to CORR is voluntary, and compliance is not monitored. Notwithstanding these limitations, this represents the largest study of MLA modeling and external validation using international LT registries to date.",https://www.sciencedirect.com/science/article/pii/S1600613522292600#sec4
Thongprayoon et al. 2023,Differences between Kidney Transplant Recipients from Deceased Donors with Diabetes Mellitus as Identified by Machine Learning Consensus Clustering,"Clinical outcomes of deceased donor kidney transplants coming from diabetic donors currently remain inconsistent, possibly due to high heterogeneities in this population. Our study aimed to cluster recipients of diabetic deceased donor kidney transplants using an unsupervised machine learning approach in order to identify subgroups with high risk of inferior outcomes and potential variables associated with these outcomes. Consensus cluster analysis was performed based on recipient-, donor-, and transplant-related characteristics in 7876 recipients of diabetic deceased donor kidney transplants from 2010 to 2019 in the OPTN/UNOS database. We determined the important characteristics of each assigned cluster and compared the post-transplant outcomes between the clusters. Consensus cluster analysis identified three clinically distinct clusters. Recipients in cluster 1 (n = 2903) were characterized by oldest age (64 ± 8 years), highest rate of comorbid diabetes mellitus (55%). They were more likely to receive kidney allografts from donors that were older (58 ± 6.3 years), had hypertension (89%), met expanded criteria donor (ECD) status (78%), had a high rate of cerebrovascular death (63%), and carried a high kidney donor profile index (KDPI). Recipients in cluster 2 (n = 687) were younger (49 ± 13 years) and all were re-transplant patients with higher panel reactive antibodies (PRA) (88 [IQR 46, 98]) who received kidneys from younger (44 ± 11 years), non-ECD deceased donors (88%) with low numbers of HLA mismatch (4 [IQR 2, 5]). The cluster 3 cohort was characterized by first-time kidney transplant recipients (100%) who received kidney allografts from younger (42 ± 11 years), non-ECD deceased donors (98%). Compared to cluster 3, cluster 1 had higher incidence of primary non-function, delayed graft function, patient death and death-censored graft failure, whereas cluster 2 had higher incidence of delayed graft function and death-censored graft failure but comparable primary non-function and patient death. An unsupervised machine learning approach characterized diabetic donor kidney transplant patients into three clinically distinct clusters with differing outcomes. Our data highlight opportunities to improve utilization of high KDPI kidneys coming from diabetic donors in recipients with survival-limiting comorbidities such as those observed in cluster 1.","We have successfully categorized kidney transplant recipients who received deceased donor kidneys from diabetic donors into 3 clusters with distinct clinical features and outcomes by using an unsupervised ML approach. Compared to clusters 1 and 2, cluster 3 recipients had superior outcomes specific to death-censored graft survival and patient survival at both 1 and 5 years. Compared to the other clusters, cluster 3 recipients had more favorable recipient and donor characteristics. All recipients in cluster 3 were first-time kidney transplant patients who received kidney allografts from younger donors with a lower KDPI score.
Cluster 1 recipients accounted for ~40% of this study cohort. Cluster 1 recipients had the least favorable recipient and donor characteristics. These characteristics included older age and presence of diabetes. Donor-recipient pairing was evident in cluster 1 and recipients in cluster 1 were more likely to receive higher KDPI allografts. Transplant outcomes, including survival, are known to be reduced in older recipients with comorbidities [16,17]. Use of higher KDPI kidney allografts in older recipients highlights opportunities to improve access and equity in kidney transplantation [18]. This compliments OPTN data, which has shown that outcomes are better with transplantation compared with remaining on the waitlist [6]. Use of older, higher KPDI, diabetic deceased donor kidney allografts is likely of greatest benefit to older recipients with minimal qualifying time, lack of available living donors, and presence of survival-limiting comorbidities. Although use of allografts with high KPDI characteristics are unlikely to be advantageous for younger recipients, diabetic kidney allografts from younger donors are likely to be of suitable quality. As suggested by findings from this unsupervised ML, kidney allografts with donor characteristics observed in clusters 2 and 3 should be considered for all waitlist patients, including those who are younger in age. Careful screening of the allograft, including consideration of a procurement biopsy, can help guide clinical decision making on the appropriate recipient. Overall, outcomes in cluster 1 highlight opportunities to increase utilization of higher KDPI kidneys in older recipients with significant survival-limiting comorbidities. Patient survival, independent of graft quality, was the largest factor responsible for influencing outcomes. Use of a higher quality deceased donor is unlikely to have yielded differences in patient survival.
Recipients and donors in cluster 3 had the most favorable characteristics including younger age. The favorable patient and death-censored graft survival observed in cluster 3 highlights the variability observed in diabetic deceased donor kidney allografts. Duration of diabetes and medical management influence allograft quality. The importance of procurement biopsies is better established in high KDPI ECD donors; however, use of procurement biopsies in standard KDPI donors (KDPI < 85%) can also be of value, particularly for donors with diabetes.
Although recipients in cluster 2 were younger, and also had more favorable characteristics, the cluster 2 cohort had decreased death-censored graft survival. All recipients in cluster 2 were re-transplant patients and this likely played a role in the graft survival observed. Similar to cluster 3, cluster 2 donors also had more favorable characteristics, such as younger age and lower KDPI, despite the presence of diabetes. It is likely that factors such as infection and rejection, often associated with re-transplantation, may have been responsible for the reduced graft survival observed.
Overall, the findings from this unsupervised ML approach highlight opportunities to further improve the utilization of deceased donor kidney allografts coming from diabetic donors. Although diabetic donor kidneys are increasingly accepted, our data still shows that only a small percentage (5%) of transplanted kidneys were diabetic donor kidneys, which is similar to other reports ranging from 3.5% to 8.8% [5,19,20,21,22,23]. The data from the UNOS registry suggested that approximately 40–50% of diabetic donor kidneys have been discarded each year from 2008 to 2019, and diabetic ECD donor kidneys had higher discarded rates, approximately 60% [3,24]. The current findings suggest that factors intrinsic to the recipient, including diabetes status and re-transplantation, more significantly impact graft and patient survival. Our study identified three different subpopulations of diabetic donor kidney recipients with distinct features and outcomes, which help us determine the recipients with high risks. These findings prompt us to better understand, evaluate, and allocate diabetic donor kidneys to shorten the waitlist and reduce the risk of death on the waitlist, although the impact of diabetic donor kidneys on patient and graft survival should be further investigated in large, prospective, and randomized studies.
The findings from this ML approach suggest that there are opportunities to further improve the utilization of deceased donor kidney allografts coming from diabetic donors. Factors intrinsic to the recipient, including diabetes status and re-transplantation, rather than presence of diabetes in the donor, appear to play more significant roles in influencing graft and patient survival. Unsupervised ML has the potential to provide healthcare professionals with a novel viewpoint on the distinct phenotypes of kidney transplant recipients who received kidneys from donors with diabetes and experienced disparate outcomes, and thus significantly improved the care and outcomes for these complex patients.
Limited biopsy data and missing reported data from the UNOS are important limitations of this study. Biopsies play a crucial role in determining the quality and suitability of kidneys for transplantation, including the utilization of kidneys from diabetic donors. However, despite their importance, biopsy data are often incomplete or unavailable. The quality of a kidney from a donor with diabetes can be influenced by several factors, such as the duration and severity of the donor’s diabetes, the presence of diabetic complications, and other comorbidities. Kidneys from diabetic donors may have microscopic changes, such as arteriolar hyalinosis and interstitial fibrosis, which can reduce the functional capacity of the kidney. Additionally, donors who had well-managed blood sugar levels and exhibit no signs or symptoms of renal impairment, such as elevated levels of protein in their urine or decreased kidney function, may not suffer from diabetic kidney disease. Our dataset had certain limitations, including the absence of information on coronary artery disease, post-transplant diabetes, and CMV prophylaxis management [25]. These factors are important considerations in the context of kidney transplantation outcomes. Therefore, future research endeavors should focus on incorporating these crucial pieces of information to gain a more comprehensive understanding of the characteristics of both recipients and donors in the UNOS database. By including these data, a more detailed analysis can be conducted to assess their impact on transplant outcomes and potentially enhance patient care in the field of kidney transplantation. Additional studies could expand upon the information that is currently available, allowing for a more comprehensive understanding of recipient and donor-related factors and their impact on transplant outcomes. Additionally, we utilized the SMD technique with a predetermined threshold exceeding 0.3 to identify the distinct characteristics associated with each cluster [26,27,28,29]. Instead of solely relying on p-values, our decision to use this cutoff value was informed by the effect size that was deemed clinically significant for the variables being investigated. This methodological approach was implemented to ensure that we captured meaningful differences among the clusters while considering the magnitude of these differences [10]. For instance, working income did not emerge as a distinct clinical characteristic specific to each assigned cluster. However, employment status and its associated factors, such as working income, may indeed play a role in post-transplant outcomes and adherence [30]. Future studies that incorporate data on employment status and post-transplant adherence could provide further insights into the relationship between working income, adherence, and transplant outcomes. Finally, our dataset did not contain waitlisted patients [31]. For instance, it is noteworthy that the impact of type 2 diabetes on the survival of waitlisted individuals is substantial [32]. As a result, it may be necessary to modify allocation policies for type 2 diabetes patients to account for the heightened risk of mortality and the possibility of their waitlist being suspended due to the presence of other medical conditions. Future studies to apply this machine learning approach in waitlisted patients would be of interest to better identify waitlisted patients who would have more survival benefit from receiving kidney transplant from donors with diabetes, high KDPI, or ECD status.",https://www.mdpi.com/2075-4426/13/7/1094
Abidi et al. 2023,Multiview Clustering to Identify Novel Kidney Donor Phenotypes for Assessing Graft Survival in Older Transplant Recipients,"Background 
Older transplant recipients are at a relatively increased risk of graft failure after transplantation, and some of this risk may relate to donor characteristics. Unsupervised clustering using machine learning may be a novel approach to identify donor phenotypes that may then be used to evaluate outcomes for older recipients. Using a cohort of older recipients, the purpose of this study was to (1) use unsupervised clustering to identify donor phenotypes and (2) determine the risk of death/graft failure for recipients of each donor phenotype.

Methods 
We analyzed a nationally representative cohort of kidney transplant recipients aged 65 years or older captured using the Scientific Registry of Transplant Recipients between 2000 and 2017. Unsupervised clustering was used to generate phenotypes using donor characteristics inclusive of variables in the kidney donor risk index (KDRI). Cluster assignment was internally validated. Outcomes included all-cause graft failure (including mortality) and delayed graft function. Differences in the distribution of KDRI scores were also compared across the clusters. All-cause graft failure was compared for recipients of donor kidneys from each cluster using a multivariable Cox survival analysis.

Results 
Overall, 23,558 donors were separated into five clusters. The area under the curve for internal validation of cluster assignment was 0.89. Recipients of donor kidneys from two clusters were found to be at high risk of all-cause graft failure relative to the lowest risk cluster (adjusted hazards ratio, 1.86; 95% confidence interval, 1.69 to 2.05 and 1.73; 95% confidence interval, 1.61 to 1.87). Only one of these high-risk clusters had high proportions of donors with established risk factors (i.e., hypertension, diabetes). KDRI scores were similar for the highest and lowest risk clusters (1.40 [1.18–1.67] and 1.37 [1.15–1.65], respectively).

Conclusions 
Unsupervised clustering can identify novel donor phenotypes comprising established donor characteristics that, in turn, may be associated with different risks of graft loss for older transplant recipients.
","Older individuals derive a survival benefit post-transplant,37 but donor and recipient characteristics may influence the outcomes of both graft failure and mortality after transplantation. In a national cohort, we generated five phenotypes of donors using unsupervised clustering. Recipients of kidneys from two clusters experienced a high risk of all-cause graft failure. While there were differences in the proportion of high-risk donors (defined using the KDRI), donors with high-risk KDRI scores were still found in low-risk clusters.

There are a number of possibilities explaining the tendency of clusters to associate with different recipient outcomes in this study. Clusters 1 and 5 comprised predominantly young, male donors with few comorbidities and the lowest KDRI scores. Recipients of kidneys from both clusters had an intermediate risk of death/graft failure; cluster 1 recipients were at higher risk, which we hypothesize may relate to an increased proportion of Black and obese donors.38,39 Interestingly, although cluster 1 kidneys had some lower risk features (younger donor age, lower risk of hypertension, lower risk of cerebrovascular COD) compared with cluster 3, recipients of these kidneys were still at higher risk of both death with graft function and death-censored graft failure, even after accounting for recipient characteristics. Cluster 4 had the highest proportions of donors with established risk factors for graft failure including obesity,38 hypertension,40 and diabetes.41 Cluster 2 had comparatively fewer donors with high-risk factors, but recipients of kidneys from this cluster were at a comparably high risk of graft loss (including death with graft function and death-censored graft failure). One striking difference is that cluster 2 had the lowest proportion of recipients at an ideal BMI (20%) while cluster 3 had the highest (45%). High donor BMI (>25) is associated with increased surgical retrieval and potentially prolonged warm ischemia time,42,43 which in turn is associated with all-cause graft failure.44 Another possibility is that unique combinations of donor characteristics that drove cluster separation also synergistically conferred risk. Only 5% of donors in cluster 3 had a cerebrovascular COD and elevated BMI (≥30 kg/m2) as opposed to 53% in cluster 2, emphasizing the importance of the inter-relation between factors. For example, BMI may not always represent fat mass, and in the absence of obesity, donors with larger BMIs (relative to recipients) may confer a lower risk of graft loss because of the contribution of more nephrons to the recipient.45 This may explain why those in cluster 3 were at low risk; although 54% of donors in this cluster had an elevated BMI, this may not have reflected obesity. Irrespective of the underlying reasons, our study does suggest that donor characteristics do influence both death and graft failure in older recipients, which is in contrast to some studies that have shown comparable outcomes for older recipients receiving low-risk or medium-risk donor kidneys.13

Recipient characteristics were generally similar across clusters, which may reflect that this was a relatively highly selected population (i.e., a smaller proportion of older adults with kidney failure are candidates for deceased donor transplantation relative to younger patients), further emphasizing the importance of identifying high-risk donor phenotypes that affect recipient outcomes. However, while the identification of differences in outcomes for donor clusters may suggest that future efforts to create prognostic models inclusive of cluster assignment are valuable, this should not be performed without external validation and should not lead to inequitable distribution of organs for older adults.46,47

Recent changes to kidney allocation in the United States have focused on incorporating utility goals such as increasing post-transplant survival rates. Inherent to this goal of utility is to adequately capture donor kidney longevity, ascertained by the KDPI (derived from the KDRI) and inclusive of an opt-in system for older candidates to receive lower-quality donor kidneys.18 Given the association between donor clusters and outcomes for recipients, using cluster assignment may be a novel way to capture organ quality and a means of assigning donor kidneys to older recipients to fit within the updated kidney allocation framework.18 In this regard, a future goal will be to determine whether cluster assignment can be mapped to more contemporary donors across the United States for older recipients. We do acknowledge, however, that a broadly applicable prediction model with good performance characteristics (considering the limited supply of donor organs) would be preferred as opposed to focusing on a particular subgroup. Furthermore, replacing an existing system that broadly attempts to address organ quality for all-comers is not the optimal approach. Therefore, at present, identifying donor phenotypes for older recipients while novel is exploratory and should serve to complement existing measures as opposed to replacing them.

This study has limitations. Our analysis was performed on national registry data; therefore, details on other important outcomes (i.e., changes in kidney function, development of donor-specific antibodies or post-transplant proteinuria) were lacking. Furthermore, we did not have details on other determinants of outcomes for older recipients including immunosuppressive medication use, episodes of acute rejection after transplantation, and patient clinical factors such as frailty.48 Although we addressed missingness with multiple imputation, there is the possibility that our outcomes may have differed with a more complete dataset. Finally, we were pragmatic in our selection of donor variables in cluster derivation and added information on donor organs (including pretransplant biopsy results49) may have better predicted death/graft failure. However, these data were not available in our derivation cohort, and our intended purpose was to use factors readily available in other registry databases.

We used an ML clustering approach inclusive only of donor characteristics and identified clusters of differing donor characteristics for a cohort of older transplant recipients. This approach may be a novel means to characterize donor phenotypes for older adults, acknowledging that donor clusters are associated with differing risks of graft loss in this population. Whether unsupervised clustering can be used to characterize donors to better inform organ allocation for older kidney transplant recipients is an important consideration for future study.",https://journals.lww.com/kidney360/fulltext/2023/07000/multiview_clustering_to_identify_novel_kidney.12.aspx
Cruz-Ramírez et al. 2013,Predicting patient survival after liver transplantation using evolutionary multi-objective artificial neural networks,"Objective
The optimal allocation of organs in liver transplantation is a problem that can be resolved using machine-learning techniques. Classical methods of allocation included the assignment of an organ to the first patient on the waiting list without taking into account the characteristics of the donor and/or recipient. In this study, characteristics of the donor, recipient and transplant organ were used to determine graft survival. We utilised a dataset of liver transplants collected by eleven Spanish hospitals that provides data on the survival of patients three months after their operations.

Methods and material
To address the problem of organ allocation, the memetic Pareto evolutionary non-dominated sorting genetic algorithm 2 (MPENSGA2 algorithm), a multi-objective evolutionary algorithm, was used to train radial basis function neural networks, where accuracy was the measure used to evaluate model performance, along with the minimum sensitivity measurement. The neural network models obtained from the Pareto fronts were used to develop a rule-based system. This system will help medical experts allocate organs.

Results
The models obtained with the MPENSGA2 algorithm generally yielded competitive results for all performance metrics considered in this work, namely the correct classification rate (C), minimum sensitivity (MS), area under the receiver operating characteristic curve (AUC), root mean squared error (RMSE) and Cohen's kappa (Kappa). In general, the multi-objective evolutionary algorithm demonstrated a better performance than the mono-objective algorithm, especially with regard to the MS extreme of the Pareto front, which yielded the best values of MS (48.98) and AUC (0.5659).

The rule-based system efficiently complements the current allocation system (model for end-stage liver disease, MELD) based on the principles of efficiency and equity. This complementary effect occurred in 55% of the cases used in the simulation. The proposed rule-based system minimises the prediction probability error produced by two sets of models (one of them formed by models guided by one of the objectives (entropy) and the other composed of models guided by the other objective (MS)), such that it maximises the probability of success in liver transplants, with success based on graft survival three months post-transplant.

Conclusion
The proposed rule-based system is objective, because it does not involve medical experts (the expert's decision may be biased by several factors, such as his/her state of mind or familiarity with the patient). This system is a useful tool that aids medical experts in the allocation of organs; however, the final allocation decision must be made by an expert.","In this study, a MOEA was designed to determine the survival of a patient after liver transplantation. In our experiment, the survival time was set at three months after the operation. With this MOEA, two sets of ANN models are obtained. One set is formed by ANN models guided by E (models with the best E value in the training phase) and optimises the probability of graft survival, and the other set is composed of ANN models guided by MS (models with the best MS value) and minimises the probability of graft failure.

To combine the errors provided by these two sets, a rule-based system was designed. This rule-based system obtains the most favourable donor–recipient pair of the top five recipients on the waiting-list, maintaining the desired principles of equity and efficiency.

From the simulation, it can be inferred that the performance of the proposed system is similar to that of the MELD method in the following percentages of cases: 40% of the time when MELD values are between 20 and 23; 100% of the time when MELD values are between 24 and 26; 20% of the time when MELD values are between 28 and 39; and 60% of the time when the recipients have the same MELD value (in this case, 27). Therefore, the proposed system complements the MELD method, which is commonly used. In general, the proposed system behaves like the MELD 55% of the time. This result suggests that our system complements the current system gaverning the allocation of organs from donors to recipients by taking into account characteristics of the donors, recipients and transplant organ.

It would be interesting to reduce the number of input characteristics in future experiments. Decreasing the complexity of the ANN models would facilitate their interpretation. Moreover, there would be a reduction in the time and cost needed to determine the necessary attribute values for donors and recipients. Another future objective is the implementation of rule-based systems in clinical practice to study their effectiveness with respect to the current allocation system (MELD).",https://www.sciencedirect.com/science/article/pii/S0933365713000122?casa_token=X4XEPhtFmyQAAAAA:xi3OPC7gwk0MmoqsNG0QkfwFsLBLG4QZDT_Hh0b_0cCUvQLSgj1PqFgtg3SUypujZdzDVJkd
Wies et al. 2023,Exploring the variable importance in random forests under correlations: a general concept applied to donor organ quality in post-transplant survival,"Random Forests are a powerful and frequently applied Machine Learning tool. The permutation variable importance (VIMP) has been proposed to improve the explainability of such a pure prediction model. It describes the expected increase in prediction error after randomly permuting a variable and disturbing its association with the outcome. However, VIMPs measure a variable’s marginal influence only, that can make its interpretation difficult or even misleading. In the present work we address the general need for improving the explainability of prediction models by exploring VIMPs in the presence of correlated variables. In particular, we propose to use a variable’s residual information for investigating if its permutation importance partially or totally originates from correlated predictors. Hypotheses tests are derived by a resampling algorithm that can further support results by providing test decisions and p-values. In simulation studies we show that the proposed test controls type I error rates. When applying the methods to a Random Forest analysis of post-transplant survival after kidney transplantation, the importance of kidney donor quality for predicting post-transplant survival is shown to be high. However, the transplant allocation policy introduces correlations with other well-known predictors, which raises the concern that the importance of kidney donor quality may simply originate from these predictors. By using the proposed method, this concern is addressed and it is demonstrated that kidney donor quality plays an important role in post-transplant survival, regardless of correlations with other predictors.","In simulated and real data we have demonstrated the usefulness of investigating a variable’s residual information together with statistical tests for the hypotheses that the variable’s importance partially or totally originates from correlated predictors. It can give further insight into a variable’s importance for prediction and thus contributes to the general need for improving the explainability of machine learning results. Given that Random Forests are only one of many options for prediction modeling in organ transplantation [6, 29, 30] and a lack of consensus about the meaning of variable importance, there will not be a single answer to the question of explainability and our work contributes one piece of information to that question. Issues and recommendations for applications
Our methods refer to applications where a single variable might be of particular interest with respect to its importance for prediction. This must be differentiated from another active field of research on how to use variable importance for variable selection [31–35] possibly based on p-values [36]. Our research has been motivated by an investigation of the role of kidney quality for post-transplant survival and this example can give some guidance on how to apply our method and interpret its results. The variable of interest was kidney quality (measured as KDPI) and we could demonstrate that its high importance for prediction partially originates from patient characteristics that are correlated to KDPI due to the allocation policy. However, our results also confirm that irrespective of these correlations kidney quality still is a major predictor for post-transplant survival even if not as high as its VIMP originally would suggest. The latter results might support findings of Bae et al. [37], who question the need for rejecting many kidneys of lower quality in the presence of a severe shortage of donor organs.
Our methods do not rely on a particular implementation of Random Forests, as the algorithm itself is not adapted but is applied to residual information that are derived in a preceding step (see Algorithm 1). This is an advantage towards for example the investigation of Conditional Permutation Importance (CPI) [18, 21] that rely on particular R implementations and are at least currently not compatible with ranger or Python implementations. However, CPIs have the advantage that conditional importance is derived by permuting a variable within strata of correlated variables and therefore do not rely on the specification of some model g and its accuracy.
To make the methods easily accessible and facilitate their application, we provide an implementation within the statistical software R. It uses the ranger implementation that is helpful in particular when it comes to the computationally challenging permutation tests.",https://pubmed.ncbi.nlm.nih.gov/37726680/
Bertsimas et al. 2019,Development and validation of an optimized prediction of mortality for candidates awaiting liver transplantation,"Since 2002, the Model for End-Stage Liver Disease (MELD) has been used to rank liver transplant candidates. However, despite numerous revisions, MELD allocation still does not allow for equitable access to all waitlisted candidates. An optimized prediction of mortality (OPOM) was developed (http://www.opom.online) utilizing machine-learning optimal classification tree models trained to predict a candidate's 3-month waitlist mortality or removal utilizing the Standard Transplant Analysis and Research (STAR) dataset. The Liver Simulated Allocation Model (LSAM) was then used to compare OPOM to MELD-based allocation. Out-of-sample area under the curve (AUC) was also calculated for candidate groups of increasing disease severity. OPOM allocation, when compared to MELD, reduced mortality on average by 417.96 (406.8-428.4) deaths every year in LSAM analysis. Improved survival was noted across all candidate demographics, diagnoses, and geographic regions. OPOM delivered a substantially higher AUC across all disease severity groups. OPOM more accurately and objectively prioritizes candidates for liver transplantation based on disease severity, allowing for more equitable allocation of livers with a resultant significant number of additional lives saved every year. These data demonstrate the potential of machine learning technology to help guide clinical practice, and potentially guide national policy.
","For almost 2 decades now, MELD has served as the scoring system used to rank liver transplant candidates on the waitlist. Although it is the case that the MELD score and its components (bilirubin, INR, and creatinine) are effective predictors of 3-month mortality, they are not the only relevant predictors. Indeed, although a simple method to stratify candidates awaiting liver transplantation, the MELD score is a linear regression method that does not accurately predict mortality for all candidates who can benefit from liver transplantation. The latter is demonstrated by our results demonstrating a significant deterioration in MELD predictive capabilities with increasing disease severity when compared to OPOM. It is important to note that it is the candidates with the highest disease severity that warrant the most accurate mortality prediction, to in return allow for the most accurate prioritization on the liver transplant waitlist. Differentiation within the latter cohort of the highest disease acuity represents the greatest challenge of this prediction problem. In contrast to MELD, which demonstrated decreasing AUC values as sicker patient strata are considered, OPOM maintained significantly higher AUCs, especially within the sickest candidate population, thus allowing for a more accurate prediction of waitlist mortality.
The use of MELD exception points within the current scoring system has represented an arbitrary, yet advantageous, solution for certain subpopulations of candidates, most notably those candidates with HCC. Indeed, Berry and Ioannou, through a competing risks analysis, demonstrated a near-complete lack of survival benefit among patients undergoing liver transplantation on the basis of MELD exception points, and thus calling into question the need for a system that artificially raised MELD scores.7 The latter “HCC advantage” has been addressed through first serial downgrades in the amount of MELD exception points granted, and subsequently, more recently, with both a delayed initiation of MELD exception points (6-month delay), as well as a cap on the extent of points an individual can achieve (MELD 34 cap).8 These modifications have been implemented with the hopes of decreasing waitlist mortality and increasing transplant rates in the non-HCC population; however, they have thus far represented insufficient and inexact changes in adequately equalizing access to liver transplants for the non-HCC population. Although well intentioned, the quest to equalize priority between the HCC and non-HCC candidates has been fundamentally inadequate, as they have utilized the assignment of exception points based on an imprecise mortality prediction.
Herein, we introduce OPOM, a novel system based on a state-of-the-art machine-learning method that has allowed for a more accurate prediction of 3-month mortality rate for all patients on the liver transplant waitlist. OPOM allocation outperformed the currently used MELD-based prediction method. In simulations, OPOM averted significantly more waitlist deaths/removals for both HCC and non-HCC candidates, and yet maintained overall transplant rates, thereby allowing for more equitable and efficient allocation of liver grafts for candidates awaiting transplantation across all levels of disease severity. As demonstrated using LSAM, the use of OPOM in place of current Match MELD scores, would save on average at least 418 more lives each year, with every UNOS region benefiting from this effect. It is notable that the overall number of transplants remains stable with OPOM allocation, albeit with an acceptable, and expected, decrease in HCC transplants to accommodate the increase in transplants of nonexception point candidates. Unlike MELD allocation, which relies on the cumbersome and inexact approach of exception point assignment, OPOM allows for accurate prioritization of all candidates based on individual characteristics, thus negating MELD’s varying levels of success in predicting mortality for different patient populations. For candidates with hepatocellular cancer, OPOM’s predictive ability is strengthened by the incorporation within the model of α-fetoprotein (AFP) levels, as well as tumor size and number.
The accurate prediction of an individual candidate’s risk of waitlist mortality/removal is paramount to ensure equitable access to liver transplantation. Whereas on the one hand MELD-based allocation with inclusion of exception points has overprioritized exception point candidates at the expense of those candidates listed with lab MELD scores, on the other hand, utilizing only a lab-based MELD score for waitlist prioritization would shift the pendulum in the opposite direction, resulting in an allocation process that greatly underserves those in need of a liver transplant but with a lab MELD score that does not reflect their severity of disease. OPOM achieves an evidence-based, unbiased, and objective middle ground for all waitlisted candidates by utilizing multiple variables with associated trajectories. Notably, there is a higher number of transplants in the female population with OPOM allocation, perhaps overcoming the systematic bias noted in MELD-based allocation for female candidates.9,10 The latter has been attributed to the inability of MELD to accurately capture the female candidate’s degree of renal insufficiency based on serum creatinine levels, resulting in lower MELD scores and thus lower transplantation rates. OPOM has provided a more complete picture of the individual candidate’s true waitlist mortality that in return has allowed for a more accurate prediction of need for liver transplantation. Although there is a decrease noted in transplants for Black and Asian candidates with OPOM allocation, with an increase in White and Hispanic patients transplanted, it should be noted that there is also a decrease in waitlist deaths for all of these candidate populations.
The 418 waitlist deaths averted with OPOM utilization is significantly more than the number predicted with implementation of MELD-Na. Indeed, MELD-Na, which was approved by UNOS in June 2014 and implemented in January 2016, was predicted through similar LSAM analyses to decrease waitlist deaths by only 52 patients a year.3 Similarly, the application of a 6-month delay in awarding exception points for HCC candidates was simulated in LSAM to achieve a higher rate of transplants for non-HCC candidates, at the expense of a lower transplant rate for HCC candidates. The downstream effect on waitlist mortality with this proposed change, compared with the current policy, was a net reduction of only 30 deaths in the non-HCC population. The latter policy was adopted in October 2015, and much like the acceptance of MELD–Na, although well intentioned, represented nominal changes in waitlist mortality in simulations when compared to liver allocation through OPOM. Although the actual number of waitlist deaths averted with LSAM under OPOM allocation may represent an overestimation, it is important to note the ability of LSAM to predict the overall directionality of change.11
Machine learning holds the potential to become an indispensable tool for clinicians, with optimized predictions based on large amounts of data.12,13 OCTs are a state-of-the-art machine-learning method.4 OCTs are decision trees similar to the classification and regression trees (CARTs), but are solved to global optimality with a novel method using mixed-integer optimization that outperforms the classical CART algorithms.14 We utilized OCTs to develop an analytical tool that takes all available patient information to predict whether the waitlisted candidate will undergo the adverse events of either death or becoming unsuitable for transplantation within 3 months. In contrast to the piecemeal way in which current policy has been constructed, our tool is trained on historical outcomes in a unified fashion, utilizing millions of data points. Instead of adding in exceptions and cutoffs ex post to decrease mortality on the waitlist, machine-learning analytical tools tackle the problem directly by building these different criteria into the model itself. The out-of-sample AUC and accuracy illustrated that OPOM performs well not only on patients without exceptions, but also on patients with HCC exceptions. Furthermore, the OPOM advantage over MELD is most notable among sicker patient populations, with OPOM outperforming both match MELD and MELD-Na in AUC analysis, thus allowing OPOM to achieve a greater “sickest-first” allocation policy.
The use of readily available, reproducible, and objective data that accurately predict liver-related mortality is essential. Although OPOM utilizes a larger number of variables than MELD does, it is important to note that many of these additional variables are linked to MELD, and it is the trajectories of change in these lab values that power OPOM’s accuracy. The latter concept is in line with studies examining the utility of changes in MELD scores for both waitlist and posttransplant mortality prediction, as well as liver transplant allocation.15,16 At first glance OPOM’s complexity, in comparison to MELD, can be overwhelming. However, it is notable that no additional data collection would be required by the transplant practitioner, as OPOM was generated based on available data within the STAR files, data that are routinely collected on all waitlisted candidates. Furthermore, OCTs are versatile tools that can allow for additional variables to be included/excluded with ease should additional priorities in liver allocation require that OPOM be modified. This could be crucial for appropriate allocation to the group of candidates with non-HCC standardized exceptions (eg, those candidates with hepatopulmonary syndrome, portopulmonary hypertension, and so on), and those candidates with non-HCC, nonstandardized exceptions.17 Although for the purposes of the initial creation and application of OPOM these non-HCC exceptions populations were grouped in the non-HCC patients population, they nonetheless would benefit from an optimized prediction method based on incorporation of consensus variables that accurately gauge their risk of mortality. To this point, granularity in the varying types of MELD exceptions within LSAM would also allow for a more accurate assessment of the differing classes of exception-point candidates, instead of a simple HCC vs non-HCC candidate comparison. It should be noted that LSAM analysis is also limited in that it only allows for an accurate assessment of waitlist deaths, as waitlist removals include not only candidates with deterioration in their condition, but also those removed due to improvement in their condition. Although additional analysis with consideration of a shorter or longer interval of waitlist risk could be considered, the risk of waitlist mortality at the 3-month interval was assessed to allow for accurate comparisons to MELD score calculations. Despite these limitations, and the fact that LSAM cannot account for center or practitioner changes in listing or acceptance behavior, LSAM remains the current simulation model employed to assess and implement national policy changes in liver allocation and distribution.
It should be noted that OPOM allocation does not address the issues in liver distribution, and the resultant geographic disparity that exists between UNOS regions and donor service areas (DSAs). However, it is worth noting that the application of this machine-learning tool is capable of saving an additional 418 lives every year—of the same magnitude as that achieved with LSAM models of wide broader sharing. Thus, the implementation of OPOM represents an avenue to achieve more equitable liver allocation within any defined geographic unit.
The application of an OPOM-based allocation system would more accurately adhere to the “sickest-first” principle. Indeed, the decrease in waitlist mortality/removal achieved through utilization of OPOM would not only represent the potential for more equitable allocation, but also would represent an important facet toward alleviating the discrepancy between supply and demand.",https://pubmed.ncbi.nlm.nih.gov/30411495/
Bezjak et al. 2023,Use of machine learning models for identification of predictors of survival and tumour recurrence in liver transplant recipients with hepatocellular carcinoma,"Background
Hepatocellular carcinoma (HCC) is one of the leading indications for liver transplantation (LT) however, selection criteria remain controversial. We aimed to identify survival factors and predictors for tumour recurrence using machine learning (ML) methods. We also compared ML models to the Cox regression model.

Methods
Thirty pretransplant donor
Thirty pretransplant donor and recipient general and tumour specific parameters were analysed from 170 patients who underwent orthotopic liver transplantation for HCC between March 2013 and December 2019 at the University Hospital Merkur, Zagreb. Survival rates were calculated using the Kaplan-Meier method and multivariate analysis was performed using the Cox proportional hazards regression model. Data was also processed through Coxnet (a regularized Cox regression model), Random Survival Forest (RSF), Survival Support Vector Machine (SVM) and Survival Gradient Boosting models, which included pre-processing, variable selection, imputation of missing data, training and cross-validation of the models. The cross-validated concordance index (CI) was used as an evaluation metric and to determine the best performing model.

Results
Kaplan-Meier curves for 5-year survival time showed survival probability of 80% for recipient survival and 82% for graft survival. The 5-year HCC recurrence was observed in 19% of patients. The best predictive accuracy was observed in the RSF model with CI of 0.72, followed by the Survival SVM model (CI 0.70). Overall ML models outperform the Cox regression model with respect to their limitations. Random Forest analysis provided several relevant outcome predictors: alpha fetoprotein (AFP), donor C-reactive protein (CRP), recipient age and neutrophil to lymphocyte ratio (NLR). Cox multivariate analysis showed similarities with RSF models in identifying detrimental variables. Some variables such as donor age and number of transarterial chemoembolization treatments (TACE) were pointed out, but these were not influential in our RSF model.

Conclusions
Using ML methods in addition to classical statistical analysis, it is possible to develop sufficient prognostic models, which, compared to established risk scores, could help us quantify survival probability and make changes in organ utilization.","Key findings
The volume of medical data is increasing on a daily basis and the need for a machine-driven way of processing huge amounts of data is evident. A multidisciplinary approach by clinicians and data scientists may facilitate the development of more precise prediction models. ML modelling, complementary to traditional statistical analysis, is a way to identify complex multidimensional and curvilinear relations between various parameters (7). This study has confirmed the predictive superiority of ML methods in comparison to traditional statistical analysis. We have also shown the importance of utilizing and comparing several different ML methods due to the specific ways in which they approach data processing. This study has also selected donor CRP as the most relevant predictor of recurrence free survival, which was not the case in similar studies. However, this needs to be confirmed with analysis of larger datasets and experience from other centers.

Strengths and limitations
The distinct feature of this study is that it is the first prospective analysis of a retrospectively collected dataset from our centre and that is a single centre study with homogenous management of patients. Using ML methods on smaller datasets is challenging, along with the delicacy of drawing general conclusions from the results. To maximize the number of data, imputation was done for missing parameters, excluding as few patients from the study as possible. The dataset, with objective parameters, can be affected by misclassification bias and any generalization of the results needs external validation on larger datasets. The major problem in ML is overfitting, which means that the model is well-suited to the existing training data, but performs poorly when given new, unseen data. To avoid biased conclusions, we took several steps to address that problem, which is commonly encountered with small datasets. First, we inspected the dataset for potential outliers which can be influential when there is a low number of an observation. We also selected relevant features, performed regularization and controlled the depth of tree-based models. To make the most of our dataset, we executed cross-validation (3- and 5-fold) to identify the best hyperparameters for all our ML methods. An example of finding the best hyperparameter alpha is given in Figure 8. Lastly, we tested our final models on a holdout test set, except for all Cox models, which was built on the entire dataset.

An external file that holds a picture, illustration, etc.
Object name is atm-11-10-345-f8.jpg
Figure 8
Hyperparameter alpha optimization process using 5-fold cross-validation for elastic net regularization Cox models.

Comparison with similar studies
Even though ML has existed for decades, it is a relatively new concept in medical data analysis and its popularity is currently increasing. Comparing the ML approaches to statistical methods such as Cox proportional hazards regression is a topic often discussed in literature (25-27). Each approach has its strengths and weaknesses. ML is algorithmic in its nature and it can identify patterns in data through numerous iterations to learn the relationships between parameters. As opposed to classical statistical modelling no assumptions about underlying distributions are made. Statistical analysis relies on hypothesis testing, data analysis and explanation of the relation between variables, while ML focuses more heavily on prediction of unseen data. Learning a model can take into account a large number of variables with their complex, nonlinear relations, while statistical analysis usually focuses on a relatively small number of parameters (25). ML is increasingly becoming an invaluable tool for evaluation of pre- and post-transplant aspects of cadaveric and living-donor liver transplantation. High-performing imaging assessments enabled by ML algorithms can provide more accurate pathohistological evaluation of graft quality and streamline the process of liver segmentation. ML can also be used to facilitate timely detection of liver tumours in the setting of HCC, as well as predicting post-transplant morbidity and mortality with greater accuracy (8,28,29). Models predicting waitlist dropout are also reported, bearing particular relevance for patients with HCC (30). Transplant oncology is a rapidly evolving field, and optimizing organ allocation in tumor patients and predicting tumour recurrence after transplantation is of great significance (8,10,31). Traditional models for patient selection and prediction of recurrence in liver transplantation for HCC rely on classical statistical methods and may be limited in a complex multifactorial setting. Nam et al. developed a model (MoRAL-AI) that uses deep neural networks to predict the HCC recurrence, taking into account tumour biology, as indicated by biomarkers, and imaging-assessed tumour size (32). Ivanics et al. conducted a comparative evaluation of multiple ML models to develop the Toronto postliver transplantation HCC recurrence calculator. Their research showed that the Coxnet model had the best metrics and outperformed RSF, survival SVM, and neural networks (DeepSurv), and demonstrated the importance of exploring various ML models on data analysis (7). The application of a ML technique has the potential to make use of retrospective data to create more precise prognostic models. However, this can also be seen as a disadvantage, depending on the quality of the dataset. In comparison to similar studies, our approach was to investigate several different ML methods and the Cox proportional hazards regression on our dataset. Creating a predictive calculator or a score based on a limited number of patients from a single center can lead to imprecise outcomes due to potential biases. However, identifying predictors of survival is important for comparison with experience from other centers. External validation of similar studies is needed to obtain clinically relevant results.

Explanations of findings
Several variables stood out as having an impact on survival and tumour recurrence and some appeared in more than one model. The most detrimental variable in all the models was donor CRP. CRP is a non-specific acute phase reactant associated with various inflammatory diseases, sepsis and malignant tumours, that is widely available, inexpensive and has been in clinical use for many years. It is synthetized in hepatocytes, both in normal and HCC cells, and can reflect the degree of local inflammation since it encourages proliferation of hepatocytes and promotes HCC growth (33-36). Inflammation creates a microenvironment that favours DNA damage and neoangiogenesis, thus facilitating tumour growth and creating a vicious circle in which tumour creates inflammation that helps it develop (37). In literature, CRP is described as a prognostic factor of several types of cancer-oesophageal squamous cell carcinoma, cervical cancer and non-small cell lung cancer (38-40). Elevated CRP levels were also found to be predictive of overall survival and tumour recurrence in non-transplanted HCC patients after liver resection or treatment with locoregional therapies (41). Albeit the evidence for influence of cadaveric donor CRP on LT is scarce in literature, previous studies have reported that an elevated CRP in HCC patients undergoing living donor LT is predictive of a poor outcome (42,43). NLR as an index has demonstrated its value in infections, cardiovascular and inflammatory diseases, and in several types of primary and metastatic cancer. The role of NLR in HCC has emerged after the observation that sorafenib treatment in HCC patients had significantly better survival benefit in those with low NLR (19,20,44). It is considered that NLR represents the balance between the protumour inflammatory status and the antitumour adaptive immunity. Increase in NLR is suggestive of an increase in overall inflammatory status or a decrease in adaptive immunity. Hence inflammation, a stimulating factor in tumour microenvironment and a well-known indicator of tumour progression, was found important in survival and tumour recurrence in our group through two separate variables in our analysis. We considered NLR as a continuous variable with a reference range according to Forget et al., even though the optimal “cut-off point” in clinical settings has been debated in literature (45-47).

AFP is a widely accepted marker with prognostic significance in HCC, and also the only tumour marker routinely used for prognostication and treatment selection of patients with HCC. Even though exact tumour staging can only be confirmed after histologic study of the explanted liver (tumour size and number, vascular invasion, differentiation), AFP is considered to be a representative parameter that correlates with vascular invasion, and thus can be very predictive of HCC recurrence (48-50). In some of the recent studies, it was not determined that AFP is a good prognostic marker for patient survival, but the rate of tumour recurrence showed a positive correlation with elevated AFP values (51,52). In our group, the median value of AFP was 11 ng/mL and HCC recurrence was reported in 19% of patients. Since AFP was found to be significant in the RSF model as well as in the Cox proportional hazards multivariate analysis, we can conclude that moderate elevation of AFP may be associated with increased incidence of HCC recurrence.

The share of elderly people in the general population is increasing and so is the age of transplant recipients. The elderly tend to have more comorbid conditions that can affect postoperative complications and survival (53,54). The mean age in our cohort was 62.3±7.1 years which supports the claim that LT is successful and feasible in older population. Although older age has been highlighted as a risk factor in our models, our results suggest that recipients should be carefully selected according to their comorbidities and that old age alone is not an exclusion factor for LT. Identifying other risk factors in combination with age could help in optimal patient selection.

Implications and actions needed
Besides previously mentioned and used ML models, deep learning with artificial neural networks is a well-established and popular method that is increasingly used in many different professional areas today (8). Unfortunately, it was not applicable to our dataset due to the small number of patients (for this method) in our cohort. In our opinion, rather than one method replacing the other, ML modelling is better considered as complementary to classical statistics. The strength of the Cox proportional hazard model lies in its interpretable parameter estimates, which have a straightforward meaning in terms expected hazard rate. Although this model works well with small datasets, the feature selection process requires careful consideration and expert medical knowledge, and should be based on the confidence interval of individual variables or statistical significance (P<0.05). ML methods, such as we used in this study, offer more flexible alternatives for analyzing large, complex, heterogeneous data with a nonlinear relationship. They can intrinsically perform feature selection providing a list of more influential variables without straightforward interpretation. We must be cognizant of the different objectives of both ML and classical statistical methods. Classical statistics is better suited for interpretation and describing the relationships between variables and the outcome of survival, while ML techniques (e.g., using a holdout test set and cross-validating hyperparameter search) are focused on creating the best possible model for accurate predictions of time of event, without intending to explicitly state the relationships between variables.",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10477658/
Mark et al. 2019,"Using machine learning to estimate survival curves for patients receiving an increased risk for disease transmission heart, liver, or lung versus waiting for a standard organ","Introduction
Over 19% of deceased organ donors are labeled increased risk for disease transmission (IRD) for viral blood-borne disease transmission. Many potential organ recipients need to decide between accepting an IRD organ offer and waiting for a non–IRD organ.

Methods
Using machine learning and simulation, we built transplant and waitlist survival models and compared the survival for patients accepting IRD organ offers or waiting for non–IRD organs for the heart, liver, and lung. The simulation consisted of generating 20 000 different scenarios of a recipient either receiving an IRD organ or waiting and receiving a non–IRD organ.

Results
In the simulations, the 5-year survival probabilities of heart, liver, and lung recipients who accepted IRD organ offers increased on average by 10.2%, 12.7%, and 7.2%, respectively, compared with receiving a non–IRD organ after average wait times (190, 228, and 223 days, respectively). When the estimated waitlist time was at least 5 days for the liver, and 1 day for the heart and lung, 50% or more of the simulations resulted in a higher chance of 5-year survival when the patient received an IRD organ versus when the patient remained on the waitlist. We also developed a simple equation to estimate the benefits, in terms of 5-year survival probabilities, of receiving an IRD organ versus waiting for a non–IRD organ, for a particular set of recipient/donor characteristics.

Conclusion
For all three organs, the majority of patients are predicted to have higher 5-year survival accepting an IRD organ offer compared with waiting for a non–IRD organ.","For all three organs, over 69% of simulated patients have a higher predicted survival accepting an IRD organ offer compared to waiting for a non–IRD organ with average wait times in our data. The difference between the 5-year survival probabilities of receiving an IRD organ versus waiting for 1 day and receiving a non–IRD organ is within 1% on average across all scenarios. This difference is positive in roughly 50% of the simulations when the estimated wait time is 5 days (or longer) for the liver and 1 day (or longer) for the heart and lung. As estimated wait times increase, the difference also increases, suggesting that patients who are likely to remain longer on the waitlist would benefit more from receiving an IRD organ (versus waiting and receiving a similar non–IRD organ later). For any of the three organs, an estimated increase (or decrease) in 5-year survival probability for receiving an IRD organ versus remaining on the waitlist for a particular set of recipient and donor characteristics, and particular wait times, can be quickly found using the benefit equations. As Table 3 shows, survival probability benefits differ by organ. This may be in part because of differences in the organ allocation systems. Further research can be done to investigate how different organ allocation systems and incentives to discard or receive an organ can affect organ discard rates.

For the heart, liver, and lung, previous studies compared the survival of IRD organs to non–IRD organs using a retrospective analysis that divided the population into two groups. While a large-scale simulation, where comparisons were made for thousands of scenarios, was conducted for the kidney,8 to our knowledge, this has not been performed for the heart, liver, and lung.

Further, to our knowledge, this is the first study to develop a simple equation that estimates the difference in the survival probabilities for receiving an IRD organ versus waiting and receiving a non–IRD organ (heart, liver, or lung) for a given recipient-donor pair.

There are several reasons behind the benefits of receiving an IRD organ. The risk of undetected infection resulting in transmission is very small. The estimated risk of undetected HIV infection by serologic screening among IRD donors was found to be 1/11 000 for HIV and 1/1000 for HCV.45 According to the same study, NAT screening was projected to have even lower undetected risks. In addition, advances in treatment for HIV and HCV have resulted in improved mortality.46, 47 For HBV, a highly effective protective vaccine is available, as well as antiviral drugs that suppress the viral replication.48

A limitation of our analysis is that our simulations cannot estimate whether there are survival probability increases (or decreases) for receiving IRD organs beyond a 5-year horizon. As the post-transplant time horizon increases, the number of patients with available survival data decreases. It is possible that receiving an IRD organ for a particular scenario may result in a higher 5-year survival probability, but waiting for a non–IRD organ may result in a higher survival probability many years later, although this appears unlikely given advances in HIV, HCV, and HBV treatments.46-48

Another limitation is that we have a relatively small sample size of data from IRD heart, liver, and lung transplants (eg, compared with kidney transplants). However, we still have over 1000 observations for IRD transplants for each organ, and by conducting 20 000 simulations of recipient-donor scenarios for each organ, we were able to predict and assess the survival benefits for significantly more scenarios; hence, our study complements other studies that focus on retrospective data analysis. Third, because treatment for HIV and HCV has improved, our models, which use data prior to 2014, are likely to be “conservative,” that is, underestimate the survival probabilities for IRD organ recipients. With current advances in HIV and HCV treatments, we expect that the survival benefits for receiving IRD organs would be even higher. However, at the time of this study, we did not have 5 years of survival data from transplant records beyond 2014. While a comparison of survival probabilities between IRD and non–IRD organs is important, there are other factors to take into account when deciding whether to receive an IRD organ such as cost and quality of life. The quality of life for a patient on the waitlist is likely lower compared to a recipient with a functioning transplant.49

Patients and physicians might overestimate the risks of receiving an IRD organ, and better tools for accurately discussing the risks during informed consent are needed.10, 50, 51 Higher utilization of organs can reduce the gap between the number of organs available for transplantation and the number of patients on the waitlist. Reducing this gap can provide lifesaving transplants to patients who otherwise may die on the waitlist. This study's comparison between receiving an IRD heart, liver, and lung and waiting for a non–IRD organ can help physicians, patients, and researchers assess the risks of receiving or declining an IRD organ offer. Further, the methods used to compare the survival of a patient receiving an IRD organ offer or waiting for a non–IRD organ can be extended to other types of nonstandard donors, such as expanded criteria donors (ECD).",https://onlinelibrary.wiley.com/doi/10.1111/tid.13181
Miller et al. 2019,Predictive Abilities of Machine Learning Techniques May Be Limited by Dataset Characteristics: Insights From the UNOS Database,"Background: Traditional statistical approaches to prediction of outcomes have drawbacks when applied to large clinical databases. It is hypothesized that machine learning methodologies might overcome these limitations by considering higher-dimensional and nonlinear relationships among patient variables.

Methods and results: The Unified Network for Organ Sharing (UNOS) database was queried from 1987 to 2014 for adult patients undergoing cardiac transplantation. The dataset was divided into 3 time periods corresponding to major allocation adjustments and based on geographic regions. For our outcome of 1-year survival, we used the standard statistical methods logistic regression, ridge regression, and regressions with LASSO (least absolute shrinkage and selection operator) and compared them with the machine learning methodologies neural networks, naïve-Bayes, tree-augmented naïve-Bayes, support vector machines, random forest, and stochastic gradient boosting. Receiver operating characteristic curves and C-statistics were calculated for each model. C-Statistics were used for comparison of discriminatory capacity across models in the validation sample. After identifying 56,477 patients, the major univariate predictors of 1-year survival after heart transplantation were consistent with earlier reports and included age, renal function, body mass index, liver function tests, and hemodynamics. Advanced analytic models demonstrated similarly modest discrimination capabilities compared with traditional models (C-statistic ≤0.66, all). The neural network model demonstrated the highest C-statistic (0.66) but this was only slightly superior to the simple logistic regression, ridge regression, and regression with LASSO models (C-statistic = 0.65, all). Discrimination did not vary significantly across the 3 historically important time periods.

Conclusions: The use of advanced analytic algorithms did not improve prediction of 1-year survival from heart transplant compared with more traditional prediction models. The prognostic abilities of machine learning techniques may be limited by quality of the clinical dataset.","We found that the prediction of 1-year outcomes after cardiac transplantation was similar between machine learning and traditional statistical methods in the central repository of patients undergoing heart transplantation in the United States. All of the models developed in this study showed similar and very modest discrimination, with C-statistics consistently ∼0.65 regardless of their complexity. A traditional statistical approach consisting of multivariate logistic regression has been previously used on the UNOS database for predicting 1-year mortality with a C-statistic of 0.65, consistent with our results. Specifically, our findings replicate the predictive capabilities of the IMPACT score.9 Although machine learning has been touted as a path to unearthing nonlinear relationships and higher-dimensional associations between variables in medicine, it remains to live up to its expectations.12 Our results raise the notion that large clinical datasets might lack the accuracy and granularity needed for machine learning methodologies to uncover unique associations.

Similar limitations in the application of machine learning methods to large datasets of patient information have been demonstrated for prediction of heart failure readmissions.2, 13 However, these methods have performed extremely well when applied to imaging information, as noted in recent reports involving head computerized tomography and echocardiography, with C-statistics >0.90.8, 14 We think that our results, when considered in conjunction with these previous findings, provide an insight into the inability of machine learning methods to overcome key systemic limitations of clinical datasets. In this case, whereas UNOS is a robust clinical registry, it is based on administrative data, which can blunt phenotyping of complex patients and attenuate the predictive ability of both traditional methods as well as machine learning techniques; indeed, this has been noted several times in efforts aimed at extracting meaningful information from the EHR.15 Our analysis illustrates this limitation as we see the AUC for neural networks plateau after inclusion of ∼25 variables in the model.

Several limitations of this conclusion must be considered. The patient journey after heart transplantation is very complex, and 1-year outcomes are likely to be determined by events after transplantation. It is very likely that detailed patient information after the surgery would significantly improve our ability to predict outcomes. However, pre-transplantation prognostication is given an inordinate amount of attention during decision making for listing and transplanting patients.16

To our knowledge, the present study is the first to compare different predictive models in patients undergoing cardiac transplantation. We demonstrated that the neural networks model demonstrated the highest discrimination and the most reliable C-statistic when validated. However, its calibration was inferior to all of the traditional models. This is a common problem with advanced methodologies, wherein algorithms can become unstable due to multicollinear predictors or overfitting due to random correlations. Future work is required to understand the best use of specific statistical methodologies to apply to clinical datasets of different characteristics, and consensus is needed on how to validate the resultant findings.17
",https://www.sciencedirect.com/science/article/pii/S1071916418308601
Briceño et al. 2014,Use of artificial intelligence as an innovative donor-recipient matching model for liver transplantation: Results from a multicenter Spanish study,"Background & Aims: There is an increasing discrepancy between
the number of potential liver graft recipients and the number of
organs available. Organ allocation should follow the concept of
benefit of survival, avoiding human-innate subjectivity. The aim
of this study is to use artificial-neural-networks (ANNs) for
donor-recipient (D-R) matching in liver transplantation (LT) and
to compare its accuracy with validated scores (MELD, D-MELD,
DRI, P-SOFT, SOFT, and BAR) of graft survival.
Methods: 64 donor and recipient variables from a set of 1003 LTs
from a multicenter study including 11 Spanish centres were
included. For each D-R pair, common statistics (simple and multiple regression models) and ANN formulae for two non-complementary probability-models of 3-month graft-survival and -loss
were calculated: a positive-survival (NN-CCR) and a negative-loss
(NN-MS) model. The NN models were obtained by using the Neural Net Evolutionary Programming (NNEP) algorithm. Additionally, receiver-operating-curves (ROC) were performed to
validate ANNs against other scores.
Results: Optimal results for NN-CCR and NN-MS models were
obtained, with the best performance in predicting the probability
of graft-survival (90.79%) and -loss (71.42%) for each D-R pair,
significantly improving results from multiple regressions. ROC
curves for 3-months graft-survival and –loss predictions were
significantly more accurate for ANN than for other scores in both
NN-CCR (AUROC-ANN = 0.80 vs. –MELD = 0.50; -D-MELD = 0.54; -
P-SOFT = 0.54; -SOFT = 0.55; –BAR = 0.67 and -DRI = 0.42) and
NN-MS (AUROC-ANN = 0.82 vs. –MELD = 0.41; -D-MELD = 0.47;
-P-SOFT = 0.43; -SOFT = 0.57, -BAR = 0.61 and -DRI = 0.48).
Conclusions: ANNs may be considered a powerful decision-making technology for this dataset, optimizing the principles of justice, efficiency and equity. This may be a useful tool for
predicting the 3-month outcome and a potential research area
for future D-R matching models.","This study is a clear contribution to consider the potential role of
ANNs as a valuable tool for organ allocation in order to obtain the
best benefit of survival. In the current scenario of graft scarcity
and waiting list deaths, the absence of a definitive and objective
system for liver-donor assignment is unacceptable.
Artificial neural networks are inherent complex computational tools which have been successfully used in biomedical
models [20–24]. ANNs are gaining acceptance in current medical
evidence (240 manuscripts published in 2011). However, only
nine manuscripts have been reported regarding LT. Of them,
one was published in 1994, when MELD score and D-R marching
were unknown concepts [25]. Another three were just methodological papers [24,26,27]. Four recent manuscripts used ANNs to predict pre-transplant end-stage liver disease mortality [28–30]
or post-transplant probability of acute rejection [31]. Only one
manuscript has used ANNs for D-R matching but Haydon et al.
used self-organizing maps (a simplified model of ANNs) and collected data from 1993 to 2002 (pre-MELD era) for further validation [32].
The huge advantage that ANNs may provide over other metrics is that they are not only static formulae; they are methods
of calculation that can be applied to every population as they
are trained and validated inside the population. Another advantage is that the more variables they have, the more effective they
will be, thus letting us consider parameters that, in the context of
a donor offer may not be adequately calibrated. Although the use
of ANNs in biomedicine as an alternative to other classification
methods is increasing, probably the easier use of common statistics for the medical staff could be a limitation. However, this may
not lead easy multivariate models to prevail over ANNs that are
more rigorous and validated tools [33,34]. The likelihood that
D-R allocations may be guided/assisted by ANNs is currently
unpredictable but also inspiring and provocative in order to avoid
the four common limitations that manuscripts in current literature analysing models of liver post-transplant share: first, none
of them gives a proper global view of donor and recipient status,
as just few variables are considered; second, variables from different countries may not be useful in others, decreasing the
potential of these models to be exported; third, common statistics are based only on positive models that achieve acceptable
rates of prediction capabilities for graft survival, but not for graft loss, a concept that is extremely important in artificial intelligence; and fourth, some of these scores have not been validated
in different populations. A key point for the proposal of ANNs as a
reliable tool for D-R matching processes is the fact that ANNs do
not behave as static scores; they are dynamic. It means that for a
specific population the ANN will learn from itself improving caseby-case its results. In this sense, we offer the potential of a methodology that, in the scenario of a future multicentre validation,
would be useful for all countries, with their own peculiarities.
Probably the future will rest on complex software available for
surgeons and clinicians that may guide the best decision to be
taken in the context of a donor offer.
Current available predictive models have different limitations: DRI [3,35] does not consider any contribution from recipient factors. Considering recipient status, MELD score [36,37] is
the basis of the allocation policies [38] but is a poor predictor
of the 3-month mortality following LT [39]. In an interesting
attempt of donor-recipient combination, Halldorson et al. [16]
and Rana et al. [6] reported D-MELD and SOFT scores. Unfortunately, both scores have not been designed for donor-recipient
matching and have not been validated probably due to the
appearance of several confounding factors. Survival-benefit as a
function of candidate disease severity and donor quality is a modern and interesting concept as an alternative to current urgencybased allocation [5] in terms of efficiency and equity [40]. However, as suggested by Schaubel et al., [5] unmeasured recipient
and donor characteristics could potentially confound the results.
Based on the ANN, for each D-R pair two probabilities have
been analysed: the probability of having a surviving organ (accuracy), and the probability of having a non-surviving organ (minimum-sensitivity). Systems based on logistic regression can
adequately classify the majority class (favourable event), but
their ability to predict the minority class (adverse event) is poor
[41]. DRI, MELD, D-MELD, SOFT, and BAR scores are all based on
logistic regression analysis and accept the ‘‘linearity’’ between
donor, graft and/or recipient factors and the survival function.
Unfortunately, as in many complex decision-making proceedings,
comprehensive biological phenomena follow a nonlinear pattern,
and linear regression analysis is a too simple approach.
In this report, the best model for predicting the probability of
graft survival for each D-R pair, reaches a 90.79% Correct Classification Rate in the generalization set. To complement this result,
NN-MS reaches a 71.42% level for graft loss prediction. The power
obtained from the combination of these two models is much
higher than the one obtained by other tools and performs such
a complex net of connections that the human mind is not able
to perform. Obtaining two models committed to maximizing
CCR and MS measures allows us to consider the information they
provide as a whole to make a D-R match. In all cases, the probabilities of belonging to the graft-survival and graft-loss groups are
heavily improved by the ANN, compared with common statistics.
It must be remarked that the ANN is a linear combination of sigmoid functions, whilst the rest are linear models in the independent (or input) variables.
In order to give strength to our analysis, MELD, DRI, D-MELD,
SOFT, and BAR scores were obtained from our database and AUROCs were performed. Some of them achieved acceptable predictive areas for the positive event, but they definitely dropped in
the negative event. Scores that only consider isolated donor or
recipient variables had lower areas (MELD and DRI) than those
combining both variables (SOFT, P-SOFT, and BAR) but always lower than those of ANNs. In this sense, the overlapping of variables and the fact that some of them are all-or-nothing scores are
pitfalls that ANNs overcome due to their mathematical strength
[42].
Although powerful, the main limitation inherent to our
research is that data have been obtained from observational databases. Although the number of patients collected is high, and the
period selected favours its homogeneity (just two years from a
clear post-MELD era 2007–08), a prospective or an external validation would be interesting to clearly test the accuracy of ANNs
in other populations. It could also be speculated that with more
than 55 variables and only 1000 patients, these models are at
high risk of overfitting; however, this may be true for usual statistics but not for neural networks, which are advanced computational tools that have a self-learning process that solves this
problem. Another limitation is that our study has a limited 3-
month graft survival end point and maybe other end points and
variables could have been included, such as patient survival, 6-
and 12-month analyses, immunosuppression, infections, or
intent-to-treat survival according to risk on the waiting list. The
previous may be of interest, but the impact of pure D-R matching
on outcome may be biased on mid/long-term analyses. Similarly,
post-transplant variables, may also be of interest, but they would
be contrary to our aim of obtaining a pure pre-transplant allocation model for D-R matching. We are also aware that most of the
scores that are being compared here were not designed to predict
3-month graft outcomes; however, they have been validated in
several populations and, although interesting, their application
is limited as they do not offer a donor-recipient combination that
fulfils the two main rules we have considered in our study:
improved post-transplant survival with MELD-driven decision
in case of a draw. It is our aim to settle the basis of a potential
external validation with other non-Spanish centres and establish
a further prospective parallel computer-guided model that could
simulate the results obtained from our ANN and could compare
them with real on-time allocation.
Our manuscript provides a robust and probably the highest
evidence to the most controversial issue in transplantation:
how can we achieve the best post-transplant outcomes analysing
the global scenario of donor and recipient variables? This methodology combines the best of the MELD score with the best of
the survival-benefit score by using ANNs, resulting in a more
objective approach in terms of equity, utility and efficiency. Furthermore, our model avoids the individualistic vision of the survival benefit theories to provide the best survival for all potential
recipients with a similar MELD score. To make it stronger and
more adequate to the current knowledge, in case of similar
results, the rule-based system allocates the organ to the patient
with the highest MELD score. Complex mathematical calculi
and computational intelligence nowadays guide several common
life procedures, by improving simple statistic calculi. So many
complex interactions should not be calculated by human, as the
potential to fail is high. In a moment of donor scarcity, we should
not only look for the lowest rate of death on the waiting list, but
also for the optimal post-transplant outcome. Our analysis does
not give a simple formula; it offers a methodology that can be
exported to every liver transplant program worldwide leading
to improved outcomes and optimized D-R matching. The ANN
method may be useful for predicting 3-month outcomes; considering the limitations of our study and with the perspective of
future external validation, ANNs could become a potentially useful area of research and maybe the best method to combine
donor, recipient and transplant variables, in order to obtain
optimal survival.",https://www.sciencedirect.com/science/article/pii/S0168827814003870
Ayllón et al. 2018,Validation of artificial neural networks as a methodology for donor-recipient matching for liver transplantation,"In 2014, we reported a model for donor-recipient (D-R) matching in liver transplantation (LT) based on artificial neural networks (ANNs) from a Spanish multicenter study (Model for Allocation of Donor and Recipient in España [MADR-E]). The aim is to test the ANN-based methodology in a different European health care system in order to validate it. An ANN model was designed using a cohort of patients from King's College Hospital (KCH; n = 822). The ANN was trained and tested using KCH pairs for both 3- and 12-month survival models. End points were probability of graft survival (correct classification rate [CCR]) and nonsurvival (minimum sensitivity [MS]). The final model is a rule-based system for facilitating the decision about the most appropriate D-R matching. Models designed for KCH had excellent prediction capabilities for both 3 months (CCR-area under the curve [AUC] = 0.94; MS-AUC = 0.94) and 12 months (CCR-AUC = 0.78; MS-AUC = 0.82), almost 15% higher than the best obtained by other known scores such as Model for End-Stage Liver Disease and balance of risk. Moreover, these results improve the previously reported ones in the multicentric MADR-E database. In conclusion, the use of ANN for D-R matching in LT in other health care systems achieved excellent prediction capabilities supporting the validation of these tools. It should be considered as the most advanced, objective, and useful tool to date for the management of waiting lists. Liver Transplantation 24 192-203 2018 AASLD.","The  management  of  waiting  lists  for  liver  transplantation  is  not  an  easy task.  The  number  of  candidates  continues  to increase  and  could  be  even  higher  if expanded indications such as colorectal liver metastases and extended criteria for HCC  and  cholangiocarcinoma  were  accepted  for  inclusion  on  the  waiting  list.  The medical community needs a tool that could combine three features: objectivity (to avoid  human  subjectivity  in  the  management  of  waiting  list),  optimization  (to achieve highest post-transplant survival rates) and justice (to give the chance to be transplanted  with  advanced  disease).  Besides,  this  tool  should  be flexible  and adapt   to   most   of   the   allocation   systems   in   all   countries   with   their   own peculiarities. We developed models entitled MADR (“madre” in Spanish means“mother”, that  helps  conceive  these  models  as  a  creative  tool  that  generates 
16multiple  individualand  unique  models). Our  findings  confirm  that  the  best  D-R matching  systemto  date  would  be  an  ANN-guided  system  trained,  tested  and optimized for each healthcare system.Several  systems  have  been  proposed  for  D-R  matching  in  LT.  All  of  them have been built using regression models or statistical findings. They work well and highlightthe  complexity  of  donor  and  recipient  matching.  None  have  emerged  as  of value in healthcare systems worldwide. Reasons for this include their heterogeneity, the different variables used and that some of them are all-or-none systems in which only a small  number  of  patients  can  be  discriminated.  Further  disadvantage  is  that  access  to transplant  may  not  be  equally  guaranteed  in  special  indications  such  as  recurrent encephalopathy or refractory ascites [9].Our  MADR-E  worked  well  in  the  Spanish  database  grouping  data  from several  centres.  The  development  of  an  ANN  for  KCH  (MADR-KCH)  had  excellent prediction capabilities which was even better than the original MADR-E. Validation of ANN as a tool for optimal D-R matching is supported by our  findings. However, exporting  the  MADR-E  ANN  to  KCH  was  unsatisfactory.  The  explanation  for  this would  include  that  ANN  is  built  for  a  different  healthcare  system  and  that  input variables  are  absolutely  different  (different  donors,  indications,  race  proportions, ...). Therefore each ANN model utilised worldwide is trained for a specific purpose in a single distinctive population. The 1-year MADR-KCH ANN was also notable. In that, the model was useful, but was less accurate than the 3-months model. This is probably because D-R interactions may not have such a direct impact on mid-term survival as they do on short-term outcomes.
17The  development  of  a  preliminary  MADR-Eu  is  interesting  and  the  results are promising. By grouping all the populations and training/testing them, excellent prediction  rates  for  graft  survival  and  non-survival  were  achieved,  which  were higher than with other scores. This potential utility needs to be evaluated with amuch  larger population  using  large  multinational  databases.  It  is  an  attractive prospect  to  think  that  it  may  be  possible  to  find  a  unique  ANN  that  may  be exportable  to  liver  transplant  programs  in  every  country.  However,  it  should  be possible  for  each  transplant  program  to  analyse  their  data  by  building  their  own ANN   and   generating   specific   D-R   matching   software.   Unfortunately,   medical records are not as accurate as they should be and databases do not equally work in every hospital. ANN could potentially work better if they could be developed using previously  recorded  data  with  no  missing  values.  A  potential  area  of  research would   be   to   prospectively   build   an   ANN   using   hundreds   of   pre-transplant variables and hundreds of post-transplant variables.Conventional  regression  analyses  use  historical  data  and  try  to  fit  them  to some  function.  The  drawback  here  is  the  difficulty  of  selecting  an  appropriate function   capable   of   capturing   all   forms   of   data   relationships   as   well   as automatically  modifying  output  in  case  of  additional  information,  because  the performance   of   a   candidate   is   influenced   by   a   number   of   factors,   and   this influence/relationship   is   not   likely   to   be   represented   by   a   simple   known regression model. An ANN, which imitates the human brain in problem solving, is a more  general  approach  that  can  handle  this  type  of  problem  by  adapting  itself, learning from every candidate and modifying with every situation.
18Artificial  neural  networks  are  complex  tools.  They  can  predict  several important  situations  from  which  the  life  of  human  populations  is  decided.  For example, during emergencies such as flood and drought seasons, reservoirs act as defence  mechanisms  to  reduce  the  risk  of  flooding  and  to  maintain  water  supply. During  this  period,  decision  regarding  water  release  iscritical  [15].  Anotherexample  is  the  prediction  of  water  levels  at  Kainji  Dam,  which  supplies  water  to Nigeria’s  largest  hydropower  generation  station  in  which  ANN  were  built  to generate  a  more  efficient  power  supply  [16].  All  the  models  used  hundreds  ofvariables  recorded  daily  for  several  years  to  build  extremely  accurate  tools  that have led to excellent prediction capabilities not reachable by the human mind and far  from  simplistic  common  statistical  models.  Nowadays,  a  huge  number  of processes   worldwide   are   predicted,   controlled   and   guided   by   ANN.   All   are specifically  designed  for  each  individual  process.  For  example,  the  ANN  designed for  the  forecast  prediction  of  one  is  not  the  same  for  another,  or  the  one  that controls  variables  affecting  flight  status  of  one  type  of  plane  is  not  usable  for another.It is extremely difficult to predict every human behaviour and every human medical  process.  But  it  is  more  complex  to  modify  human  feelings  and  to  adapt them   to   artificial   intelligence.   Even   the   most   sophisticated   robot   may   not accurately   consider   individual   factors   and   ethical   issues   are   unlikely   to   be accurately  modelled.  For  example,  an  ANN  would  be  unlikely  to  consider  an adequate  donor  for  a  third-graft  recipient  that  developed  a  primary  graft  non-function  in  his  first  transplant  and  a  late  arterial  thrombosis  after  his  second transplant.  The  likelihood  is  that  this  recipient  would  never  be  allocated  a  donor,  due  to  the  poor  outcomes.  The  solution  would  be  to  create  a  specific  ANN  for retransplants  or to  bypass  the  ANN  solution  generating  a  mixed  model  in  which artificial intelligence and human factors would coexist. The  research  field  that  our  group  has  started  is  at  the  very  beginning.Leaving the decision of who will get a graft and who will not and thus, who will dieto  software  will  not  satisfy  everyone.  But  there  are  now  many  examples  such  as plague  control,  flight  behaviour,  water  level  controls,  dock  openings  or  weather forecasting  that  are  ANN-controlled  and  may  lead  to  the  survival  of  thousandsof people  everyday.  The  medical  community  has  to  explore  the  interface  between human  decisions  and  software-guided  analyses  which  is  moving  in  favour  of complex computational tools. A prospective trial may be the next step to make the transplant  communityconsider  these  tools  and  to  further  apply  the  results  of  our analysis  that  shows  that  ANN  may  accurately  predict  graft  outcomes  and  guide donor-recipient matching decisions in different healthcare systems.",https://pubmed.ncbi.nlm.nih.gov/28921876/
Straw & Wu 2022,Investigating for bias in healthcare algorithms: a sex-stratified analysis of supervised machine learning models in liver disease prediction,"Objectives
The Indian Liver
The Indian Liver Patient Dataset (ILPD) is used extensively to create algorithms that predict liver disease. Given the existing research describing demographic inequities in liver disease diagnosis and management, these algorithms require scrutiny for potential biases. We address this overlooked issue by investigating ILPD models for sex bias.

Methods
Following our literature review of ILPD papers, the models reported in existing studies are recreated and then interrogated for bias. We define four experiments, training on sex-unbalanced/balanced data, with and without feature selection. We build random forests (RFs), support vector machines (SVMs), Gaussian Naïve Bayes and logistic regression (LR) classifiers, running experiments 100 times, reporting average results with SD.

Results
We reproduce published models achieving accuracies of >70% (LR 71.31% (2.37 SD) – SVM 79.40% (2.50 SD)) and demonstrate a previously unobserved performance disparity. Across all classifiers females suffer from a higher false negative rate (FNR). Presently, RF and LR classifiers are reported as the most effective models, yet in our experiments they demonstrate the greatest FNR disparity (RF; −21.02%; LR; −24.07%).

Discussion
We demonstrate a sex disparity that exists in published ILPD classifiers. In practice, the higher FNR for females would manifest as increased rates of missed diagnosis for female patients and a consequent lack of appropriate care. Our study demonstrates that evaluating biases in the initial stages of machine learning can provide insights into inequalities in current clinical practice, reveal pathophysiological differences between the male and females, and can mitigate the digitisation of inequalities into algorithmic systems.

Conclusion
Our findings are important to medical data scientists, clinicians and policy-makers involved in the implementation medical artificial intelligence systems. An awareness of the potential biases of these systems is essential in preventing the digital exacerbation of healthcare inequalities.","In recent years, research has highlighted that medical biases and female under-representation may significantly contribute to differences in healthcare outcomes; in our paper, we have examined how this phenomena may extend into ML.6–8 10 28 We present several key findings:

Model reproduction and demonstration of disparity: We have demonstrated a previously unobserved sex disparity that exists in published ML classifiers based on the ILPD dataset.
Error disparities: Sex disparities in Accuracy and ROC_AUC fluctuate depending on model and the balance between error rates, however, sex differences in specific error rates are consistent. We observe a consistently lower recall and correspondingly higher FNR for females. Of note, RF and LR classifiers are reported as the most effective on the ILPD dataset, however, these models demonstrate the greatest disparity in the FNR when trained on the original dataset (RF, FNR disparity −21.02% (p<0.05); LR, FNR disparity −24.07%, (p<0.05)). Clinically, this FNR disparity would materialise as an inequality in disease detection that negatively impacts females, with higher instances of missed disease.
Balanced training: Training on sex-balanced data improved overall performance for all classifiers. In the case of the LR classifier, accuracy improves overall and for the sexes separately, indicating that with the right model selection addressing poor performance for the under-represented group does not need to come at the expense of the majority group.
Impact of model architecture on disparity: Our experimental outcomes were not consistent across models, indicating that bias mitigation techniques may need to be tailored to model choice.
Analysis of feature ranking: Our comparison of feature importance reinforces existing clinical research that highlights the sex differences in the role of liver biomarkers.
Implications for data science
Our experiments demonstrated that sex-specific feature selection and addressing under-representation of females may be an important bias mitigation technique when developing ML algorithms in medicine. Furthermore, we illustrate that there is no consistent solution across all classifiers, suggesting techniques need to be tailored to model choice. ML models also present novel opportunities for improving existing practice and addressing health disparities that relate to biochemical discrepancies between the sexes. Given the evolving evidence that critiques the use of ‘unisex’ biochemical thresholds, ML models that do not rely on these defined thresholds may pose a superior alternative if developed with an awareness of the subtle sex differences in disease manifestation.

Implications for clinical medicine and public health
Classification algorithms are being increasingly used in healthcare settings to assist clinicians in medical diagnosis.20 Unless these algorithms are evaluated for biases, they may only improve care for a subset of patients and consequently increase healthcare inequalities.7 By evaluating ML models for demographic biases before they are implemented in digital medicine, we can mitigate the perpetuation of these inequalities into digital systems.

Furthermore, insights from model development can be used to inform current clinical care. Our data exploration of feature correlation demonstrated sex differences in feature importance. Such research can inform practising clinicians on the relevance of different indicators for the patient in front of them, for example, albumin may be more indicative of pathology in males.11 Lastly, examining disparities in algorithmic performance offers an opportunity to reflect on which patients may be being missed in current practice. Throughout our analysis, we demonstrated a persistently high FNR for females, suggesting that female disease is at risk of being overlooked. Examining the physiological profile of algorithmic false negatives presents an opportunity to better understand which patients are at risk of being misdiagnosed.

It should be noted that the ILPD does not include demographic information on race or ethnicity.22 Racial biases have been reported in the biochemical tests used across different subspecialties, resulting in worse care for marginalised racial groups.29 30 A key limitation of our study is that we cannot perform a race stratified analysis. Furthermore, we are unable to evaluate the relevance of other demographic features. An intersectional approach to healthcare inequalities would consider the mediating impact of socioeconomic class, or the compounding impact of gender (as opposed to sex) and sexuality on marginalised patients. Accounting for the complex nature of these intersectional relationships requires more advanced modelling and new bias evaluation techniques.",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9039354/
Wadhwani et al. 2019,Predicting ideal outcome after pediatric liver transplantation: An exploratory study using machine learning analyses to leverage Studies of Pediatric Liver Transplantation (SPLIT) data,"Machine learning analyses allow for the consideration of numerous variables in order to accommodate complex relationships that would not otherwise be apparent in traditional statistical methods to better classify patient risk. The Studies of Pediatric Liver Transplantation (SPLIT) registry data was analyzed to determine whether baseline demographic factors and clinical/biochemical factors in the first year post-transplant could predict ideal outcome at 3 years (IO-3) after liver transplantation (LT). Participants who received their first, isolated LT between 2002–2006 and had follow-up data 3 years post-LT were included. IO-3 was defined as alive at 3 years, normal ALT (<50) or GGT (<50), normal glomerular filtration rate (GFR), no non-liver transplants, no cytopenias and no post-transplant lymphoproliferative disease (PTLD). Heat map and random forests analyses (RFA) were used to characterize the impact of baseline and 1 year factors on IO-3. 887/1482 SPLIT participants met inclusion criteria; 334 had IO-3. Demographic, biochemical and clinical variables did not elucidate a visual signal on heat map analysis. RFA identified non-white race (vs. white race), increased length of operation, vascular and biliary complications within 30 days, and duct-to-duct biliary anastomosis to be negatively associated with IO-3. UNOS regions 2 and 5 were also identified as important factors. RFA had an accuracy rate of 0.71 (95% CI: 0.68–0.74); PPV= 0.83, NPV = 0.70. RFA identified participant variables that predicted IO-3. These findings may allow for better risk stratification and personalization of care following pediatric liver transplantation.","As the field of pediatric liver transplantation has evolved, we seek to personalize treatment strategies to optimize outcome and value. As patients move through the pathway of selection, wait list management, peri-transplant and post-transplant, we asked if it was possible to predict who at 1 year will have the IO-3 at 3 years. As a proof of concept and to move the field closer to personalized medicine, we utilized intelligent techniques to leverage available data in the SPLIT database. RFA allows for a completely different evaluation of registry data that is not traditionally employed. While no obvious signal was evident on the heat map, RFA allowed us to identify several variables for predicting IO-3 at 3 years. The RFA classifier had an accuracy of 0.71 which exceeds that of the naïve prediction classifier. Furthermore, accuracy was assessed using the out-of-bag method which supports development of a robust classifier. This methodology, over traditional statistical approaches, allows for the flexible use of multiple variables to aid in classification as opposed to uncovering the specific relationship between predictor and outcome variables, while simultaneously minimizing the risk of overfitting the data.

Notably, non-white race was the variable of most importance and predictive of not attaining the IO-3. This was the variable of most importance despite including a variable for insurance (public or private; a marker of socio-economic status) in the RFA. The reasons for this are likely multi-factorial and may reflect systemic bias (both societal and health system) experienced by minorities across the phases of care. Previous studies in pediatric LT have suggested a disparity in access to care, presentation to care, and waitlist priority for patients of non-white race.19–22 Black adults who received kidney transplants were found to have increased prevalence of cardiac disease pre-transplant which could suggest that comorbidities may contribute to this racial gap.23 Furthermore, this finding could reflect differential socio-economic backgrounds and immunosuppressant adherence rates.24 Our findings support the case for continued attention in racial disparity and equity in access to care for children within our system.

Pre-transplant supplemental feedings and length of operation may reflect the severity of illness prior to transplant. Length of operation has not been previously implicated in affecting ideal outcomes. Increasing length of operation may reflect the stability and health of patient prior to transplantation, graft type, may be associated with previous operations, difficulty in explant hepatectomy, transplant center or likelihood of open abdomen and delayed closure. It is likely a surrogate marker for complexity of patient, but perhaps prior planning could decrease the importance of this variable. Its relationship to overall outcomes is intriguing and introduces the idea of establishing a time threshold or benchmark in order to improve overall outcomes.

Participant’s UNOS region was also found to be predictive of IO-3. UNOS divides the United States into 11 different regions for purposes of organ allocation. Notably, participants from region 2 (Delaware, Maryland, New Jersey, Pennsylvania, West Virginia and Washington DC) were less likely to have the IO and participants from region 5 (Arizona, California, Nevada, Utah and New Mexico) were more likely to have IO. Once again, the reasons for this are likely multifactorial and may reflect organ availability, institutional differences and patient-level characteristics. However, SPLIT does not have equal representation across UNOS regions, and; as highlighted in Table 1, there were differential inclusion/exclusion rates across UNOS regions. Therefore, definitive conclusions cannot be drawn due to selection bias.

The majority of participants without IO-3 had abnormal liver enzymes. Our estimates were based on an ALT and GGT cut-off of 50, which likely under-estimates the incidence of patients with ongoing inflammation.25 This is further supported by data on patients ineligible for participation in a multi-center immunosuppression withdrawal trial due to silent immune-mediated liver injury despite appearing clinically stable.3,26 Kidney injury is the second most frequent complication and likely reflects, at least in part, pre-existing kidney disease, episodes of acute kidney injury and non-immune complications of immunosuppressive medications. These findings further support the need for research strategies that personalize immunosuppression and optimize allograft health without complications.

This study has several limitations. Notably, there were differences in the demographic characteristics of participants who had IO-3 data available and those who did not. This may bias findings from the analyses. Furthermore, there were baseline differences in those who had the IO-3 and those who did not. Specifically, participants without IO-3 were more likely to come from households without intact marriages, be of non-white race and have higher calculated PELD score. There are also limitations to RFA. Finally, limitations common to registry studies such as representative sample, missing data and quality of the data apply to this study. Specifically, the SPLIT registry is not a mandatory reporting database of all LT recipients (like UNOS) so it may not represent the entire population of transplanted U.S. children.

Long-term morbidity is significantly increased for LT recipients who survive the first year compared with age-matched controls in the general population. Pediatric recipients compared to adult recipients face increased risk of morbidity given their potential for longer life expectancy and thereby increased likelihood for longer cumulative exposure to immunosuppression. The challenge remains to ensure optimal allograft health and functional outcomes, while striving to minimize the complications of immunosuppression. Interestingly, in our cohort, only 38% attained IO-3 while Ng, et al4 found that 32% of participants attained IO at 10 years post-transplant. This suggests that patients are at highest risk of morbidity in the few years immediately following transplant. Being able to identify subgroups of pediatric LT recipients who require additional care could unlock targeted interventions for those at highest risk and prevent morbidity such as re-transplantation, given the significant cost estimates of re-transplantations being upwards of $300,000. Conversely, if we can predict from variables at 1 year who is likely to have long term success, it may allow resources to be targeted towards those at higher risk. However, future work is needed to identify what predictive variables are modifiable and whether that affects the long-term outcomes of pediatric LT survivors. This is aligned with the national push for precision medicine and a newer concept--precision public health.27,28 Precision public health “can be simply viewed as providing the right intervention to the right population at the right time”.27 This study sought to use machine learning algorithms to better predict who is at risk for not attaining the IO-3. The authors hope this will catalyze future research that ultimately lead to greater personalization of care for pediatric transplant recipients.",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7980252/
Killian et al. 2023,Prediction of Outcomes After Heart Transplantation in Pediatric Patients Using National Registry Data: Evaluation of Machine Learning Approaches,"Background: The prediction of posttransplant health outcomes for pediatric heart transplantation is critical for risk stratification and high-quality posttransplant care.

Objective: The purpose of this study was to examine the use of machine learning (ML) models to predict rejection and mortality for pediatric heart transplant recipients.

Methods: Various ML models were used to predict rejection and mortality at 1, 3, and 5 years after transplantation in pediatric heart transplant recipients using United Network for Organ Sharing data from 1987 to 2019. The variables used for predicting posttransplant outcomes included donor and recipient as well as medical and social factors. We evaluated 7 ML models-extreme gradient boosting (XGBoost), logistic regression, support vector machine, random forest (RF), stochastic gradient descent, multilayer perceptron, and adaptive boosting (AdaBoost)-as well as a deep learning model with 2 hidden layers with 100 neurons and a rectified linear unit (ReLU) activation function followed by batch normalization for each and a classification head with a softmax activation function. We used 10-fold cross-validation to evaluate model performance. Shapley additive explanations (SHAP) values were calculated to estimate the importance of each variable for prediction.

Results: RF and AdaBoost models were the best-performing algorithms for different prediction windows across outcomes. RF outperformed other ML algorithms in predicting 5 of the 6 outcomes (area under the receiver operating characteristic curve [AUROC] 0.664 and 0.706 for 1-year and 3-year rejection, respectively, and AUROC 0.697, 0.758, and 0.763 for 1-year, 3-year, and 5-year mortality, respectively). AdaBoost achieved the best performance for prediction of 5-year rejection (AUROC 0.705).

Conclusions: This study demonstrates the comparative utility of ML approaches for modeling posttransplant health outcomes using registry data. ML approaches can identify unique risk factors and their complex relationship with outcomes, thereby identifying patients considered to be at risk and informing the transplant community about the potential of these innovative approaches to improve pediatric care after heart transplantation. Future studies are required to translate the information derived from prediction models to optimize counseling, clinical care, and decision-making within pediatric organ transplant centers.
","Principal Findings
In this study, we compared 7 ML models and 1 DL model and examined their ability to predict rejection and mortality 1, 3, and 5 years after pediatric heart transplantation. There has been increasing use of advanced mathematical modeling using large data sets to predict outcomes in pediatric transplantation [10-12]. However, despite initial experience, much work needs to be done to further evaluate and refine the best strategies and modeling techniques to optimally use these methods for advancing clinical care. In this study, RF, XGBoost, and AdaBoost demonstrated the highest AUROC values throughout the posttransplant outcomes across the 3 observation windows. As a decision tree–based ensemble ML algorithm, RF has been shown to yield the best performance in many other studies on small, tabulated data sets, which is also the case in our study. A possible reason is that RF generally performs well when the data set has a mix of categorical and numeric features; in addition, RF is less influenced by outliers than other algorithms. Nonetheless, based on best practice in ML modeling, one would need to experiment with multiple ML algorithms on a particular data set to see which ML model works best. In our study, when AUPRC was used as the primary performance measure, XGBoost outperformed other models in 3 of the 6 outcomes and yielded slightly better performance than RF. The NN slightly outperformed other models in 2 outcomes. Most importantly, the use of SHAP values to evaluate the relative importance of predictors in these models adds to the clinical interpretability, utility, and potential translation into clinical care. We also observed that the DL model consistently had worse performance than the ML algorithms, which may be related to the small amount of data available because, empirically, DL models perform better with a large number of data points. This can also suggest that DL modeling in this clinical scenario may not be the most appropriate strategy. This finding is also consistent with our previous analysis, which used data from a single transplant center in the southwestern United States [15]. However, further research is needed to validate this conclusion.

The results from this modeling demonstrate the important challenges of using registry and administrative data to model adverse medical events during posttransplant care of pediatric HT recipients. Prior research and modeling of posttransplant data in pediatric care similarly found poor-to-fair predictive utility and sensitivity using classification and regression trees, RF, and artificial NN approaches [10-12]. Previous research using RF has identified key factors in predicting ideal posttransplant outcomes 3 years after liver transplantation [10]. However, results from ML models in pediatric transplantation across kidney, liver, and heart recipients from 1 center were similarly suboptimal [15]. In adult populations, predictive validity with ML approaches has not achieved encouraging results [28,36-43]. Many of these studies have focused only on mortality in adult HT recipients, offering little insight for pediatric transplant teams managing instances of other important outcomes such as rejection in a much more heterogenous population. Despite the UNOS being the largest registry of data for pediatric transplant patients, there are inherent data quality issues that may limit the optimal use of these analytical approaches. Therefore, urgent efforts are needed to improve quality of data entry and reduce the amount of missing data.

Model Interpretation
SHAP values [34] were used in this study to provide greater interpretability of the results and to quantify the relative influence of individual variables within these models. Our data highlight the importance of graft status immediately after transplantation as being a salient predictor in nearly every model. Graft function immediately after transplantation is affected by a complex interplay of donor, preservation, recipient, and perioperative factors. These factors are unique in individual patients; however, the presence of suboptimal graft function immediately after transplantation is a strong predictor of 1-, 3-, and 5-year rejection and mortality. This observation does not necessarily change clinical management currently; however, it highlights the importance of in-depth evaluation and optimization of donor, recipient, and transplantation factors, which can influence graft function and the strength of its influence on important clinical outcomes; for example, donor myocardial function, ischemic time, and sensitization are a few factors that can influence graft function after transplantation. Other factors such as pretransplant use of ventricular assist devices and mechanical ventilation are important factors in predicting clinical outcomes as well. Furthermore, liver or kidney dysfunction and being listed as status 1A, all of which can be considered surrogate markers for a patient who is sicker, have important predictive influence on the outcomes. Various donor factors such as weight, height, and BMI, as well as recipient-to-donor weight ratio, influenced the predictive models. We hypothesize that these factors were likely related to the smaller children who are more likely to have CHD and, in addition, may have a larger impact owing to the donor-recipient size discrepancy in thoracic cavity. Likewise, other factors such as pretransplant medical factors, including the number of prior cardiac surgeries and a diagnosis of CHD, were important predictors across various models and outcomes. Previous studies have shown that a single-ventricle physiology secondary to hypoplastic left heart syndrome influences outcomes; however, this was not the case in our study. In addition, longer waitlist duration likely secondary to medical or surgical factors, such as organ dysfunction, human leukocyte antigen sensitization or mismatch, and the need for other procedures were important factors in the predictive models. These medical factors have been similarly identified in prior research using ML approaches in other transplantation data, including those of adult populations [15,28,41-43]. Patient social factors predicting outcomes across the time frames in this study included age, ethnicity, level of education, and sex, which have been reported as important predictors in prior research [15,28,41-43]. Female and adolescent patients have been shown to be at greater risk for rejection episodes [44-46] and mortality than male or younger patients [47-51]. Our study also highlighted that recipient ethnicity was an important predictor for 5-year mortality. Obviously, it is difficult to predict why that is the case, but it does call for a need to further understand the complex interplay of various psychosocial factors.

Improving Future Modeling
Our modeling efforts build on prior studies through the inclusion of posttransplant data through subsequent observation windows using TRF data. Despite this, posttransplant health outcomes for children and adolescents remain challenging to predict with better-than-modest accuracy. The UNOS data constitute a large and valuable registry of transplant patients nationally, yet this administrative database as is may not be optimal for prediction of specific posttransplant health outcomes owing to the lack of granularity at important clinical time points [43]. Importantly, these data sets also lack important data collected on psychological, social, and environmental factors, which can help predict long-term outcomes. In addition to medical factors, psychosocial variables and family functioning are well-known to influence outcomes [52-54]. Usually, psychosocial variables and family functioning are not well represented in these databases, limiting an important aspect of care, which affects opportunities for effective predictive modeling. Despite the importance of psychological and social determinants of posttransplant pediatric heart transplantation outcomes, these valuable data are not available in the UNOS database or in similar transplant data sets, such as the Studies of Pediatric Liver Transplantation [55] and Scientific Registry of Transplant Recipients [56] databases. The absence of such parameters can likely affect the predictive ability of these models; for example, previously, UNOS data captured physician- or transplant team–reported nonadherence (UNOS variable: recipient noncompliant during this follow-up period [PX_NCOMPLIANT]), but this variable has been excluded from TRF forms since 2007. Although physician proxy reports, reports, or opinion of patient medication adherence have inherent measurement issues [13], the lack of this critical predictor from these data sets and our inability to include these in modeling algorithms is a major loss in predictive utility, especially because of the known strong association between medication nonadherence and numerous posttransplant outcomes [2-5,50,57,58]. To overcome these limitations, the inclusion of granular longitudinal structured and unstructured clinical and psychosocial data within the patient EHR (eg, text from clinical notes) using these advanced analytical methods is the next step to refine the modeling algorithms, thereby increasing chances of better predictive capability.

Limitations
This study has several limitations, including the inherent ones related to the use of database and registry data; for example, all rejections were treated as though they were of the same grade. In this work, we treated the 3 outcomes independently, although 1 outcome may in fact be a cause of another. Nonetheless, we built different models for different outcomes. In future work, we will build multiclass models with different combinations of outcomes as the prediction outcome. In this work, we grouped together patients in the UNOS database from 1987 to 2019. In future work, we will account for era and changes in clinical practice and ways to determine outcomes. This work aims to demonstrate the promise and limitations of using ML compared with using registry data in predicting posttransplantation outcomes in pediatric recipients. Because of the number of models and algorithms we evaluated, we used default parameters for the ML algorithms. With further hyperparameter tuning, we may be able to further improve the prediction performance of these models. We also converted categorical variables to numeric variables when building the prediction models. Another approach would have been to use a one-hot coding scheme for all categorical variables. However, because of the small sample size, number of categorical variables, and number of categories in these variables, one-hot coding would have resulted in a very sparse data set. Nonetheless, we created one-hot variables for 8 important diagnoses for transplantation outcome prediction.",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10334720/
Thongprayoon et al. 2023,Distinct Phenotypes of Non-Citizen Kidney Transplant Recipients in the United States by Machine Learning Consensus Clustering,"Better understanding of the different phenotypes/subgroups of non-U.S. citizen kidney transplant recipients may help the transplant community to identify strategies that improve outcomes among non-U.S. citizen kidney transplant recipients. This study aimed to cluster non-U.S. citizen kidney transplant recipients using an unsupervised machine learning approach; Methods: We conducted a consensus cluster analysis based on recipient-, donor-, and transplant- related characteristics in non-U.S. citizen kidney transplant recipients in the United States from 2010 to 2019 in the OPTN/UNOS database using recipient, donor, and transplant-related characteristics. Each cluster's key characteristics were identified using the standardized mean difference. Post-transplant outcomes were compared among the clusters; Results: Consensus cluster analysis was performed in 11,300 non-U.S. citizen kidney transplant recipients and identified two distinct clusters best representing clinical characteristics. Cluster 1 patients were notable for young age, preemptive kidney transplant or dialysis duration of less than 1 year, working income, private insurance, non-hypertensive donors, and Hispanic living donors with a low number of HLA mismatch. In contrast, cluster 2 patients were characterized by non-ECD deceased donors with KDPI <85%. Consequently, cluster 1 patients had reduced cold ischemia time, lower proportion of machine-perfused kidneys, and lower incidence of delayed graft function after kidney transplant. Cluster 2 had higher 5-year death-censored graft failure (5.2% vs. 9.8%; p < 0.001), patient death (3.4% vs. 11.4%; p < 0.001), but similar one-year acute rejection (4.7% vs. 4.9%; p = 0.63), compared to cluster 1; Conclusions: Machine learning clustering approach successfully identified two clusters among non-U.S. citizen kidney transplant recipients with distinct phenotypes that were associated with different outcomes, including allograft loss and patient survival. These findings underscore the need for individualized care for non-U.S. citizen kidney transplant recipients.
","Our unsupervised ML approach identified two clinically distinct clusters of non-U.S. citizen kidney transplant recipients with differing post-transplant outcomes. Cluster 1 patients, accounting for 28.5% of non-U.S. citizen kidney transplant recipients, were featured by young age recipients receiving preemptive kidney transplant or a short dialysis duration of less than one year. Patients in cluster 1 had working incomes and private insurance. Most donors in cluster 1 were non-hypertensive Hispanic living donors with a low number of HLA mismatches. In contrast, cluster 2 patients, accounting for 71.5% of non-U.S. citizen kidney transplant recipients, were primarily characterized by receipt of non-ECD deceased donor with a KDPI < 85% (89.5% had KDPI < 85%, 9.6% had KDPI ≥ 85%, and only 1.0% had living donors). Kidney transplants in cluster 2 patients had a longer cold ischemia time, higher utilization of machine-perfusion, and a higher incidence of DGF. While cluster 2 patients had a higher degree of HLA mismatches and DGF than cluster 1, they received more thymoglobulin and less steroid-free regimens compared to cluster 1. This may have resulted in the comparable 1-year acute rejection rate between the two groups. Nevertheless, cluster 2 patients had significantly higher 5-year death-censored graft failure and mortality.

A previous study using the U.S. Renal Data was conducted to assess outcomes of adult kidney transplant recipients (years 1990 and 2011) with Medicaid [4]. The investigators demonstrated that kidney transplant recipients, regardless of U.S. citizenship status, had comparable outcomes [4]. These findings would thus be reassuring as to the safety of kidney transplants for non-U.S. citizens. The previous study was limited, however, as it only addressed individuals with public insurance, thus ruling out the majority of non-U.S. citizen recipients in Cluster 1 of our study. Our study provides novel understanding of the phenotypes of non-U.S. citizens regardless of insurance or socioeconomic status.

It is notable that UNOS/OPTN data on non-U.S. citizenship candidate listings were recorded as “non-U.S. citizen/U.S. resident” or “non-U.S. citizen/non-U.S. resident” after March 2012 [3]. Prior to March 2012, non-U.S. citizenship candidates on the waiting list were documented as “resident alien” or “nonresident alien,” and they were not reassigned to the updated status [3]. In our study, we did not exclude patients with “resident alien” or “nonresident alien” status in order to truly capture and represent all non-U.S. citizen kidney transplant recipients in the United States. While the subtype of U.S. citizenship status was not one of the key phenotypes that differentiated the two identified clusters, we found that cluster 2 patients has a higher proportion of resident aliens (also termed permanent resident or a lawful permanent resident) [4]. Non-citizen/non-resident status included both patients that traveled to the U.S. for either the purpose of seeking an organ transplant or reasons other than transplantation [4,10]. Non-citizen/non-resident patients could be foreign students or business people traveling to the U.S, consistent with the findings of recipients with higher education levels and higher working incomes with private insurance. Individuals falling into the non-citizen/non-resident status may also be those who traveled to the United States with a living donor. These socioeconomic factors have been shown to be associated with better graft and recipient outcomes [7]. During the study period, the top five reported countries of citizenship of non-U.S. citizen/non-U.S. resident transplant recipients who traveled to the U.S. for transplant were Kuwait, Qatar, Mexico, Saudi Arabia, and United Arab Emirates. The top five reported countries of citizenship of non-U.S. citizen/non-U.S. resident recipients who traveled to the U.S. for other reasons were Mexico, El Salvador, India, Guatemala, and Chile (Table S1). While our study demonstrated a higher proportion of non-citizen/non-resident patients that traveled to the U.S. for either the purposes of seeking an organ transplant or for reasons other than transplant in cluster 1, as compared to cluster 2, the overall number of non-U.S. citizen/ non-U.S. resident recipients who traveled to the U.S. for a reason other than transplantation was higher in cluster 2 (650 patients (8%)) than cluster 1 (344 patients (11%)).

Our study has several limitations. We used the UNOS database to assess the phenotypes of non-citizen adult kidney transplant recipients in the United States. Thus, the findings of our study are not representative of non-citizen kidney transplant recipients in other countries [25,26] or pediatric transplant recipients [10,27]. Second, while there have been concerns about kidney transplant outcomes among “undocumented” aliens or residents [3], there are no “illegal” or “undocumented” terms identified in the UNOS database to identify patients who did not have a visa or who had overstayed the duration of their visa. While undocumented immigrants are considered as non-resident aliens [4], the expression “non-resident alien” is broad and also includes individuals granted permission by the U.S. government to enter the U.S. on a temporary basis as a non-immigrant alien for purposes which include tourism, business, education, medical care, or temporary employment. While a number of non-resident aliens have higher education without economic barriers, some others may enter into medical care with few resources, lack of acculturation, minimal health insurance, and little understanding of strategies to navigate the complex healthcare system, all of which may delay access to needed care [2,8]. Given the heterogeneity of non-resident aliens, a ML approach may have unique advantages for identifying distinct phenotypes. As of April 2012, undocumented immigrants are considered as “non-U.S. citizen/U.S. resident” in the updated terminology [3,4,10]. However, the term “non-U.S. citizen/U.S. resident” is still not specific to undocumented immigrants, and this term also includes a permanent resident or a lawful permanent resident. We found a comparable proportion of non-U.S. citizen/U.S. resident status in both cluster 1 and cluster 2. Given no definite identification of undocumented immigrants in the database, future studies to identify phenotypes of these vulnerable groups of patients are needed. Furthermore, kidney transplant recipients undergo rigorous screening and must satisfy specific criteria as part of the selection process. Consequently, the non-U.S. citizen/U.S. resident transplantation population shown here may not reflect the general trends described in other non-U.S. citizen patient populations outside of transplantation [28,29,30].",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10144541/
Rocha et al. 2023,Adaptive Content Tuning of Social Network Digital Health Interventions Using Control Systems Engineering for Precision Public Health: Cluster Randomized Controlled Trial,"Background:Social media has emerged as an effective tool to mitigate preventable and costly health issues with social network interventions (SNIs), but a precision public health approach is still lacking to improve health equity and account for population disparities.

Objective:This study aimed to (1) develop an SNI framework for precision public health using control systems engineering to improve the delivery of digital educational interventions for health behavior change and (2) validate the SNI framework to increase organ donation awareness in California, taking into account underlying population disparities.

Methods:This study developed and tested an SNI framework that uses publicly available data at the ZIP Code Tabulation Area (ZCTA) level to uncover demographic environments using clustering analysis, which is then used to guide digital health interventions using the Meta business platform. The SNI delivered 5 tailored organ donation–related educational contents through Facebook to 4 distinct demographic environments uncovered in California with and without an Adaptive Content Tuning (ACT) mechanism, a novel application of the Proportional Integral Derivative (PID) method, in a cluster randomized trial (CRT) over a 3-month period. The daily number of impressions (ie, exposure to educational content) and clicks (ie, engagement) were measured as a surrogate marker of awareness. A stratified analysis per demographic environment was conducted.

Results:Four main clusters with distinctive sociodemographic characteristics were identified for the state of California. The ACT mechanism significantly increased the overall click rate per 1000 impressions (β=.2187; P<.001), with the highest effect on cluster 1 (β=.3683; P<.001) and the lowest effect on cluster 4 (β=.0936; P=.053). Cluster 1 is mainly composed of a population that is more likely to be rural, White, and have a higher rate of Medicare beneficiaries, while cluster 4 is more likely to be urban, Hispanic, and African American, with a high employment rate without high income and a higher proportion of Medicaid beneficiaries.

Conclusions:The proposed SNI framework, with its ACT mechanism, learns and delivers, in real time, for each distinct subpopulation, the most tailored educational content and establishes a new standard for precision public health to design novel health interventions with the use of social media, automation, and machine learning in a form that is efficient and equitable.","Principal Results
In this work, we proposed an SNI mechanism that uses high-resolution assessments and controllability in adaptive interventions to increase the engagement in organ donation campaigns by tailoring educational content to different population groups. The Meta Business Ads Manager platform was used to deliver the SNI with 3 parameters: the ZCTA randomized for each controlled trial (optimal and nonoptimal), the daily budget for each intervention unit, and the platform optimization goal to increase impressions. The SNI targeted demographic environments in 2 phases: pre- and postoptimization. In the preoptimization phase, the SNI delivered all intervention units with an equal proportion of the daily budget. In the postoptimization phase, the efficiency control mechanism (PID controller) was applied to continually tune the optimization arm of the intervention.

The optimization mechanism developed in this study enabled efficient budget allocation in the optimal intervention arm, resulting in a greater level of engagement per exposure. This has important public health implications as it allows highly specific targeting of educational health content to diverse populations to allow an equitable spread of information to populations in need.

PID Controller Enables Tailored Educational Content per Demographic Environment and Increases Content Efficiency
PID controllers are loop-based control mechanisms used to maintain process variables close to desired set points. The PID controller acts as a physician analyzing the glucose level of a patient with diabetes; if the glucose level goes up, the insulin dosage should increase proportionally, and the opposite would happen if the glucose level decreases. Those mechanisms continuously calculate the differences between the current values of process variables and their respective set points to correct the system parameters concerning proportional, integral, and derivative terms. This methodology allows for the first time to plan, quantify, and optimize in real time public health education campaigns, making them more equitable, efficient, and cost-effective.

Concerning social network
Concerning social network interventions, the PID controller can increase people’s engagement in public health campaigns by tailoring educational content to different population groups. Thus, we need to define an appropriate performance measure to achieve the intervention goals, for example, the number of clicks per impression. In our study, we used organ donation registrations as the educational campaign, but the same can be applied to any other health awareness and education needs.

Cluster Characterization and Content Optimization per Demographic Environment
This study aimed to use systems control theory for ACT in a SNI to promote awareness regarding organ donation. In particular, an ACT mechanism enabled the efficient automation of budget adjustments given each content’s level of engagement.

The population stratification prior to the ACT-enabled content tailoring for each demographic environment separately. In clusters 1 and 2, both rural demographic environments with a predominantly white population, content highlighting a personal story (EdC-Baseline) achieved a greater engagement level. In cluster 3, a demographic environment with high socioeconomic status, both contents highlighting personal stories regarding the relationship between parents and children (EdC-1 and EdC-Baseline) accomplished more engagement than the other contents. In cluster 4, a group with a greater proportion of Hispanic and African American populations, content with dense concepts about organ donation (EdC-3) had higher engagement at the beginning of the postoptimization phase; however, at the end of the intervention, content displaying a personal story attained more engagement. Cluster 4 was the only one that did not present a significant engagement increase in the optimal arm compared with the nonoptimal arm (β=.0936; P=.05), demonstrating the need for more personalized and community-centered content development in addition to the ACT mechanism herein proposed.

We observed that using the PID controller on SNI promoted people’s awareness regarding organ donation since the total number of clicks increased by 15.47% from the nonoptimal arm to the optimal one. According to the regression, an additional 0.2187 (95% CI 0.162–0.276) click rate (C/I) was obtained in the optimal arm during the optimization phase.

The precision public medicine assumption is that the effect of intervention varies across distinct subpopulations. Therefore, we need to unveil such subpopulations and tailor the intervention accordingly. We have demonstrated (Figure 8) that the intervention effect depends on the social determinants of health regarding the underlying subpopulations. We not only uncovered such demographic environments (ie, clusters of zip codes with similar social determinants of health), but also proposed a mechanism to adaptively adjust the intervention (ie, the educational contents) based on how each underlying population responds. Once demographic environments are uncovered, our approach can be naturally extended to populations other than California, including those speaking languages other than English.

Limitations
Even though the study addressed the California state disparities by stratifying the zip codes into distinct and meaningful demographic environments, the digital divide among minorities may still be biasing the results. This study did not assess the intervention’s effect on the number of organ donor registrations in California, which still needs to be tested in an adequately powered study.

Another limitation concerns the PID parametric configuration, that is, the values of proportional, derivative, and integral constants. Finding suitable values for those constants is necessary because they interfere with the convergence rate and fine-tuning of the SNI mechanism. We defined those parameters after a preliminary analysis, which provided an acceptable performance for the mechanism proposed in this manuscript. However, each problem has an optimal set of constants that maximize the PDI’s performance in the respective context. Thus, we could improve the achieved solution by defining those constants through an optimization process driven by the SNI problem. Additionally, the error measure that drives SNI regarded only the difference between engagement and exposure. We could improve the SNI performance by using other relevant variables, such as complete video views, comments, and shares.",https://www.jmir.org/2023/1/e43132/
Cerqueira et al. 2022,How can we predict the kidney graft failure of Portuguese patients?,"Background: The gap between offer
and need for a kidney transplant
(KT) has been increasing. The
Kidney Donor Profile Index (KDPI)
is a measure of “organ quality” and
allows estimation of graft survival,
but could not apply to all populations.
Knowledge of our kidney donor and
recipient population is vital to adjust
transplant strategies. Methods: We
performed a retrospective evaluation
of donors and recipients of KT
regarding two kidney transplant units:
Centro Hospitalar Universitário
de Coimbra, CHUC (Coimbra,
Portugal) and Centro Hospitalar
Universitário de São João, CHUSJ
(Porto, Portugal), between 2013 and
2018. We then did statistical analysis
and modeling, correlating these KT
outcomes with donor and recipient
characteristics, including KDPI.
Artificial intelligence methods were
performed to determine the best
predictors of graft survival. Results:
We analyzed a total of 808 kidney
donors and 829 recipients of KT.
The association between KDPI and
graft dysfunction was only moderate.
The decision tree machine learning
algorithm proved to be better at
predicting graft failure than artificial
neural networks. Multinomial
logistic regression revealed recipient
age as an important prognostic factor
for graft loss. Conclusions: In this
Portuguese cohort, KDPI was not
a good measure of KT survival,
although it correlated with GFR 1
year post-transplant. The decision
tree proved to be the best algorithm
to predict graft failure. Age of the
recipient was the most important
predictor of graft dysfunction.","Our pool of Portuguese donors had a
significantly higher value of KDPI than the USA
cohort (26% had a KDPI > 85% vs 9.2% in the
USA).(5)
Our cohort had a median KDPI of 65%,
similar to that described by other European groups
(Germany with 66%,(1) Spain with 69.4%,(2) and
Ireland with 51%.(3) The number of patients in
the high range of KDPI was also noteworthy.
(Figure 3). Despite this fact, our survival rate at five
years post-transplant was superior to the one
reported by the OPTN (according to data of
February 8th, 2019): 93% vs 74.4%.
Our number of graft dysfunctions and acute
rejections was low. However, our short follow-up
time and the number of patients lost to followup most likely contributed to these results.
In our cohort, there was no association
between the incidence of graft dysfunction
and transplant using kidneys from expanded
criteria donors, a finding similar to what AriasCabrales C et al.
(3) had found. One explanation
for this could be that the majority of data
linking ECD to lower graft survival are based
on data from the US population, which shows
different demographics and characteristics from
the European population and possibly limits
external validation of this data. The medium age of donors of kidneys used
for transplant in our cohort was 51.6 years ± 14.5
(Figure 4). Donors over 60 years represented
less than a third of the total population of kidney
donors (26.4%). In our study, although KDPI correlated with
eGFR post-transplant, there wasn’t a statistically
significant correlation between donor KDPI and
graft survival. Hence, we could not consider
KDPI a good prognostic test to determine graft
survival in the Portuguese population, in contrast
to other European cohorts.(1, 3) This could be due
to our short follow-up, and hence the low number
of graft failures, which could have reduced the
power of data. The low number of kidney graft
failures may also have led to a lower discriminatory
power for identifying factors associated with graft
dysfunction or rejection. Another bias for this
study is the retrospective nature of this analysis.
Multinomial logistic regression results
indicated that recipient age was a significant
predictor for graft loss (approximately 5% less
likely to occur for each year older a recipient was
at the time of transplant). This finding is in line
with the study from Lehner et al, which found
that in their cohort older recipient age prevented
death-censored graft loss.(1)
Regarding machine learning methods used
as algorithms to predict kidney graft failure, the
DT showed to be a better predictor than ANNs
with higher accuracy and sensitivity values. The
recipient and donor’s age showed the highest
discriminatory power for predicting graft
dysfunction.
Figure 4. Histogram showing
distribution of age of donors
Recipient’s PRA, HLA-DR compatibility,
and donor’s cause of death showed lower
discriminatory power than expected. Regarding
PRA, this can be due to a variable degree of
sensitivity in immunological analysis over
the years. Older immunological methods
had a lower degree of sensitivity for detecting
allosensitization than standard current ones. This
could have led to an erroneously higher number
of patients categorized as “PRA of 0%”. The
lower discriminatory power of “cause of death”
may be related to the high proportion of patients
admitted with a cerebrovascular cause of death in
our dataset.
As for machine learning methods, DTs
showed better accuracy than ANNs. However, for
these classifiers’ application on a large scale in the
health area, it is necessary to use other balancing
sets or to create a Portuguese database containing
the data of all patients involved in kidney
transplantation (donors and recipients). In that
way, ANNs and DTs will have a higher training
set that will improve the algorithm performance
(hence with greater precision in prediction).
In conclusion, in this study, although KDPI
correlated with eGFR 1 year post-transplant, it
couldn’t accurately predict graft failure. Hence,
we could not consider it as a good criteria for
accepting an organ or not. On multinomial logistic regression, only age of the recipient proved
to be a good prognostic factor. Regarding the use
of artificial intelligence methods on the field of
kidney transplant, DTs showed a good accuracy.
We believe that this study provides an
important reflection about our kidney donor
and recipient population, and paves the way for
future work. As the worldwide debate about data
protection and its legislation continues, it seems
clear that wide and detailed database records
could be a valuable asset to fully characterize KT
donor and recipient population and establish
which factors influence kidney graft survival in
the long term.
This knowledge is critical for us to be able
to select the best organ for each proponent
recipient and ultimately provide better care to
our population rooted in the values of equity,
efficiency, and fairness.",https://www.redalyc.org/journal/5642/564272587006/564272587006.pdf
Aminian et al. 2020,Predicting 10-Year Risk of End-Organ Complications of Type 2 Diabetes With and Without Metabolic Surgery: A Machine Learning Approach,"OBJECTIVE 
To construct and internally validate prediction models to estimate the risk of long-term end-organ complications and mortality in patients with type 2 diabetes and obesity that can be used to inform treatment decisions for patients and practitioners who are considering metabolic surgery.

RESEARCH DESIGN AND METHODS
A total of 2,287 patients with type 2 diabetes who underwent metabolic surgery between 1998 and 2017 in the Cleveland Clinic Health System were propensity-matched 1:5 to 11,435 nonsurgical patients with BMI ≥30 kg/m2 and type 2 diabetes who received usual care with follow-up through December 2018. Multivariable time-to-event regression and random forest machine learning models were built and internally validated using fivefold cross-validation to predict the 10-year risk for four outcomes of interest. The prediction models were programmed to construct user-friendly web-based and smartphone applications of Individualized Diabetes Complications (IDC) Risk Scores for clinical use.

RESULTS
The prediction tools demonstrated the following discrimination ability based on the area under the receiver operating characteristic curve (1 = perfect discrimination and 0.5 = chance) at 10 years in the surgical and nonsurgical groups, respectively: all-cause mortality (0.79 and 0.81), coronary artery events (0.66 and 0.67), heart failure (0.73 and 0.75), and nephropathy (0.73 and 0.76). When a patient’s data are entered into the IDC application, it estimates the individualized 10-year morbidity and mortality risks with and without undergoing metabolic surgery.

CONCLUSIONS
The IDC Risk Scores can provide personalized evidence-based risk information for patients with type 2 diabetes and obesity about future cardiovascular outcomes and mortality with and without metabolic surgery based on their current status of obesity, diabetes, and related cardiometabolic conditions.","Using long-term data on cardiovascular outcomes of metabolic surgery and nonsurgical patients and rigorous statistical techniques, accurate prediction models were constructed and integrated into a risk calculator to estimate the likelihood of long-term end-organ complications and death in patients with type 2 diabetes and obesity. The area under the curve of prediction models ranged from 0.66 to 0.81, with the best discrimination ability for all-cause mortality. The calibration curves indicated that the prediction models do not substantially overestimate or underestimate risk. Direct comparison of IDC Risk Scores with RECODe models on a separate cohort of nonsurgical patients suggested better performance of new models.

Although risk factors for cardiovascular morbidity and mortality in patients with type 2 diabetes have been widely described, there are few published prediction models. The most widely used diabetes-specific prediction model is the UKPDS risk engine. It estimates 10-year risk of coronary artery events and stroke from a noncontemporary cohort of ∼5,000 patients with type 2 diabetes without heart disease who were enrolled between 1977 and 1991. Ten predictive factors included age, sex, ethnicity, smoking status, duration of diabetes, HbA1c, systolic blood pressure, total cholesterol, HDL cholesterol, and atrial fibrillation (24,25). However, the validity of this model for patients with diabetes treated with contemporary medications remains uncertain.

The available prediction models to estimate the future risk of macrovascular complications, microvascular adverse events, and mortality in patients with type 2 diabetes have some limitations (18,24–34). They usually only address single outcome (mainly risk of all-cause mortality) and do not simultaneously provide estimates for multiple clinically important end points for individual patients with type 2 diabetes and obesity. Furthermore, most available models use a limited number of baseline variables to estimate the risk for a shorter time horizon (i.e., 5-year risk prediction) in patients without major baseline comorbidities. In the current study, 26 diverse baseline variables, including demographic data, history of medical comorbidities, laboratory data, and medications, were considered in our models to predict the 10-year risk. More importantly, the common statistical technique used for construction of available prediction models in the literature has been the Cox proportional hazards modeling. To develop the IDC risk scores, in addition to classical regression models (including the Cox proportional hazards), we used a machine learning approach and built random forest prediction models. The machine learning–derived risk-prediction models yielded favorable discrimination and larger cross-validated IPA at 10 years compared with traditional regression models for two of the eight outcomes of interest. In addition, to the best of our knowledge, there is no other prediction model to estimate future risk of end-organ complications of diabetes and obesity after metabolic surgery.

These prediction models may be clinically useful for health care professionals, including primary care physicians, endocrinologists, cardiologists, and bariatric surgeons, and patients with type 2 diabetes and obesity to provide evidence-based predictions of the 10-year risk of adverse cardiovascular outcomes based on the current health status. The calculator uses clinical and laboratory information that is readily available in clinical practice.

A web version of the IDC Risk Scores is accessible at https://riskcalc.org (under the Obesity menu) and as a smartphone application (BariatricCalc). When the required values are entered into the calculator, the IDC Risk Scores are computed to assist in shared decision making for patients who are considering metabolic surgery for management of their obesity and diabetes. Particularly, the models would be clinically useful to identify individuals who have a high risk of death and major end-organ complications of diabetes with continuation of usual care. Within this group, if the predicted risk estimates are substantially lower after metabolic surgery, a decision to undergo surgical intervention might represent an opportunity for improved patient management. Conversely, when the risk estimates of adverse cardiovascular events are relatively comparable or higher after surgical intervention, continuation of usual care might be recommended. The IDC Risk Scores potentially address physician and patient concerns in preventing a wide range of life-threatening adverse events of diabetes and obesity.

This study has several limitations. First, the aim of this study was to build prediction models to optimally discriminate the outcomes of interest. The aim was not to build causal models. The rank of each variable in Fig. 2 does not necessarily show the importance of that variable in the chain of causation. For example, use of aspirin as one of the most important variables in the models of coronary artery events does not mean aspirin causes coronary artery events. Second, data from the EHR were retrospectively obtained, which would be associated with coding errors and misclassification. Furthermore, although a comprehensive propensity matching was performed, residual measured or unmeasured confounders could have influenced the outcomes. To develop prediction models, a large sample size is needed, which was made possible using a retrospective design. Similar to previous studies, the mortality risk models had the best discrimination ability among our models. That is probably due to the fact that mortality is a well-defined outcome with minimal risk of misclassification. Third, some possible important variables, including duration of diabetes, family history of coronary artery disease, LDL, HDL, and urinary albumin-to-creatinine ratio, were missing from too many patients and could not be included in the models. Fourth, because 95% of surgical patients underwent either gastric bypass or sleeve gastrectomy, the predictive surgical models may not be generalizable to other metabolic surgical procedures. Nonetheless, these two procedures are the two most common metabolic procedures in the current practice. Fifth, relatively few nonsurgical patients were exposed to newer classes of diabetes medications that are associated with significant cardiovascular benefits. In a similar fashion, 5% of the surgical group had gastric banding, which is the least effective surgical procedure for type 2 diabetes. Ideally, the prediction models should be periodically updated in order to estimate the risk for contemporary medical and surgical treatments, and not for old suboptimal therapies. Sixth, development of prediction models to estimate short- and long-term major complications of metabolic surgery would be the subject of future projects. Seventh, although internal cross-validation and comparison with the RECODe models were done, lack of an external validation cohort and evidence on clinical usefulness of prediction models are other limitations. However, it is challenging to find appropriate cohorts with adequate follow-up time to assess cardiovascular outcomes after metabolic surgery for external validation. Despite these limitations, this was the first effort to identify predictors and construct a risk calculator using both traditional regression methods and a machine learning approach with sufficient accuracy in calibration and discrimination to predict long-term cardiovascular outcomes after metabolic surgery.

In conclusion, this study showed that major adverse cardiovascular events (all-cause mortality, coronary artery events, heart failure, and nephropathy) are predictable outcomes in patients with type 2 diabetes and obesity who undergo metabolic surgery or receive usual diabetes care. The IDC Risk Scores can provide personalized evidence-based risk information for patients with type 2 diabetes and obesity about future cardiovascular outcomes and mortality with and without metabolic surgery, based on their current status of obesity, diabetes, and related cardiometabolic conditions. The prediction models would benefit from external validation.",https://diabetesjournals.org/care/article/43/4/852/35751/Predicting-10-Year-Risk-of-End-Organ-Complications
Sapir-Pichhadze & Oertelt-Prigione 2023,P32: a sex- and gender-sensitive model for evidence-based precision medicine: from knowledge generation to implementation in the field of kidney transplantation,"Precision medicine emerged as a promising approach to identify suitable interventions for individual patients with a particular health concern and at various time points. Technology can enable the acquisition of increasing volumes of clinical and “omics” data at the individual and population levels and support advanced clinical decision making. However, to keep pace with evolving societal realities and developments, it is important to systematically include sex- and gender-specific considerations in the research process, from the acquisition of knowledge to implementation. Building on the foundations of evidence-based medicine and existing precision medicine frameworks, we propose a novel evidence-based precision medicine framework in the form of the P32 model, which considers individual sex-related (predictive [P1], preventive [P2], and personalized [P3] medicine) and gender-related (participatory [P4], psychosocial [P5], and percipient [P6] medicine) domains and their intersection with ethnicity, geography, and other demographic and social variables, in addition to population, community, and public dimensions (population-informed [P7], partnered with community [P8], and public-engaging [P9] medicine, respectively). Through its ability to contextualize and reflect on societal realities and developments, our model is expected to promote consideration of diversity, equity, and inclusion principles and, thus, enrich science, increase reproducibility of research, and ensure its social impact.","Precision medicine emerged as a promising approach, overcoming limitations of evidence-based medicine while maintaining focus on methodological rigor. Technology, in enabling the acquisition of increasing volumes of data at the individual and the population levels, can support advanced clinical decision making. To keep pace with evolving societal realities and developments, however, we propose that precision medicine research be enhanced by sex- and gender-specific considerations from the acquisition of knowledge to its implementation. Moreover, a broader consideration of principles of equity, diversity, and inclusion, including self-reported ethnicity and cultural background, is also bound to affect both individual patient experience and outcomes.67 To realize the benefits of evidence-based precision medicine, we proposed a revised framework in the form of the P32 model (Figure 2), which considers individual-level sex-related (P1–P3) and gender-related (P4–P6) domains, in addition to group-level population, community, and public dimensions (P7–P9), and outline recommendations on how to consider them in clinical transplantation and research (Table 113,68, 69, 70, 71, 72, 73, 74, 75, 76, 77). Through its ability to contextualize and reflect societal realities and developments, our model is expected to promote diversity, equity, and inclusion principles and, thus, enrich science, increase reproducibility of research, and enhance its social impact.",https://www.sciencedirect.com/science/article/pii/S0085253823000534
Brahmbhatt et al. 2022,The lung allocation score and other available models lack predictive accuracy for post-lung transplant survival,"Background
Improved predictive models are needed in lung transplantation in the setting of a proposed allocation system that incorporates longer-term post-transplant survival in the United States. Allocation systems require accurate mortality predictions to justly allocate organs.

Methods
Utilizing the United Network for Organ Sharing database (2005-2017), we fit models to predict 1-year mortality based on the Lung Allocation Score (LAS), the Chan, et al, 2019 model, a novel “clinician” model (a priori clinician selection of pre-transplant covariates), and two machine learning models (Least Absolute Shrinkage and Selection Operator; LASSO and Random Forests) for predicting 1-year and 3-year post-transplant mortality. We compared predictive accuracy among models. We evaluated the calibration of models by comparing average predicted probability vs observed outcome per decile. We repeated analyses fit for 3-year mortality, disease category, including donor covariates, and LAS era.

Results
The area under the cure for all models was low, ranging from 0.55 to 0.62. All exhibited reasonable negative predictive values (0.87-0.90), but the positive predictive value for was poor (all <0.25). Evaluating LAS calibration found 1-year post-transplant estimates consistently overestimated risk of mortality, with greater differences in higher deciles. LASSO, Random Forests, and clinician models showed no improvement when evaluated by disease category or with the addition of donor covariates and performed worse for 3-year outcomes.

Conclusions
The LAS overestimated patients’ risk of post-transplant death, thus underestimating transplant benefit in the sickest candidates. Novel models based on pre-transplant recipient covariates failed to improve prediction. There should be wariness in post-transplant survival predictions from available models.","In this large retrospective analysis, we demonstrated that the LAS and several other models lacked a meaningful ability to accurately predict post-transplant mortality using pre-transplant covariates. The LAS overestimated patients’ risk of death (particularly for those in the highest deciles of LAS) and thus may disadvantage some waitlisted patients. Machine learning models, disease-specific models, and models with donor characteristics did not improve predictive accuracy. Because of the importance of longer-term survival after LTx, in keeping with proposed UNOS policy changes, we assessed 3-year survival models, but found the predictive accuracy was even worse than that seen in the 1-year models. Several variables individually were associated with re-transplant or death within 1-year. Unfortunately, aggregate models designed by expert selection or machine learning, even when incorporating variables with significance when isolated, were unable to meaningfully predict post-transplant mortality. The available covariates do not capture the data needed to do so. These findings are disappointing, but very relevant as the US attempts to revise its lung allocation system with a focus on prioritizing post-transplant survival.

Our findings confirm an earlier study by Gries, et al,17 who used an ISHLT dataset and demonstrated that a model based on pre-transplant covariates poorly predicted 1-year or 5-year survival (AUC 0.553 and 0.591, respectively), the LAS had poor predictive ability at 1-year and 5-years (AUC 0.58 and 0.566, respectively), and there was no improvement when fitting these models for individual disease groups.17

In the 2019 study by Chan, et al,10 the Houston Methodist model was predictive of 1-year mortality and was able to designate patients into risk categories, but we were unable to replicate these findings in our larger, more contemporary dataset. The Houston Methodist model was designed by Chan, et al,10 using the UNOS dataset, containing 10533 patients who underwent LTx between 1994-2014, to identify and randomly cohort the 633 patients who underwent LTx at Houston Methodist Hospital into equal development and validation cohorts with which to form a predictive model. The Chan, et al,10 model had an AUC for 1-year mortality of 0.74 and 0.67 for the development cohort and validation cohort, respectively. On our attempt to recreate the Chan, et al,10 model within our development cohort, we found an AUC for 1-year mortality of only 0.59. The difference in populations and size of our cohorts may account for the difference in AUC of the original model to our recreation. Chan, et al,10 also found that the LAS had a lower AUC for 1-year mortality, at 0.58 and 0.55 for their development and validation cohorts respectively, similar to the AUC for the LAS for our cohort (0.55).

In a recent study by Parker, et al,8 the predictive accuracy of the LAS was analyzed using a Cox proportional hazard model and post-transplant survival Cox proportional hazard model which comprise the LAS. They, too, found poor calibration between predicted and observed waitlist survival, post-transplant survival, and LAS, respectively. The authors suggested that prediction could be better with updated models, specifically mentioning machine learning, and noting that the lack of donor variables may have contributed to the LAS not being effective for predicting post-transplant mortality.8 Unfortunately, the novel models assessed in our study did not show improved predictive accuracy despite utilizing machine learning techniques, including alternate pre-transplant recipient covariates, including available donor covariates, or stratifying by diagnosis.

We were unable to identify a model which provided better 1-year or 3-year accuracy in predicting survival. The OPTN Board of Directors established continuous organ distribution as the preferred framework to distribute all organs18 to improve transparency and equity in organ allocation. Lung was selected as the first organ to make this change, leading to the proposal of the composite allocation score (CAS) by the OPTN Lung Transplantation Committee.9 The Lung CAS will utilize 5-year predicted post-transplant outcomes model, rather than the 1-year predicted post-transplant outcomes model currently utilized.11 The 5-year model, much like the 3-year model considered in this study, did not have better predictive accuracy than the 1-year model, but was demonstrated to have similar level of confidence as the 1-year models in the report by the Scientific Registry of Transplant Recipients.11 The 5-year model allows consideration of a longer outcome period and, in the context of continuous allocation, showed greater variability across age groups than 1-year models, which may allow for stratification by age.11

Numerous studies have endeavored to validate the effectiveness of the LAS as a system to allocate organs to those most in need of a donor lung and its success in predicting post-transplant mortality. Earlier studies suggested that those with higher LAS experienced worse absolute survival after transplant than patients with lower LAS.19, 20, 21, 22, 23, 24 Since its introduction, the mean LAS in the upper quartile of patients who receive transplants has steadily risen,25 suggesting sicker patients being listed for transplant. Earlier studies looked at absolute survival and worse outcomes may have reflected a sicker population being listed. Later studies show that recipients with higher LAS experienced greater survival benefit than those with lower LAS and that nuances exist between various diagnostic groups.25, 26, 27

Though several studies have shown modifiable pre-transplant variables, such as weight and albumin levels, that correlate with post-LTx mortality, our study did not find significant correlation when considering these as covariates aggregated within models.28, 29, 30, 31, 32, 33 Several scoring systems, such as the Oto-Score34,35 and the Louisville-UNOS scale in conjunction with the LAS,36 predict post-transplant outcomes including 1-year mortality of the organ recipient based on donor and organ variables. The inclusion of donor covariates in a unifying model to predict post-LTx outcomes did not improve predictive accuracy in our study.

Our study involved a thorough approach to evaluating the available predictive models for post-LTx survival in a contemporary national sample of LTx recipients. There are, however, limitations to this study. First, we are limited to variables available in the OPTN registry. The data available may be insufficient to design a model with highly predictive accuracy. There may be factors we do not capture, whether in the OPTN registry or otherwise, which impact long-term mortality (see Table S-6). Second, while our data set was up to date, we have fewer outcomes, particularly 3-year survival from the most recent years, which could have impacted our assessment of 3-year survival prediction.

Our findings serve several cautionary tales. The LAS can overestimate an individual patient's risk of death (particularly those with the highest LAS) and potentially limit access to transplant for certain patients. There should be wariness in the clinical utility of short-term survival predictions from the LAS and other models based on pre-transplant recipient covariates, and there is no identifiable model to reliably predict medium- and long-term survival after transplant. This may be due, in part, to the limitations of which variables are available in the OPTN registry. Additionally, we must consider whether the diversity of pulmonary diagnoses leading to end-stage lung disease lend to a single, unifying model for predicting post-transplant outcomes – though models stratified by diagnosis do not perform more accurately.

Stewardship of donated organs while caring for vulnerable patients is a solemn responsibility. Maintaining equity in allocating these organs while assuring that recipients and donated organs have the best chance for long-term survival are inexorable tenets. Though implementation of the LAS improved waitlist mortality and resulted in increased transplant rates, there continues to be room for improvement. Developing accurate models to predict long-term post-transplant survival is vitally important for ethical organ allocation and scarce resource-utilization.",https://www.sciencedirect.com/science/article/pii/S1053249822019477?casa_token=HiLt9CNnNv8AAAAA:LCYp8Kneu2Bn3xVbptwHvEhM9JrK6UqHPxYSD8pzG6i3tNN-2zqsbM8Beb7pVH17bbzOnu4W
Reeve et al. 2019,Generating automated kidney transplant biopsy reports combining molecular measurements with ensembles of machine learning classifiers,"We previously reported a system for assessing rejection in kidney transplant biopsies using microarray-based gene expression data, the Molecular Microscope® Diagnostic System (MMDx). The present study was designed to optimize the accuracy and stability of MMDx diagnoses by replacing single machine learning classifiers with ensembles of diverse classifier methods. We also examined the use of automated report sign-outs and the agreement between multiple human interpreters of the molecular results. Ensembles generated diagnoses that were both more accurate than the best individual classifiers, and nearly as stable as the best, consistent with expectations from the machine learning literature. Human experts had ≈93% agreement (balanced accuracy) signing out the reports, and random forest-based automated sign-outs showed similar levels of agreement with the human experts (92% and 94% for predicting the expert MMDx sign-outs for T cell-mediated (TCMR) and antibody-mediated rejection (ABMR), respectively). In most cases disagreements, whether between experts or between experts and automated sign-outs, were in biopsies near diagnostic thresholds. Considerable disagreement with histology persisted. The balanced accuracies of MMDx sign-outs for histology diagnoses of TCMR and ABMR were 73% and 78%, respectively. Disagreement with histology is largely due to the known noise in histology assessments (ClinicalTrials.gov NCT01299168).","To achieve stable estimates of molecular rejection states in kidney transplant biopsies, we studied an international population representing prevalent renal transplants in experienced centers. We established that ensembles of many classifiers gave the optimal combination of accuracy (AUCs) and stability (correlations between predictions from separate training sets) in test sets, consistent with previous findings using ensemble learning. Report sign-outs by experts following guidelines and automated sign-outs showed ≈95% agreement (eg, for TCMR: yes/no), with most discrepancies in boundary cases near the thresholds. The agreement between histologic diagnoses and MMDx was lower: accuracies of 82% for ABMR and 90% for TCMR, and balanced accuracies of 78% and 73%, respectively. We conclude that an ensemble of molecular algorithms improves the stability of molecular estimates, and that the method can be used to generate highly reproducible diagnoses, either by human experts or by an automated sign-out system. Noise in histology diagnoses places limits on the concordance between histologic and molecular diagnoses.

The present results are consistent with the increasing popularity of ensemble-based approaches in the machine learning literature. Depending on the computational details, they may be described using terminology such as voting, committees of experts, bagging, boosting, stacking, etc. In this study, we used relatively simple ensemble methods at several different stages. Some (rf, gbm, C5.0) of the classifier methods used to predict the initial base scores—all the nonarchetype classifier scores that go into the report—are themselves ensemble methods. We then take the median of all the classifier methods’ predictions as a second ensemble step. Finally, for future reports a random forest classifier—the most common ensemble method seen in practice—can be used to predict and potentially replace expert sign-outs.

Although diagnostic categories are usually required, that is, for determining treatment options, our overall philosophy is that it is best to have not only a categorical diagnosis but the associated probabilities. With these probabilities, confidence in the diagnosis can be assessed. In some cases, additional relevant information regarding a case may be known, allowing the clinician to weigh evidence from all sources. This will be particularly important when the biopsies are near cutoff values between diagnostic categories, something that is readily apparent in a molecular diagnosis but much less so in most pathology reports.

This analysis does not include a comparison of ABMR stages, which are not readily translated between MMDx and histology.11 In MMDx, ABMR is categorized as early-stage, fully developed, and late-stage by archetypal analysis,11 whereas histology categorizes ABMR as either active or chronic active ABMR, with the implication of chronic inactive ABMR. (The issue of ABMR histologic stage is ambiguous and should be clarified in future Banff meetings.) Many biopsies are also called transplant glomerulopathy or suspicious for ABMR. Detailed molecular and histology comparisons with outcomes are in progress to compare these 3 classes and understand staging, because stage, as well as activity estimates, should be a key determinant of treatment choices as well as prognosis (manuscript in preparation).

While the present report focuses on indication biopsies, the system is equally applicable to protocol biopsies, but the results will depend on the protocol. Many of the indication biopsies in the present population are relatively normal, and thus the indication biopsy population provides a complete range of phenotypes that overlap the findings in the protocol population. We also have extensive studies in protocol biopsies taken at 6 weeks 34 and in protocol biopsies performed in patients screened for DSA-positivity.35 The case mix in protocol biopsies will usually not represent the more severe phenotypes, which by definition will usually trigger indication biopsies.

Misdiagnosis of rejection can cause hazardous over- or undertreatment and lead to increased graft loss. Central MMDx testing offers potential benefits with regard to this problem in terms of precision, accuracy, and international standardization. Future reports will evolve to include automated ensemble sign-outs such as the random forests shown here. While discrepancies between the molecular MMDx diagnosis and histology can be due to errors in either platform, various considerations support our belief in the superiority, on average, of MMDx (Table 1). Precision comes from high technical and biological reproducibility, even in the small samples dictated by IRBs for research studies. Accuracy comes from the measurement of molecules related to the disease states and from the machine learning approaches. Standardization is an advantage for guiding treatment in diverse centers, especially as more expensive and potentially toxic treatments become available for ABMR. Moreover, a stable molecular estimate of the disease state permits recalibration of the histology system, where many of the rules were of necessity opinion-based, rather than data-generated. DSA measurements can also be evaluated for their probable relevance.",https://pubmed.ncbi.nlm.nih.gov/30868758/
Cruz-Ramírez et al. 2012,Memetic Pareto differential evolutionary neural network used to solve an unbalanced liver transplantation problem,"Donor–recipient matching constitutes a complex scenario difficult to model. The risk of subjectivity and the likelihood of falling into error must not be underestimated. Computational tools for the decision-making process in liver transplantation can be useful, despite the inherent complexity involved. Therefore, a multi-objective evolutionary algorithm and various techniques to select individuals from the Pareto front are used in this paper to obtain artificial neural network models to aid decision making. Moreover, a combination of two pre-processing methods has been applied to the dataset to offset the existing imbalance. One of them is a resampling method and the other is a outlier deletion method. The best model obtained with these procedures (with AUC = 0.66) give medical experts a probability of graft survival at 3 months after the operation. This probability can help medical experts to achieve the best possible decision without forgetting the principles of fairness, efficiency and equity.","In this paper, the problem of donor–recipient matching when treating liver transplantation has been analysed and studied in order to help medical experts on the complex decision of donor–recipient allocation. To do so, different selection methods are used to obtain artificial neural network models from a multi-objective evolutionary algorithm that can help medical experts in donor–recipient allocation. These models are obtained from the Pareto front built through a multi-objective evolutionary algorithm where accuracy and the minimum sensitivity are the measures considered for evaluating model performance. Minimum sensitivity is used to avoid the design of models characterized by high global performance in general, but bad performance when considering the classification rate for each class (survival or non-survival). In addition, the sensitivity of each class and the AUC metric have been used to detect trivial and random classifiers, respectively.

A pre-processing procedure has been applied to the original dataset. This pre-processing removes the patterns with extreme values in the majority class and duplicates the patterns of the minority class using the well-known synthetic minority over-sampling technique algorithm. The results obtained show that the algorithm performs more consistently when using the pre-processed dataset.

With two of the best models (one with the best performance in the 
 metric and the other in the 
 measure) obtained using the pre-processed dataset, a rule-based system could be used to perform donor–recipient matching. This rule-based system should be generated by medical experts and machine learning experts, to uphold the principles of fairness, efficiency and equity. Current allocation systems are based on the risk of dying while on the waiting-list, and do not recognise distinctions in “donor organ quality”. With the rule-based system, “donor organ quality” would be taken into account to improve allocation and ensure the survival of recipients.",https://link.springer.com/article/10.1007/s00500-012-0892-7
Perez-Ortiz et al. 2012,Hybrid Multi-objective Machine Learning Classification in Liver Transplantation,"This paper constructs a hybrid, multi-objective and evolutionary algorithm based on Differential Evolutions using neural network
models and q-Gaussian basis units in order to develop an efficient and
complete system for donor-recipient assignment in liver transplantation.
The algorithm is used for the classification of a binary dataset and will
predict graft survival at 15 and 90 days after the transplantation. Other
hybrid approaches combining artificial neural networks with evolutionary computation and well-known algorithms are presented in order to
compare the obtained performance of both mono and multi-objective
methods, using other methods such as Support Vector Machines and
Discriminant Analysis. Some supervised attribute selection methods were
previously applied, in order to extract the most discriminant variables
in the problem presented. The models obtained allowed medical experts
to predict survival rates and to come to a fair decision based on the
principles of justice, efficiency and equity","We can conclude for this case that although the Correct Classification Rate and
Minimum Sensitivity are correlated objectives, i.e. some kind of linear relation
can be seen between the solutions in the Pareto fronts, a multi-objective approach
using Artificial Neural Networks is preferred in order to explore the complete
set of solutions for the classification. The MPDENN algorithm has shown good
performance for this specific database, improving the results of some state-ofthe-art methods. Also, the conclusion drawn may be that in some really complex
problems, as in this case, it could be interesting to apply some feature selection to
remove noise from the data. It is important to analyse the problem we are dealing
with before applying a mono or a multi-objective methodology, and clarify which
are the metrics we want to maximize, in this specific case, we want a good balance
between Correct Classification Rate and Minimum Sensitivity. As future works
the database could be analysed taking into account two different classes: graft
failure before and after three months after transplantation. Patterns classified
in first class could be analysed with the obtained models in this paper.",https://link.springer.com/chapter/10.1007/978-3-642-28942-2_36
Dorado-Moreno et al. 2016,"
Ordinal Evolutionary Artificial Neural Networks for Solving an Imbalanced Liver Transplantation Problem","Ordinal regression considers classification problems where there exists a natural ordering among the categories. In this learning setting, thresholds models are one of the most used and successful techniques. On the other hand, liver transplantation is a widely-used treatment for patients with a terminal liver disease. This paper considers the survival time of the recipient to perform an appropriate donor-recipient matching, which is a highly imbalanced classification problem. An artificial neural network model applied to ordinal classification is used, combining evolutionary and gradient-descent algorithms to optimize its parameters, together with an ordinal over-sampling technique. The evolutionary algorithm applies a modified fitness function able to deal with the ordinal imbalanced nature of the dataset. The results show that the proposed model leads to competitive performance for this problem.","This paper proposes a new fitness function for an evolutionary artificial neural network in imbalanced problems, together with the application of an ordinal over-sampling technique to palliate the skewness of the distribution of the patterns belonging to each class. The proposal (IM-ORNET) is evaluated in a extremely imbalanced liver transplantation problem and it improves the results with respect to previous works [11], obtaining a good balance in performance for all the metrics considered. The application of the ordinal over-sampling techniques is able to improve the results for minority classes, without harming the ordinal structure of the dataset.

Some future researching lines include extending the database to include more European hospitals and applying the model in a real environment to check its performance when compared to the use of the MELD score. Another possible future research line is to dynamically adjust the weights introduced for each class (
) depending on the average performance of the population for each of the classes, instead of using static weights.",https://link.springer.com/chapter/10.1007/978-3-319-32034-2_38
Atallah et al. 2019,Intelligent feature selection with modified K-nearest neighbor for kidney transplantation prediction,"The prediction of kidney transplantation outcome is an important challenge and does not need emphasis because of the lack of available organs. Graft survival prediction is significant to help physicians to take the right decision and enhance survival rate by changing medical procedure. Also, it helps in the best choice of the existing kidney donor and the immunosuppressive management suitable for a patient. But the exact prediction of the graft survival is still not accurate despite of the advancements in this field. The purpose of our research is to design an intelligent kidney transplantation prediction method to solve the prediction problem by utilizing data mining methods. The novelty of this study is focused in presenting: (a) an integrated prediction method, (b) a new intelligent feature selection method, and (c) a modified K-nearest neighbor. Choosing the proper variables is accomplished by merging three feature selectors. The new proposed feature selection method is accomplished using gain ratio, naïve Bayes, and genetic algorithm. Next, the cleaned dataset is utilized to provide quick and precise outcome throughout a modified K-nearest neighbor classifier. Each stage of this proposed method has been evaluated using intense experiments. Experimental results demonstrate the efficiency of all the steps of the proposed method. Additionally, the proposed method has been evaluated versus latest methods. The results presented that this method outperformed all latest and similar literature methods. This method can as well be employed to other related transplant datasets.","The prediction of kidney transplantation result is very important and does not require emphasis. Hence, successful prediction method is an essential task. In this study, we have designed a new proposed method to classify graft result. This study introduced a method composed of three stages, namely: (i) data organization phase (DOP), (ii) variable selection method (VSM), and (iii) outlier rejection and prediction phase (PP). The proposed method combines variable selection with outlier rejection and machine learning methods to enable better predictive abilities. The proposed prediction method includes new intelligent feature selection procedure and a modified K-nearest neighbor. The new proposed feature selection procedure collects between gain ratio, naïve Bayes, and genetic algorithm, which chooses the essential features from the dataset. Additionally, the proposed modified K-nearest neighbor introduced the outlier rejection and prediction module that used distance based measures with K-nearest neighbor to classify patients. The efficiency of the proposed method is evaluated using urology and nephrology center dataset. Each stage of the proposed method has been assessed throughout intense experiments. The overall method is examined to verify the compatibility of the proposed method. The evaluation results emphasis that our proposed method is efficient. The proposed VSM can select the reduced variables list efficiently. Results likewise specify the efficiency of the proposed outlier rejection and prediction phase as it enhances the prediction accuracy. Experimental results presented that the proposed method gave more precise results than most recent methods. In general, the results offered a new method that could benefit in progress the result of kidney transplantation. This method can as well be employed to other related transplant datasets.",https://link.springer.com/article/10.1007/s42452-019-1329-z#Sec11
Placona et al. 2020,Can donor narratives yield insights? A natural language processing proof of concept to facilitate kidney allocation,"Although expedited placement could ameliorate stagnant kidney utilization, precisely identifying difficult-to-place organs is crucial to mitigate potential harms associated with this policy. Existing algorithms have only leveraged structured data from the Organ Procurement and Transplantation Network (OPTN); however, detailed, free text case information about a donor exists. No known research exists about the utility of these data. We developed a model to predict the probability of delay or discard for adult deceased kidney donors between 2010 and 2018, leveraging donor free text data. The resultant model had a c-statistic of 0.75 compared to 0.80 (Reduced Probability of Delay or Discard [model], r-PODD) and 0.77 (Kidney Donor Profile Index, KDPI) on the test dataset. Analysis of the top predictive words suggest both known and potentially novel clinical factors (ie, a known factor such as hypertension vs a novel factor such as stents), and nuanced social factors (intravenous drug use) could negatively affect kidney utilization. These findings suggest that donor narratives have utility; the natural language processing (NLP) model is only moderately correlated with existing indices and provides directional evidence about additional cardiovascular risk factors that may affect kidney utilization. More research is needed to understand the potential to enhance existing indices of kidney utilization to better enable and mitigate the effects of policy interventions such as expedited placement.","4.1. Assessment of donor narrative utility
Our analysis demonstrates that free text donor narratives could be a potent, untapped data source that could augment our current prediction of kidney utilization. Encouragingly, we found that an NLP-model based solely on free text donor narratives yields a similar level of predictive power as traditional indices developed using structured data elements. The OPTN captures factors, such as diabetes and hypertension, within the structured data; and, the r-PODD model demonstrates slightly superior discriminative performance. Our model demonstrates that nuanced PHS risk data points, cardiovascular risk factors, and some other clinical factors (ie, COPD, hysterectomy) may affect kidney utilization, which are currently not captured in structured data by the OPTN.

4.2. Identified factors and relation to the literature
Although our effort to leverage DonorNet narratives is novel, efforts to exact useful information from free text fields is not new to transplantation or health care in general. Previous pilot studies within the field of transplantation have demonstrated the ability of NLP to improve the prediction of graft failure.29,30 There exist systematic reviews of use of free text and NLP in health care31,32 (not intended to be an exhaustive list on the subject). Although this model may be perceived as a “black box,” we have leveraged a tool to provide the readers insight into what words within the free text are driving the model’s predictions.27 For example, the NLP model highlights specific types of PHS risk (ie, intravenous drug use) associated with discard. This finding is aligned with research on kidney offer declines33 in addition to the similar factors, hypertension, diabetes, and insulin use used in previous models.9, 10, 11 Having a directional understanding of these nuanced associations, in relation to PHS risk, is important given that the Centers for Disease Control and Prevention (CDC) is reevaluating PHS risk definitions.34

Given that the model lines up with existent literature is encouraging when evaluating some of the more novel factors identified by the model. The NLP model identified a number of cardiovascular factors that have not been previously identified. Key words such as stents, CHF, and cholesterol negatively affect kidney utilization within this model. This finding makes sense if one considers the literature on kidney disease and cardiovascular health,35 especially in light of the fact that kidney disease is underdiagnosed.36 These findings could be informative on the types of additional data points to collect or to better understand and estimate kidney utilization.

4.3. Future efforts to identify and combine other data sources
Although we have demonstrated that the NLP methods on free text donor narratives hold promise, we recognize that more work needs to done. To augment existing models requires developing methods, ranging from text extraction to combining models, to determine if we can improve existing utilization and yield models. Although we have not implemented a method to combine structured and unstructured data into a single model, a rigorous evaluation of combination techniques is needed, especially as more complex sources of unstructured data are considered. Deep learning has shown promise to learn from complex data.18

Delving into the issue further, the field of transplantation may benefit from an NLP-driven approach to identifying data that is not currently in the structured OPTN data fields. Other existing text fields in the OPTN system, such as vital signs streamed from an intensive care unit or perfusion pump operating signals, may be able to improve predictions of organ utilization. With the right analytical approaches combined with more disparate data sources, we can further refine our existing utilization models and leverage techniques to understand what is driving the predictions.

4.4. Implementation considerations
The use of this model in a production setting also presents specific challenges not further covered in this study, but we mention them here. Resourceful DonorNet users who know that their donor text entries will be processed by a statistical algorithm may try to incorporate words or phrases associated with their desired outcome (ie, they might try to game the system). Fortunately, trying to game the model is difficult due to the underlying math (adding words or repeating does not guarantee your score will increase/decrease). In addition, it may be possible to identify text entries that substantially differ from what is typically entered (anomaly detection).

4.5. Threshold considerations
One challenge, as stated in the introduction, is the trade-off between efficiency and equity. Given that the scope of this paper was to identify sources of data to ameliorate this trade-off, we recognize that even in the highest decile (Table 2), no model was able to achieve a sensitivity >30% on its own; however, the PPV for each model was above 60% (roughly 2 to 3 times that of baseline). In subsequent deciles (Table 2), all models’ performance was more modest, which might reflect the difficulty in identifying organs that will experience difficulty being placed. Hence, policies to expedite organs may need to assess whether or not to leverage a sliding scale, a confidence-linked delay to alternative allocation policy based on the number of declines.
",https://www.sciencedirect.com/science/article/pii/S1600613522222874
Bell et al. 2022,Predicting Liver Utilization Rate and Post- Transplant Outcomes from Donor Text Narratives with Natural Language Processing,"Liver transplantation is a critical, life-saving treatment option for patients with terminal liver disease. Despite an organ shortage, many donated livers are discarded for reasons such as poor organ condition and physical incompatibility with a recipient. Current clinical models for liver risk assessment only utilize tabular data and result in poor precision and recall. Critical information relevant to this decision-making is likely included in the free-text clinical notes from donor evaluations that contain pertinent medical and social history of the donor that is currently unavailable in tabular data sources. This article describes the development of a model using these free-text clinical notes using a variety of Natural Language Processing (NLP) and machine learning (ML) techniques to predict the outcomes of three key metrics: 1) liver utilization rate, 2) 30-day mortality rate, and 3) 1-year mortality rate. The free-text narratives were useful for predicting liver utilization, with an associated area under the curve (AUC) score of 0.81, but were not useful for predicting both mortality outcomes, with associated AUC scores of 0.53 and 0.52, for 30-day and 1-year mortality, respectively. Using a locally interpretable model-agnostic explanations (LIME) algorithm, key phrases, like “dcd” and “alcohol” were found to be associated with unutilized livers, while “brain” and “heroin” were associated with utilized livers. Based on these findings, modeling donor text narratives may substantially contribute to improved decision-making and outcomes of liver transplantation.","The results confirm the efficacy of donor narratives in predicting liver utilization but show their limited predictive ability for short term (30-day) and long term (I-year) mortality. The best NLP model trained for predicting liver utilization used a logistic regression model and had a test AUC score of 0.81. Of the models built, the logistic regression model had a specificity of 0.62 and the gradient boosting model had a specificity of 0.97. Both models were able to fairly accurately predict one of the two classes, suggesting that they identified important features of both outcomes. However, the predictions on the same test of the two models had a Spearman's rank correlation coefficient of only 0.74, with model predictions agreeing on only 84% of test observations classified with a 0.5 threshold.

The limited predictability of the mortality outcomes may stem from the fundamental relationship between utilization and mortality: high-risk livers are likely not being recovered from deceased donors. Any donor information which may lead to increased mortality likely was flagged prior to organ recovery from the deceased donor. In further analyses, it may be necessary to include recipient information to build a more accurate mortality model. 

The results of the LIME algorithm identified unigrams associated with DCD and brain death as predictive of the unutilized and utilized classes, respectively, aligning with results of other studies. Clinically, DCD leads to warm ischemic damage of organs (including the liver), causing a higher risk of mortality with transplantation [13]–​[14]. Conversely, brain dead donors typically yield higher quality organs [15]. Similarly, “marijuana” and “heroin,” among the top predictive words for the utilized liver class, reflect the policies that have been adopted to alleviate the organ shortage. For example, livers infected with hepatitis C-a common consequence of using intravenous drugs, like heroin-may be transplanted to hepatitis C positive recipients [16]. This suggests that the results of the algorithm performed well in identifying predictive unigrams and bigrams.

The LIME algorithm identified unigrams associated with alcohol use as being predictive of unutilized livers. While alcohol abuse is a leading cause of liver cirrhosis, alcohol abuse has not been proven to have a negative effect on post-transplant outcomes [17]–​[18]. These results may indicate potential biases during offer acceptance, stemming from the belief that alcohol abuse negatively impacts liver quality, and further investigation should be done to understand these effects.

Notably, the DRI, which is based on tabular data, does not highlight alcohol use as a significant factor in liver risk assessment [9]. One possible explanation for this is that the DDR form only includes one yes/no question for heavy alcohol use, described as 2+ drinks per day, which is perhaps insufficient information to factor into the assessment [4]. In fact, only four yes/no questions in the DDR form pertain to drug use, which is most likely insufficient to provide a full context of a donor's history of drug abuse. Much like with alcohol use, there is often a perceived increased risk of negative post-transplant outcomes in donors with a history of drug abuse, especially cocaine [19]. Additional data collection on drug usage in the DDR form would generate more informative tabular data and allow researchers to more easily examine the effect of drug use in liver donors without having to examine the unstructured text fields used in this study.",https://ieeexplore.ieee.org/document/9799424
Asghari et al. 2022,Classifying Comments on Social Media Related to Living Kidney Donation: Machine Learning Training and Validation Study,"Background: Living kidney donation currently constitutes approximately a quarter of all kidney donations. There exist barriers that preclude prospective donors from donating, such as medical ineligibility and costs associated with donation. A better understanding of perceptions of and barriers to living donation could facilitate the development of effective policies, education opportunities, and outreach strategies and may lead to an increased number of living kidney donations. Prior research focused predominantly on perceptions and barriers among a small subset of individuals who had prior exposure to the donation process. The viewpoints of the general public have rarely been represented in prior research.

Objective: The current study designed a web-scraping method and machine learning algorithms for collecting and classifying comments from a variety of online sources. The resultant data set was made available in the public domain to facilitate further investigation of this topic.

Methods: We collected comments using Python-based web-scraping tools from the New York Times, YouTube, Twitter, and Reddit. We developed a set of guidelines for the creation of training data and manual classification of comments as either related to living organ donation or not. We then classified the remaining comments using deep learning.

Results: A total of 203,219 unique comments were collected from the above sources. The deep neural network model had 84% accuracy in testing data. Further validation of predictions found an actual accuracy of 63%. The final database contained 11,027 comments classified as being related to living kidney donation.

Conclusions: The current study lays the groundwork for more comprehensive analyses of perceptions, myths, and feelings about living kidney donation. Web-scraping and machine learning classifiers are effective methods to collect and examine opinions held by the general public on living kidney donation.","Principal Findings
This study confirmed
This study confirmed that the comments available from the internet can provide data on the general perception of living donation. Our trained model identified 11,027 comments related to LKD and 192,192 comments unrelated to LKD. Above, we present a sample distribution of comments that were incorrectly classified and their associated error types. There was a great deal of nuance and subtlety in the comments that could cause confusion for human classifiers, further increasing the difficulty for the machine classifier.

Many users wrote comments expressing their opinions regarding current policies. Though there was disagreement regarding how, nearly all users were supportive of making organs and transplants more accessible. There was notable support for a policy that would give preference or priority to designated or past organ donors when they face the need for organs. In the context of compensation for donation costs, it was also common to observe conversations regarding the legalization of organ sales. The two sides of this were primarily concerns about taking advantage of vulnerable populations and confidence in ethical market self-regulation. The various sources from which the comments were retrieved provided different kinds of comments. Comments that contained opinions about policy were most likely to be retrieved from the NYT, though they were also common on Reddit. There were also several self-reported accounts in the NYT and YouTube comments of someone or their spouse having previously been a living donor.

The character-restricted nature of Twitter meant that comprehensive ideas were less likely to be captured. Twitter was also more likely to produce comments in which people asked for donations or advocated for a loved one in need of an organ. Meaningful comments from YouTube were more often from people who had previous experience with transplants, either as patients or donors. While many of the Reddit comments were of little use, the “ask me anything” (AMA) subreddit provided a veritable treasure trove of information. There were threads written by people who had donated altruistically and invited people to “AMA.” This format, more than any other we encountered, seemed to yield the most thoughtful questions, concerns, and even resolutions to those concerns (to paraphrase one such person upon learning about a voucher system for the donor’s loved ones: “I’ve considered doing this before and never actually [done] anything. This has inspired me to sign up. Thank you!”).

Though there were positive responses from many users, some users were more cynical. One such user expressed the following: “the risk to living donors is also downplayed...people are guilted into acting as living donors only to find themselves at greater risk down the line.” Others wrote about frustrating experiences with the medical system or other worries, but we did not observe any blatantly false ideas in the comments. Lack of information was much more common than possession of misinformation.

To efficiently compile relevant information from comments and opinions found on the web, we used deep neural networks trained with specific criteria-driven classification labels. With this approach, we were able to develop a model that could identify comments related to LKD with an expected accuracy of 84%. Though further work remains to refine these results and classify these related comments according to the relevant factors, this first stage of classification indicates that the method could potentially be a valuable tool to extract themes related to barriers to and motivations for living donation. Because the topic is so nuanced, well-defined classification criteria for training data will be a vital part of developing a successful model. It is vital to have multiple people collaborating on training data annotation to ensure uniformity. Without these measures, the viability of this approach becomes less certain.

We note that the sizeable number of comments classified as irrelevant was to be expected to some extent. We suggest the following reasons to explain why our model incorrectly classified irrelevant comments as related to LKD: First, the size of the training data was relatively small compared to the total number of comments classified (1174/203,219). We project that with more (and more correctly labeled) training data, the model would yield better predictions. Second, models based on neural networks tend to have generalization errors that are sometimes identified as gaps [32]. Third, as mentioned above, there exists a great deal of nuance in this topic, and certain words that have no real significance may appear to the model as being important. For example, “parts” could be seen as a word that indicates “parts” of a body (ie, an organ), but it is simply a common word used in many settings.

For deceased kidney donation, there are a handful of studies that have utilized modern computer-science methods to analyze motivations and challenges associated with kidney donation. A recent study [33] discussed the use of natural language processing to glean information about deceased donors and the prospective utility of their kidneys. This information was retrieved from the United Network for Organ Sharing’s DonorNet program, in which organ procurement organizations enter raw text about the donors’ medical and social history, the history of their admissions, and other noteworthy information. A similar study [34] gathered 342 Spanish articles that contained the text “donacion de organos.” The authors found that a positive perception of kidney donation may be a contributing factor to the high rate of kidney donation in Spain. In another study [35], social media posts were collected to study the limitations of social messaging campaigns for deceased kidney donation.

Through the process of manual classification of training data, we observed nearly all barriers noted in the prior literature listed above, as well as early indicators of patterns. For example, the data suggested that the most frequent factors seen in the comments were directly related to the potential impact on prospective donors: considerations of immediate costs and risks of donating and the consequences of such a decision on those close to a donor. Broader influences, such as culture and belief systems, the influence of family members, and perceptions of the medical system, were less relevant to decisions related to living donation and more relevant to decisions related to deceased donation. In our manually labeled data, we did not observe the influence of HCPs as a factor that influenced a prospective donor’s decision to donate. Prior research indicates that barriers to donation attributable to HCPs include, for instance, lack of communication between transplant and dialysis teams, lack of training and information among HCPs, and negative attitudes held by some HCPs toward LKD [10].

Our study also recognized that the content and the quality of comments varied rather significantly depending on where they were retrieved. The AMAs of Reddit invited people to ask whatever questions they had, to be answered by someone who had been through the process personally. The downside of this particular resource is that there were only a few AMAs from living kidney donors. Comments from the NYT were more dependent on the content of the article to which they were attached, had no dialogue with the author, and were more conducive to debates on policy than to answering questions from curious prospective donors. Further analysis may provide greater insights into what kinds of internet sources yield the most meaningful information.

Limitations and Future Work
These collected data provide several opportunities for research on LKD. The data can be used for more complicated analysis, such as topic modeling and clustering, with the purpose of detecting barriers and motivations in multisource data sets. Future work may consider the following: instead of a first-stage binary classification, it may be beneficial to consider 4 classifications, such as “irrelevant,” “recipient-related,” “deceased donation,” and “LKD-related.” As deceased donation and recipient-related issues are commonly intertwined with conversation about policies, such identification may also help mitigate the misclassification of those topics and reduce the number of entirely irrelevant comments that are erroneously classified as related. Other methods, such as multi-task learning models, could make predictions for comments based on their media source without requiring an independent model for each source.

Additionally, we assumed that each comment should be read independently to aid the model classification. However, it is sometimes possible to maintain an association between comments. For example, in Reddit, each comment has an ID, and if it is a reply, there is a parent ID connecting it to the original comment to which the user is replying. By using this association, the assumption of independence may not be necessary, because it can be better understood that the comment is being written (or not written) in the context of LKD. This would likely help reduce the number of comments which—alone—do not contain enough information to determine their relevance to LKD (“insufficient information”).

We observed that there was very little propagation of myths or blatantly false ideas. Among comments that discussed deceased donation (ie, that were unrelated to LKD), there were cynical comments that doctors might reduce life-saving efforts for a dying patient so that an organ could be harvested quickly. While cynicism or frustration with personal experiences appeared in some related comments, misconceptions about LKD were usually nested in expressions of fear or concern (the “risk of donation” category, for example). We suggest that users are more likely to have no (or very little) information about LKD than to have incorrect information. The comments generally indicated that people were curious and prone to ask questions about LKD and wanted to make suggestions about how to increase the number of living donors.

We also acknowledge that more comments could be added to the training data, as the given number of labeled comments was a result of the time-consuming nature of the annotation process. In this exploratory study, we focused on estimating the necessary sample size through a human-annotation process and defining possible labels for the first time. The labeled comments are available upon request from the authors. Finally, we acknowledge that this data is not necessarily representative of all populations. Though internet access continues to expand globally, the distribution of users is not uniform, and each source will have different user bases. For example, according to the 2022 Global Digital Overview Report [36], Reddit users are twice as likely to be men than women, and other studies, discussed in Amaya et al [37], have estimated that between 80% and 90% of global Reddit users are aged 18 to 34 years. Each other source is likely to have its own unique demographic features that should be considered when making inferences from the data.

There is a significant need to understand why people do or do not choose to be living kidney donors. Although prior literature has made contributions toward understanding the context surrounding donation, there is no publicly available data set with information about the thoughts of the broader population on the matter. This project has taken one step toward filling this gap by scraping 203,219 unique internet user comments and tweets and developing a machine-learning classification model to identify comments related to LKD. The documents classified as relevant to LKD were compiled into a single database and are available upon request from the authors. With this database, the groundwork has been laid for more comprehensive analysis of the feelings and ideas that people have surrounding LKD. The data could also be used to identify common misconceptions about donation or information that could lead to changing minds. While rigorous classification of decision-making factors remains to be performed, the findings from this study show that machine learning is a promising tool for the capture and classification of internet comments related to LKD.",https://medinform.jmir.org/2022/11/e37884
Clement & Maldonado 2021,Augmenting the Transplant Team With Artificial Intelligence: Toward Meaningful AI Use in Solid Organ Transplant,"Advances in systems immunology, such as new biomarkers, offer the potential for highly personalized immunosuppression regimens that could improve patient outcomes. In the future, integrating all of this information with other patient history data will likely have to rely on artificial intelligence (AI). AI agents can help augment transplant decision making by discovering patterns and making predictions for specific patients that are not covered in the literature or in ways that are impossible for humans to anticipate by integrating vast amounts of data (e.g. trending across numerous biomarkers). Similar to other clinical decision support systems, AI may help overcome human biases or judgment errors. However, AI is not widely utilized in transplant to date. In this rapid review, we survey the methods employed in recent research in transplant-related AI applications and identify concerns related to implementing these tools. We identify three key challenges (bias/accuracy, clinical decision process/AI explainability, AI acceptability criteria) holding back AI in transplant. We also identify steps that can be taken in the near term to help advance meaningful use of AI in transplant (forming a Transplant AI Team at each center, establishing clinical and ethical acceptability criteria, and incorporating AI into the Shared Decision Making Model).
","Toward Meaningful Use of AI in Transplant
Drawing on the
Drawing on the definition of meaningful use of electronic health records, we define meaningful use of AI as the application of AI to guide clinical decision making and improve patient outcomes in transplantation. Most of the algorithms that have been developed in transplant only ever touch retrospective data and are never put into clinical practice. Reaching meaningful use of AI in transplant is an interdisciplinary challenge that requires careful work and requires practice, discussion, and research on issues that can begin immediately.

At the Transplant Center Level: Assemble a Transplant AI Team Just as Quality Improvement is most effective when conducted by a team with a deliberate process, AI deployment should be done by a team potentially consisting of clinicians, administrators, AI experts, and ethicists. Every center should consider assembling a Transplant AI team spearheaded by an “AI Champion” to evaluate AI tools as they become available (or to coordinate the development of in-house tools as needs are identified). QI is a learned skill, and we anticipate that AI evaluation, deployment and adoption is likely to be a learned skill as well. Supplementary Table 2 lists some individuals to consider including on the Transplant AI team; the exact composition of the team may fluctuate based on the project, but members might include surgeons, nephrologists and hepatologists, infectious disease specialists, pharmacists, nurses, hospital administrators, data analytics personnel, biostatisticians, human-computer interaction researchers, and ethicists.
Although many pre-built AI systems exist (or can be purchased), it will often be beneficial to bring in an expert to assist with developing or evaluating an AI tool. A university’s biostatistics division is also a good place to start a discussion, but experts willing to collaborate are often available across any research university (e.g., within the statistics, computer science, data science, or information systems departments).

At the Field Level: Establishing Clinical and Ethical Acceptability Criteria When is an AI good enough? Currently, there is no agreement about how to evaluate an AI for adoption. It may be worthwhile for journals to set a minimum standard for evaluation metrics reported for AI methods, and a valuable contribution with any AI paper will be to compare the results from the new model to the current standard of practice.
Whether an AI is acceptable to put into practice is a function of how accurate it is for a given patient population, and the potential benefit it offers over the status quo (of an unaided human decisionmaker) weighed against the downside risk if the AI is incorrect. The field of transplant would benefit from having criteria or guidelines to assist with these decisions. One metric to consider is the degree to which human experts agree with the AI’s predictions; this metric has been widely used to evaluate AI, including in transplant (18). However, any cutoff for human agreement is arbitrary and just because experts agree with the AI does not mean it is right; likewise, disagreement with the AI does not mean that it is incorrect in a given case. Another metric to consider is the accuracy of the AI compared to the typical human decisionmaker. Finally, the cost/benefit of the AI should be considered, the calculation of which will vary based on context.

At the Field Level: Incorporate AI Into the Shared Decision Model The Shared Decision Making (SDM) Model involves the patient in clinical decisions (19). The SDM offers potential benefits in transplant care because of the chronic nature of care, the complexity of the condition, and the role of the patient in managing their own care, but how AI is incorporated into shared decisions is unknown. When and how should you discuss the clinical team’s use of AI with patients? Consider a patient who is asking questions about their current immunosuppression regimen (eg, the need for steroids or current tacrolimus dose). It is unknown how a given patient will respond if they are told that AI was a factor in a clinical decision (such as which immunosuppressant is recommended). Perhaps sharing that an AI trained on millions of datapoints concurs with a recommendation will help improve buy-in (20). On the other hand, there is evidence that people trust doctors less when they rely on expert-based AI systems, so revealing that AI was consulted may reduce patient trust and buy-in (21).
There are also questions about ethical requirements to disclose when AI was utilized. However, we note that ethical norms do not require clinicians to disclose when they sought a second opinion from a colleague.",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8226178/
Strauss et al. 2023,Artificial intelligence-based clinical decision support for liver transplant evaluation and considerations about fairness: A qualitative study,"Background:
The use of large-scale data and artificial intelligence (AI) to support complex transplantation decisions is in its infancy. Transplant candidate decision-making, which relies heavily on subjective assessment (ie, high variability), provides a ripe opportunity for AI-based clinical decision support (CDS). However, AI-CDS for transplant applications must consider important concerns regarding fairness (ie, health equity). The objective of this study was to use human-centered design methods to elicit providers’ perceptions of AI-CDS for liver transplant listing decisions.

Methods:
In this multicenter qualitative study conducted from December 2020 to July 2021, we performed semistructured interviews with 53 multidisciplinary liver transplant providers from 2 transplant centers. We used inductive coding and constant comparison analysis of interview data.

Results:
Analysis yielded 6 themes important for the design of fair AI-CDS for liver transplant listing decisions: (1) transparency in the creators behind the AI-CDS and their motivations; (2) understanding how the AI-CDS uses data to support recommendations (ie, interpretability); (3) acknowledgment that AI-CDS could mitigate emotions and biases; (4) AI-CDS as a member of the transplant team, not a replacement; (5) identifying patient resource needs; and (6) including the patient’s role in the AI-CDS.

Conclusions:
Overall, providers interviewed were cautiously optimistic about the potential for AI-CDS to improve clinical and equitable outcomes for patients. These findings can guide multidisciplinary developers in the design and implementation of AI-CDS that deliberately considers health equity.","This study systematically assessed perceptions about the design of fair AI-CDS for LT evaluation in 53 stakeholder respondents across 2 LT centers. Overall, future users of AI-CDS were cautiously optimistic about the potential for fair AI-CDS to improve outcomes for patients. From the data, we created a conceptual model that shows the relationships between the emergent themes and standards for AI-CDS, such as explainability, transparency, fairness, and trustworthiness (Figure ​(Figure2).2). While the perceptions are specific to this case study domain of LT evaluation, we used them to identify potentially generalizable recommendations for fair AI-CDS (Table ​(Table4).4). These recommendations can be considered from 2 perspectives. First, the perspective of the provider or potential user can consider these best practices as a guide for gauging the fairness of AI-CDS. Second, the perspective of the developer/implementer (eg, data scientists, engineers, and clinicians) can consider these best practices when designing, developing, and implementing fair AI-CDS. As AI becomes more widespread, providers are increasingly involved in its development, and this study demonstrates the benefits of a user-centered or human-centered design approach. Our participants expressed the perspective that the responsible use of AI-CDS includes an understanding by the user that people created the data and algorithms.25 Additionally, regardless of the design, the user has a responsibility to use the tool in the best interest of the patient,29 which includes the equitable use of the AI-CDS.30 The tool may give outputs or recommendations that a provider or a group of providers will use to enhance their decisions.31 Aligned with this concept, this study demonstrated participants want to work alongside AI-CDS. They described AI-CDS as akin to adding a new transplant team member as opposed to replacing their roles as decision-makers. Similar to trusting and professional relationships that develop between team members, participants valued transparency, knowledge, and familiarity of AI-CDS interactions and using it to build trust in the system. Explainability can be paralleled to human-to-human trust building through clear and honest communication. They voiced concern about the data not capturing all relevant aspects and a human component being necessary to cross-check the fairness of the AI-CDS output. While our findings are consistent with the literature about providers valuing the revolutionary potential of AI more than they fear being replaced,32,33 others have found providers with less AI understanding are more concerned about being replaced.34 Since setting and background AI exposure may influence user attitudes and adoption, understanding user perceptions of the human-machine interaction is key for AI-CDS design and implementation.

In this study, participants described fair AI-CDS might help with equity by identifying specific resources needed for each patient. Using AI-CDS to recognize SDOH patterns and targeting resources is in parallel with other literature using big data from observational data sets35 and the concept of precision medicine. Precision medicine defines subgroups of patients and targets therapy to them individually to improve outcomes.36 Although the concept of precision medicine is typically applied to recognizing genomic patterns, it may be applicable to SDOH as well. Our findings suggest that users want a shift from offering the same set of resources for all patients (ie, equality) to harnessing the power of AI-CDS for more personalized resource allocation (ie, equity). This potential for individualization of care was identified by participants when they described future potential target points in the process map (Figure ​(Figure1)1) where rich SDOH data could be collected (eg, social worker assessments) and decisions could be made (eg, triaging at time of referral, evaluation clinic, and committee meeting).

Our findings that patients should play a role in the use of AI-CDS is consistent with literature about patient considerations being important for building trust in AI.29 Some participants felt the patients should provide feedback in the development of the tool. While this involvement would add insights to the tool, the timing of patient feedback depends largely on the clinical application. Importantly, this case study was for an AI-CDS designed purely for use by clinicians and was early in the design phase, so patient feedback without more concrete example would have been difficult. Future studies incorporating patients in the fair design of AI-CDS would be helpful for patient-facing tools or assessment in the deployment phase (eg, ease of provider explaining tool output to patients). Importantly, due to updated guidance from the Food and Drug Administration, patient engagement may be required for particular AI-CDS if it is considered a medical device.37,38

Our findings may be transferable39 to other contexts, particularly given the large sample size, adequate response rate, and dual-center perspectives. The context for this study was specific to LT and participants were within academic settings. Due to the rapidly evolving landscape of transplantation and research environment, these users may be more open to AI-CDS. AI-CDS being designed for users outside these settings may have different perspectives. Other similar fields might be oncology treatment or life-saving decisions in resource-limited settings (eg, monoclonal antibody use for COVID-19 infection). Indeed, since all clinical decisions affect patients from various backgrounds, the framework proposed here was kept broad so it could be considered widely and still be applicable. Study limitations

This study should be considered with its limitations. The providers interviewed for this study had limited experience with AI-CDS, so this may have informed their responses and users with more experience may respond differently. Although the sample size was not large enough to identify themes specific to AI-CDS experience or provider type on the transplant team, future work could explore these areas. Additionally, the recommendations are limited by this application and should be expanded on based on additional insights from other domains. This study was with transplant team members and not patients, so the results do not include the patient perspective. Since the goal of this study was to understand the perceptions of the users of the tool, which are transplant team members, patient perspectives should be investigated in future work. Follow-up work including patient comprehension about AI-CDS and provider communication about the outputs would provide additional complementary knowledge.

This multicenter study deployed a human-centered design approach to understand perspectives of users about a fair AI-CDS tool specific to their domain. Overall, providers are cautiously optimistic for AI-CDS if it is developed ethically, transparently, and in collaboration with clinical experts. This qualitative work highlights the gaps in knowledge and is hypothesis-generating for future research on AI-CDS in transplantation. Our findings can be harnessed not only by developers, but also by users, administration, and policymakers that may be involved in developing, deploying, and evaluating these tools.",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10497243/
Medved et al. 2018,Improving prediction of heart transplantation outcome using deep learning techniques,"The primary objective of this study is to compare the accuracy of two risk models, International Heart Transplantation Survival Algorithm (IHTSA), developed using deep learning technique, and Index for Mortality Prediction After Cardiac Transplantation (IMPACT), to predict survival after heart transplantation. Data from adult heart transplanted patients between January 1997 to December 2011 were collected from the UNOS registry. The study included 27,860 heart transplantations, corresponding to 27,705 patients. The study cohorts were divided into patients transplanted before 2009 (derivation cohort) and from 2009 (test cohort). The receiver operating characteristic (ROC) values, for the validation cohort, computed for one-year mortality, were 0.654 (95% CI: 0.629–0.679) for IHTSA and 0.608 (0.583–0.634) for the IMPACT model. The discrimination reached a C-index for long-term survival of 0.627 (0.608–0.646) for IHTSA, compared with 0.584 (0.564–0.605) for the IMPACT model. These figures correspond to an error reduction of 12% for ROC and 10% for C-index by using deep learning technique. The predicted one-year mortality rates for were 12% and 22% for IHTSA and IMPACT, respectively, versus an actual mortality rate of 10%. The IHTSA model showed superior discriminatory power to predict one-year mortality and survival over time after heart transplantation compared to the IMPACT model.","The purpose of this study was to compare the IMPACT and IHTSA models with regards to the prediction accuracy of one-year mortality on the UNOS database. There exist some biases in both models when used on the UNOS data set for the time era 1997–2008. Because IMPACT was developed on these data and IHTSA on the ISHLT dataset, which consists in part of the same UNOS data, the models may be subjected to a non-negligible overfit to the data, skewing the result towards a more positive value. Therefore, we chose to validate the models on a later time era, which has no overlapping patients with the training set.

The results show that the IHTSA model exhibited improved performance and accuracy compared to the IMPACT model. Even though IMPACT was designed to predict one-year mortality and IHTSA was created for long-term survival, IHTSA shows better discrimination on one-year mortality.

This study could also prove the benefits of using deep learning modelling techniques. Such techniques are inspired by the human brain. They consist of a network of “neurons” that emulate the properties of their real counterparts. Using multiple processing layers makes it possible to learn representations of data with multiple levels of abstraction7. These methods have improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains8.

Our results show that the IHTSA model can be applied to predict short-term mortality with greater accuracy than a more traditional risk-based model based on logistic regression. Although the comparison of ROC curves to evaluate models in a statistically valid manner is controversial, the ROC curve is currently the most developed statistical tool for describing performance9,10. The improvements seen can be explained by the difference in the variable selection, such as the absence of donor risk factors in the IMPACT model, but also by the the neural network’s ability to handle interactions between variables and nonlinearities. An increased donor age has in previous reports been shown to have a negative influence on short-term survival6,11. To examine this, we compared the difference of the deep learning model and the logistic regression model using the same variables. Here, we show a substantial improvement when using the deep learning approach compared with the traditional approach. Furthermore, we could show that the predictive availability for the deep learning model was less dependent on the variables included compared with a standard model. Donor variables showed to be of less importance than expected. A possible explanation for that may be the deep learning technology has an increased ability to identify new patterns with the data it has available. It is interesting to note that the two models do not show a considerable overlap of features. Only five features are shared by the two models out of 18 for IMPACT and 43 for IHTSA. If we compare the overlapping variables with the seven most important variables for IHTSA, we find that three of them are shared: age, diagnosis, and mechanical ventilation6.

One disadvantage of the deep learning technique is that it yields a black box model with a limited ability to explicitly identify possible causal relationships. Logistic regression, on the contrary, makes it feasible to determine the strongly predictive variables based on the size of the coefficients. To cope with the lack of a well-established method for interpreting the weights of a connection matrix in a neural network, the developers of the IHTSA algorithm used a classification and regression tree (CART), fitted to the predicted median survival time, to assess the relative importance of the features6. Furthermore, the web-based calculator (http://ihtsa.cs.lth.se) makes it possible to estimate the survival on a computer or mobile device.

During 2011, approximately 17,000 donors were reported12. Unfortunately, not more than one-third of all donors could be utilised for heart transplantation. One explanation for this may be the uncertainty in the risk of early and late graft dysfunction, which means that some suitable donors are not accepted. Although there are many donor predictors of allograft discard in the current era, these characteristics seem to have little effect on recipient outcomes when the hearts are transplanted, which also is confirmed in this study13. A more liberal use of cardiac allografts with relative contraindications may be warranted. A calculator would allow us to conveniently perform batch estimation of survival for multiple patients at the same time. This would allow the IHTSA model to be used as a virtual recipient-donor matching tool that models survival for potential recipients on a waiting list when there is a donor heart available. This could potentially increase the number of organs that could be used compared with a traditional criterion-based model6. Additionally, it will make it easier for other research groups to validate the model.

The results of this study carry limitations associated with the retrospective analysis of a registry database, the quality of the source data, the number of missing data, and the lack of standardization associated with multicenter studies (such as different immunosuppressive regimens and different matching criteria). However, those limitations are the same for both models. Even if a comparison of risk models remains controversial, the C-index is probably the best statistical tool for describing performance. A C-index of <0.7 may seem low, but it should be kept in mind that the IHTSA model predicts long term survival, and to the best of our knowledge, it is higher than previously reported studies.",https://www.nature.com/articles/s41598-018-21417-7
Li et al. 2023,A Transformer-Based Deep Learning Approach for Fairly Predicting Post-Liver Transplant Risk Factors,"Liver transplantation is a life-saving procedure for patients with end-stage liver disease. There are
two main challenges in liver transplant: finding the best matching patient for a donor and ensuring
transplant equity among different subpopulations. The current MELD scoring system evaluates a
patient's mortality risk if not receiving an organ within 90 days. However, the donor-patient matching
should also take into consideration post-transplant risk factors, such as cardiovascular disease, chronic
rejection, etc., which are all common complications after transplant. Accurate prediction of these risk
scores remains a significant challenge. In this study, we will use predictive models to solve the above
challenge. We propose a deep learning framework model to predict multiple risk factors after a liver
transplant. By formulating it as a multi-task learning problem, the proposed deep neural network was
trained on this data to simultaneously predict the five post-transplant risks and achieve equally good
performance by leveraging task balancing techniques. We also propose a novel fairness achieving
algorithm and to ensure prediction fairness across different subpopulations. We used electronic health
records of 160,360 liver transplant patients, including demographic information, clinical variables, and
laboratory values, collected from the liver transplant records of the United States from 1987 to 2018.
The performance of the model was evaluated using various performance metrics such as AUROC,
AURPC, and accuracy. The results of our experiments demonstrate that the proposed multitask
prediction model achieved high accuracy and good balance in predicting all five post-transplant risk
factors, with a maximum accuracy discrepancy of only 2.7%. The fairness-achieving algorithm
significantly reduced the fairness disparity compared to the baseline model.","In conclusion, our multitask learning model with task balancing and fairness-achieving algorithms
offers a promising approach to predicting multiple risk factors post-liver transplantation. The model's
ability to balance task performance and address demographic disparities can lead to more accurate and
equitable predictions. However, researchers and practitioners should be mindful of the limitations in
the data and potential biases that cannot be addressed by the algorithm alone. To fully realize the
potential of deep learning for risk prediction, ongoing efforts to improve data quality,
representativeness, and the development of robust fairness-enhancing techniques are essential.
One limitation of the study is the reliance on the limited available dataset, which may not
comprehensively represent all demographic subgroups. This lack of representativeness could lead to
biased predictions for underrepresented groups, potentially exacerbating existing disparities in
healthcare outcomes. To address this issue, future research should aim to collect more diverse and
inclusive datasets, ensuring that all relevant demographic groups are adequately represented and
accounted for in the model.
Additionally, the fairness-achieving algorithm may not fully address all potential biases and unfair
issues, as some disparities may arise from factors not captured in the dataset. For example, there may
be unobserved variables, such as socioeconomic status or access to healthcare, that contribute to
disparities in post-transplantation outcomes. To mitigate these limitations, researchers should consider
incorporating additional data sources and features that capture a broader range of factors influencing
health outcomes.
Unmeasured confounders and data quality issues may also impact the model's performance and
fairness. Inaccurate or incomplete data can lead to biased predictions, undermining the model's ability
to provide equitable risk assessments. To address these concerns, researchers should prioritize data
cleaning and preprocessing techniques that minimize errors and inconsistencies in the dataset.
Moreover, efforts should be made to validate the model's performance using external datasets and realworld clinical settings, ensuring that the model is both accurate and generalizable.
In summary, while our multitask learning model with task balancing and fairness-achieving
algorithms shows promise in predicting multiple risk factors post-liver transplantation, it is crucial to
acknowledge and address the limitations in the data and potential biases. By improving data quality,
representativeness, and incorporating robust fairness-enhancing techniques, we can work towards more
accurate and equitable risk predictions that ultimately contribute to better healthcare outcomes for all
patients.",https://arxiv.org/abs/2304.02780
Zadeh et al. 2024,Building analytical models for predicting de novo malignancy in pancreas transplant patients: A machine learning approach,"This research aimed to predict de novo malignancy in patients who underwent pancreas transplants using a machine learning approach. We constructed various predictive models based on data from the Organ Procurement and Transplantation Network (OPTN) to classify malignant patterns in transplant patients. The models were trained using medical records spanning from 1987 to 2020 to identify essential prognostic components associated with malignancy development. Various datasets from the United Network for Organ Sharing Standard Transplant Analysis and Research (UNOS-STAR) database were used to evaluate each model's performance using the areas under the receiver operating characteristic (ROC) and precision-recall (PR) curves. Our findings demonstrated the effectiveness of machine learning in predicting de novo malignancy in pancreas transplant patients. Specifically, we identified the recipient’s B2 and DR1 antigens and the donor’s DDAVP hormone as significant factors associated with malignancy. Our study highlights the importance of integrating medical records from various sources to enhance the accuracy of predictive models for organ transplantation."," summary of
A summary of the models' performance is presented in Table 3. It lists the average accuracy, sensitivity, specificity, precision, and F1-score for each classifier, based on 5-fold cross-validation. Accuracy measures the overall correct classification rate, precision quantifies the proportion of true positive predictions out of all positive predictions, sensitivity assesses the percentage of true positive predictions out of all actual positives, specificity calculates the proportion of true negative predictions out of all actual negatives, and the F1-score is the harmonic mean of precision and sensitivity. The results indicate that gradient boosting achieved the highest accuracy rate, outperforming other models in terms of sensitivity and precision as well. Furthermore, the models were evaluated using the AUROC model assessment measure. Table 3 demonstrates that the AUROC measure supports the conclusion that the gradient boosting model has the superior performance compared to all the predictive models under consideration. We further investigated the precision-recall (PR) curves, known for their suitability in imbalanced datasets. The PR curve effectively underscores our model's capability to precisely discern positive cases, particularly in our context, where we concentrate on identifying malignancies. The outcomes reaffirmed the superior performance of the gradient boosting model relative to the other predictive models considered in this study (for detailed information, refer to the supporting files).

After analyzing the results of each predictive model, it became evident that there are several noteworthy similarities in the significant variables that were identified by each model (see the supporting files for details). Table 4 presents the top 50 variables that are crucial in detecting a post-transplant malignancy in patients with pancreas transplants, as determined by the split-based approach within the SAS Enterprise Miner’s gradient boosting model. The values listed in Table 4′s validation importance column indicate the relative importance of each variable on the validation set, with a score of one signifying the highest level of significance. The detailed lists of important variables identified by other machine learning models utilized in this research can be found in the supporting files provided for this article. Several interesting findings can be derived from the preceding table. First, recipient age emerges as a crucial factor directly influencing post-transplant malignancy and appears to be the most significant variable. Patients with malignancy exhibit a significantly higher average age of 42 years (p < 0.05) compared to patients without malignancy, who have an average age of 37 years. In general, the likelihood of an individual being diagnosed with different types of cancer tends to increase with his or her age. This is because as individual age, their cells are subjected to a greater number of environmental factors leading to higher mutations accumulate over time, making the cells more susceptible to becoming cancerous. Additionally, our immune system becomes less effective at identifying and destroying abnormal cells, which can also increase the risk of cancer (Blackadar, 2016).

The next important variable is PEAK_PRA, which stands for peak panel reactive antibody, measures the highest level of antibodies that a patient has produced against human leukocyte antigens in response to previous exposure. Although high levels of pretransplant panel reactive antibodies do not directly indicate the presence of malignancy, they are used as an indicator of an individual's level of sensitization to HLA antigens, which is important in the context of organ transplantation. Malignancy is a multifaceted disease with various risk factors and causes, and while cancer patients may have high levels of antibodies against HLA antigens due to their treatment or cancer, prior studies suggested that there is no direct causal relationship between pretransplant panel reactive antibodies and malignancy (Singh et al., 2003). It is worth noting that this variable was statistically significant (p < 0.0001), indicating its potential importance in predicting post-transplant malignancy.

The donor's Arginine Vasopressin (AVP) hormone, also referred to as antidiuretic hormone (ADH), is the third most important factor in predicting de novo malignancy, and its effects became statistically significant (p < 0.0001) according to the LARS model. AVP is made by the hypothalamus and subsequently released by the pituitary gland. It plays a key role in regulating blood pressure and water retention by minimizing urine or more reabsorption in proximal tubule, thus maintaining proper hydration levels. Additionally, AVP has vasoconstrictive effects that can elevate blood pressure.(Cuzzo, Padala, & Lappin, 2021). There is evidence to suggest that AVP may contribute to the development and progression of certain malignancies. One-way AVP may contribute to malignancy through its effect on angiogenesis, which is the formation of new blood vessels. AVP has been shown to increase angiogenesis in some studies, which can promote tumor growth and metastasis (Ripoll et al., 2020, Sinha et al., 2020). AVP has also been involved in the modulation of tumor cell migration and invasion. Some studies have found that AVP can enhance the invasive properties of cancer cells, particularly in breast cancer (Keegan, Akerman, Péqueux, & North, 2006) and prostate cancer (Zhao et al., 2019). Despite these findings, the exact role of AVP in malignancy is not yet fully understood, and more research is needed to fully elucidate its effects.

DONOR DR1 ANTIGEN is one of the human leukocyte antigens (HLA) that are important in organ transplantation because they determine whether a donor organ is compatible with the recipient's immune system. While HLA antigens, including DR1, have been implicated in certain cancers and autoimmune disorders, there is no clear evidence to suggest that DR1 antigen alone is directly associated with malignancy. Prior research suggests that the presence or absence of DR1 antigen in a donor may influence the compatibility of the donor organ with the recipient's immune system, but it does not affect the risk of malignancy in either the donor or the recipient (Whittington, Prislovsky, Beaty, Albritton, Radic, & Rosloniec, 2022).

The fifth important variable identified as statistically significant (p < 0.0001) by both gradient boosting model and the LARS model is donors’ pulmonary infection (PULM_INF_DON). There is evidence to suggest that there may be a relationship between pulmonary infection and malignancy. While the precise relationship is not fully characterized, several studies have reported associations between the two. One study published in the Journal of Thoracic Oncology in 2012 found that patients with pulmonary cancer were more likely to have a history of pulmonary infections, such as pneumonia or tuberculosis, compared to patients without lung cancer. The study also found that the presence of pulmonary infections was associated with more advanced stages of lung cancer at diagnosis (Lande et al., 2012). Another study published in the International Journal of Cancer in 2014 found that individuals with a history of pulmonary infections had a greater risk of developing lung cancer compared to patients that had no history of infection. The study also reported that the incidence of lung cancer was higher among patients with a history of severe infections compared to those with mild infections (Etzioni & Gulati, 2016). While the exact mechanisms behind the association between pulmonary infections and malignancy are not yet fully understood, some theories suggest that chronic inflammation associated with infections may promote the development and progression of cancer. It is worth mentioning that while the correlation between pulmonary infection and malignancy has been investigated in individuals who have a history of both, it has not been studied in recipients who receive transplants from an infected person.

Desmopressin which is naturally produced by the pituitary gland is a synthetic analog of vasopressin. Desmopressin works by enhancing water reabsorption in the collecting tubules, thereby decreasing urine production (Cuzzo, Padala, & Lappin, 2021). Our study found a correlation between lower levels of desmopressin and increased malignancy. Desmopressin has been identified as a safe hemostatic agent that may possess antitumor effects. Its mechanism of action involves selectively activating the V2 vasopressin membrane receptor (V2r), which is expressed in both cancer cells and the circulatory system around it (Garona et al., 2015).

Among other variables, SGOT_DON is an interesting variable that has been identified as statistically significant by both the gradient boosting and LARS models (p < 0.0001). SGOT (serum glutamic oxaloacetic transaminase), also recognized as AST, is an enzyme found predominantly in the liver, but also in other organs such as the heart, muscles, and kidneys. SGOT_DON is a blood test that quantifies this enzyme in the blood of the donors. Elevated levels of SGOT/AST (serum glutamic oxaloacetic transaminase) in the blood can be indicative of liver damage or disease. While there is some evidence to suggest that elevated SGOT/AST levels may be associated with malignancy (Albhaisi & Qayyum, 2021). Some studies have found that elevated SGOT/AST levels were associated with a higher risk of Urological malignancies (Laukhtina et al., 2021, Su et al., 2020). Another study published in the Journal of Clinical Oncology in 2018 found that elevated SGOT/AST were correlated with a higher incidence of death in patients with advanced cancer, regardless of cancer type. The study suggested that elevated SGOT/AST levels may be a marker of systemic inflammation and poor prognosis in advanced cancer (Lewis, Khaldoyanidi, Britten, Wei, & Subklewe, 2022). According to our study, there is a positive association between SGOT/AST levels and the likelihood of developing de novo malignancy.

Recipient’s B2 (RB2) Antigen is another important variable that our models have identified as statistically significant with a significance level of p < 0.0001. B2 cells, also known as B lymphocytes, are a type of immune cells that recognizes a foreign antigen by producing antibodies to fight against foreign substances, such as viruses and bacteria. B2 cell activation refers to the process by which B2 cells are stimulated to produce antibodies in response to the presence of an antigen, which is a substance that the body recognizes as foreign. The presence of B2 antigen in recipients may have some effect on de novo malignancy. There is a complex relationship between malignancy (cancer) and B2 cell activation. In general, B2 cells are essential in recognizing cancer cells and destroying them. However, in some cases, B2 cells can also contribute to the development and progression of cancer. For example, chronic B2 cell activation and inflammation can create an environment that is conducive to the growth and spread of cancer cells. In addition, some types of cancer cells can secrete molecules that can activate B2 cells and promote their survival, leading to the production of more antibodies and cytokines that can further promote cancer growth (Kaliss, 1958, Liu et al., 2019). Bregs B cells, also known as regulatory B cells, are a specialized subgroup of B cells that suppresses the immune response. They can produce cytokines and other signaling molecules that suppress the activity of other immune cells, including T cells and natural killer cells, which are important for controlling tumor growth. In addition, Bregs B cells can also promote the development of immune tolerance to the tumor, which can further suppress the immune response (Abebe, Dejenie, Ayele, Baye, Teshome, & Muche, 2021).

On the donor side, Donor’s B1 Antigen seems to be an important variable for malignancy in patients with pancreas transplant. The relationship between de novo malignancy and B1 Antigen has been a topic of interest in the field of cancer research for many years, however, little is known in this specific research area. B1 Antigen is a protein found on the surface of certain immune cells and can be used as a marker for the presence of malignancy in individuals, specifically B1 Antigen CD20 which is often found on B-cells within B-cell lymphomas (Munker, Marion, Ye, & Dreyling, 2007). Studies have shown that B1 Antigen is often overexpressed in de novo malignancy. The expression of B1 Antigen in various types of malignancy, including lymphoma and leukemia, has been investigated and found that the protein was significantly overexpressed in de novo cases compared to those with a history of malignancy. Additionally, B1 Antigen is suspected to play a role in the development and progression of de novo malignancy. Researchers have discovered that the expression of B1 Antigen was significantly more than that of a healthy control (Nadler et al., 1981). The relationship between de novo malignancy and B1 Antigen is significant and complex. The results from the regression model suggest that patients with de novo malignancy exhibit substantially higher expression levels of the B1 antigen (p < 0.01). Further research is needed to fully grasp to the extent at which this difference in expression occurs as well as the overarching relationship between the two topics in question.

In this study, we examined the impact of various attributes related to body mass index (BMI) on predicting post-transplant pancreas malignancy. These attributes encompassed: BMI_CALC, representing the body mass index of the transplant recipient and reflecting the patient's overall health status; BMI_DON_CALC, indicating the body mass index of the organ donor before or at the time of donation, potentially influencing the transplant outcome; BMI_TCR, denoting the body mass index of the transplant candidate at the time of listing for the pancreas transplant; and END_BMI_CALC, signifying the calculated body mass index of the transplant candidate at the time of pancreas removal or the current time. Our analyses using gradient boosting and LARS models revealed that BMI-related attributes significantly impacted the prediction of post-transplant pancreas malignancy (p < 0.0001), thus highlighting their critical role as predictive factors. Notably, previous research has shown that higher BMI can contribute to the development of certain types of cancer types such as colon, kidney and breast cancer (Li, Han, Liu, Leng, & Dong, 2015), providing additional context to our findings.

In addition, our results suggest that the amount of lipase enzyme in the individual’s blood is associated with post-transplant malignancy (p < 0.0001). Lipase is an enzyme that is essential for the digestion of fats, and it is produced in the mouth, stomach, and pancreas. The pancreas, in particular, releases lipase as part of its exocrine function, which helps break down macronutrients in the bowels. Lipase also plays a crucial role in cell signaling and can be present at points of inflammation. Abnormal levels of lipase can indicate a failure of these necessary functions and may require further investigation.

In clinical settings, serum lipase levels are monitored to determine the progression of the pancreas’s inflammatory response to diagnose these issues. Prior research found that in high levels lipase can be used as a biomarker to determine the progression of possible malignancies present such as pancreatic cancer, pancreatitis, panniculitis, and polyarthritis (Miksch et al., 2020, Oh et al., 2017). In some cases, cancer can cause inflammation in the pancreas, which can lead to elevated lipase levels. For example, pancreatic cancer can cause pancreatitis, which can result in increased lipase levels (Ventrucci, Pezzilli, Gullo, Platé, Sprovieri, & Barbara, 1989).

In summary, this study demonstrated that through analysis of pre- and post-transplant data over time, machine learning algorithms can detect significant transplant outcomes, such as cancer, malignancy, or graft failure. Our results can be used to generate personalized recommendations for patients, enabling healthcare providers to identify those who may need specialized clinical attention. This study confirmed that patient’s B2 antigen, donor’s arginine vasopressin, DR1 antigen, synthetic anti diuretic hormone (DDAVP), lipase terminal lab SGOT (AST) are among the most important variable to monitor in patients with a pancreas transplant.

Our machine learning algorithms can be seamlessly integrated into major EHR providers such as Epic and Cerner, complementing existing clinical decision support systems that physicians currently use (Chen & Decary, 2020). This integration enables healthcare providers to efficiently identify patients at risk of de novo malignancy after organ transplantation. Notably, these HER providers actively promote the adoption of predictive algorithms within their system by offering financial incentives to hospitals. This initiative encourages the implementation of advanced predictive models, leading to improved patient care and better outcomes.

It is recommended to consider implementing guidelines for regular malignancy screening after a pancreas transplant. These guidelines can aid healthcare providers in identifying individuals who require specialized clinical care, and it is recommended to implement similar protocols for malignancy screening after a pancreas transplant. For instance, incorporating condition-outcome rules similar to those presented in the supporting file of this article would be valuable for understanding the predictors of post-transplant pancreas malignancy. By following these if-else premises, healthcare providers can easily assess the likelihood of post-transplant malignancy based on specific ranges of the top variables. For example, if the values of the attribute “Age” fall below 42.5, the likelihood of post-transplant malignancy can be as high as 80%. These condition-outcome rules offer critical insights for better risk assessment and personalized predictions, enabling clinicians to make informed decisions and provide appropriate care to patient’s post-transplant.

This research has limitations that provide suggestions for future work. Firstly, our machine learning models only relied on numerical transplant data, whereas text-based diagnosis notes data is crucial for predicting de novo malignancy. Future studies, therefore, should aim to analyze both textual and non-textual data to predict de novo malignancy. Secondly, this study did not differentiate between the different types of de novo malignancies. Future studies can leverage the presented models to explore various types of malignancy cancers and provide practical recommendations specific to each type. Thirdly, it would be beneficial to explore other predictive algorithms and variable selection methods, such as Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs), and Recursive Feature Elimination (RFE), to improve accuracy and effectiveness of de novo malignancy identification and detection. Furthermore, future research can deploy other advanced attribute importance evaluation methods, such as the one proposed by Levashenko, Rabcan, and Zaitseva (2021) to gain deeper insights into the significance of the various attributes utilized in this research. Finally, future research should assess the effectiveness of preventative care for patients with organ transplants who are at risk of developing cancer after transplantation. This assessment can help determine the impact and efficacy of preventive innervations in reducing the incidence or progression of malignancies.",https://www-sciencedirect-com.myaccess.library.utoronto.ca/science/article/pii/S0957417423020869?via%3Dihub
Miller et al. 2022,Temporal shift and predictive performance of machine learning for heart transplant outcomes,"Background
Outcome prediction following heart transplant is critical to explaining risks and benefits to patients and decision-making when considering potential organ offers. Given the large number of potential variables to be considered, this task may be most efficiently performed using machine learning (ML). We trained and tested ML and statistical algorithms to predict outcomes following cardiac transplant using the United Network of Organ Sharing (UNOS) database.

Methods
We included 59,590 adult and 8,349 pediatric patients enrolled in the UNOS database between January 1994 and December 2016 who underwent cardiac transplantation. We evaluated 3 classification and 3 survival methods. Algorithms were evaluated using shuffled 10-fold cross-validation (CV) and rolling CV. Predictive performance for 1 year and 90 days all-cause mortality was characterized using the area under the receiver-operating characteristic curve (AUC) with 95% confidence interval.

Results
In total, 8,394 (12.4%) patients died within 1 year of transplant. For predicting 1-year survival, using the shuffled 10-fold CV, Random Forest achieved the highest AUC (0.893; 0.889-0.897) followed by XGBoost and logistic regression. In the rolling CV, prediction performance was more modest and comparable among the models with XGBoost and Logistic regression achieving the highest AUC 0.657 (0.647-0.667) and 0.641(0.631-0.651), respectively. There was a trend toward higher prediction performance in pediatric patients.

Conclusions
Our study suggests that ML and statistical models can be used to predict mortality post-transplant, but based on the results from rolling CV, the overall prediction performance will be limited by temporal shifts inpatient and donor selection.","In this study, we applied popular ML and statistical methods to predict death within the first-year post-cardiac transplant. In the shuffled 10-fold CV, we demonstrated substantial improvements in predictive performance with tree-based models. However, there was less of a difference when predictive performance was assessed using the rolling CV procedure, which likely provides a better estimate of prospective model performance. The results of our study suggest that ML and statistical models could be used to predict mortality in the first year post-transplant, but the overall prediction performance will be impacted by temporal shifts in patient and donor selection.

Efforts to improve risk stratification with clinical risk scores have demonstrated modest prediction performance. While it is unclear how accurate an approach needs to be for clinical implementation, higher model discrimination is clearly desirable. Weiss et al. evaluated a risk score, including 12 variables, which achieved a c-statistic of 0.65 in the derivation cohort.11 A separate score including 11 variables achieved a c-statistic of 0.62 in the validation cohort.12 The reported AUC of the Scientific Registry of Transplant Recipient model ranges from 0.67 to 0.83,33 but it's not clear how data was separated for training and testing for these estimates. However, given the large number of potential variables which could be considered when predicting post-transplant risk, ML methods are an attractive alternative solution. Villela et al. demonstrated an AUC of 0.66 for death or re-transplant using AutoML with stacked gradient boosting machines trained and tested in 18,612 patients undergoing cardiac transplant.18 Nilsson et al. investigated an alternate artificial intelligence approach, using deep-learning with the model achieving an AUC of 0.650 for prediction of 1-year all-cause mortality.34 The present analysis demonstrates substantial improvements in XGBoost and RF models when trained on a full range of years using shuffled CV, with AUC of 0.820 and 0.893 respectively. The present analysis demonstrates substantial improvements in XGBoost and RF models when trained on a full range of years using shuffled CV, with AUC of 0.820 and 0.893 respectively. The higher predictive performance for these models compared to linear methods in the shuffled CV may be related to the ability of tree-based methods to allow for complex and flexible modeling when using large sample sizes. Medved et al. used a simple temporal split with patients transplant before 2009 (n = 22,263) for training and patients transplanted in 2009 or later (n = 5,597) for testing.13 They demonstrated that a deep-learning model had higher predictive performance compared to an established clinical risk score (AUC 0.65 vs 0.61),13 which is similar to that demonstrated in the rolling CV procedure in the present study. Lastly, we show that prediction performance is higher for short-term outcomes (90-day survival). However, our study also highlights the importance of considering the methods used for training and testing ML models, and whether they are a valid measure of the intended application when assessing their performance. For example, with shuffled cross-validation the derived models are informed by data from past, present, and future patients which doesn't represent the intended clinical use.

In addition to the shuffled 10-fold CV procedure, we performed rolling CV. This procedure better reflects the process of generating an ML score from existing observations and applying them prospectively to a new set of patients. For example, in the shuffled CV procedure, the model was trained with patients who were transplanted before and after the patient on whom it was tested, which is not possible for prospective implementation. We demonstrated that prediction performance of ML models was significantly lower in this validation procedure compared to the shuffled 10-fold CV procedure, which is most likely a reflection of temporal shift. From the examined algorithms, LR, RF, and XGBoost performed well, superseding Cox regression. However, there was no single algorithm performing the best in every setting, thus an ensemble method like stacking could provide the optimal model.35 Given the changes that occur in donor and recipient factors over time,1 rolling CV, or another method based on temporal splitting of data may provide a more accurate estimate of prospective prediction performance. Our results for LR may reflect a best-case scenario for regression-based risk scores since it was developed with all available information and (in rolling CV) was updated to include recent patient data. Importantly, the more modest discrimination in this context has implications for how transplant center performance is assessed.36 One way to mitigate the dataset shift could be through a shorter training window. However, we did not identify an optimal time frame for including patients in the training set. Future studies could investigate different methods to account for temporal shift to improve predictions.37 Alternatively, updating models more frequently than on an annual basis could help reduce these errors and, in particular, with increased frequency when there are significant changes to organ allocation.38 Regardless of the approach, the optimal method for validation will be an important consideration for any attempts at predicting post-transplant outcomes and, similarly to this study, ML pipelines should be made public to achieve higher reproducibility.

It seems that prediction performance may be superior in pediatric patient populations. A separate study of pediatric patients in the UNOS database used a random split of patients for training (n = 2,802) and testing (n = 700). In the validation set, the RF model had the highest prediction performance (AUC 0.72) for 1-year mortality.39 Compared to this study, we were able to demonstrate superior performance with shuffled CV for the tree-based ML methods. In contrast to the adult population, the difference in prediction performance between models was smaller, potentially reflecting the smaller sample size. In the rolling CV, we demonstrated higher AUC for all methods in pediatric compared to adult populations. Although the reasons for this difference in performance are not known, it's possible that there are less significant temporal shifts in pediatric donor and recipient characteristics.

The results of variable importance ranking can also potentially inform clinical practice. Our analysis identified many variables as important predictors of risk which are traditionally considered when estimating patient risk such as ischemic time40 and recipient age.41 Additionally, all models identified measures of liver function as important predictors of outcomes,42 as well as creatinine at transplant. However, it is worth noting that the ranking of these characteristics varied between models. Interestingly, other factors such as ECMO support,43 and pulmonary hypertension44 which are typically considered clinically important were not consistently given high importance by the ML models. There were also some variables which were unexpectedly important in predictions, such as donor tattoos and Syphilis serology. While ML methods are able to account for collinearity,45 it is possible that these variables are markers for other unmeasured donor or patient behaviors. Regardless, variable importance ranking can be looked at as an excellent source for hypothesis generation, and overall, these results support the concept of considering all available donor and recipient characteristics rather than relying on 1 or 2 factors believed to carry the most clinical significance.

ML or statistical models could be prospectively implemented to improve donor selection and overall patient outcomes. Transplant programs could maintain a database with recipient characteristics and then prospectively assess risk based on each donor's specific characteristics. This information could be used by physicians when considering donor offers, particularly in the context of concerning donor or recipient risk factors. Alternatively, programs could start with an estimation of patient attributable risk, with default imputation for all donor characteristics, to determine the anticipated risk from accepting the “average” donor. This information could be used to provide patients with an individualized prediction of risk to allow a more informed consent discussion regarding the transplant process. The amount of variable entry required for ML modeling is similar to many clinical risk scores,11, 12, 13, and is already performed at many centers to assist with outcome reporting. However, simplified models with a limited number of variables could also be considered.46 Cox regression achieved the best calibration on its own, while with the rest of the methods we advise using a calibration method as part of the pipeline. However, the clinical implementation of these models will need to be further defined since post-transplant risk would need to be balanced against the ongoing risk associated with waiting for transplant, potentially also predicted with ML, to ensure optimal patient outcomes. For instance, some low-risk recipients may be best served by accepting higher-risk donors if the predicted 1-year mortality was sufficiently low compared to the risk of not proceeding with transplant. Once these aspects of clinical implementation are clarified, dedicated studies will be warranted to determine the net clinical impact of integrating ML risk predictions into decisions regarding donor selection.

Limitations
Our study has a few important limitations. We relied on the UNOS database and some data was missing or may not be coded correctly. However, this would tend to make our results more generalizable. We applied a few different methods for ML risk prediction but did not perform a comprehensive analysis of all potential approaches. Additionally, we compared our methods to regularized logistic regression which has not been traditionally used for post-transplant risk modeling but is better suited to handling a large set of variables with suspect collinearity. We also did not introduce interaction terms in the linear models, as we were evaluating these methods in the “machine learning” setting, without manual fine-tuning for specific interactions. Models incorporating these terms may have higher prediction performance but would most likely still be influenced by temporal shift. There are many factors which could improve ML prediction performance but are not available in the UNOS database. Lastly, we have not performed an analysis of the potential implications on donor selection or downstream impacts on donor utilization and wait-list times. Future prospective studies are warranted to better assess the impact of ML-based donor-recipient risk prediction on both important metrics.",https://www-sciencedirect-com.myaccess.library.utoronto.ca/science/article/pii/S1053249822018824
Yu et al. 2022,Artificial intelligence for predicting survival following deceased donor liver transplantation: Retrospective multi-center study,"Background
Previous studies have indicated that the model for end-stage liver disease (MELD) score may fail to predict post-transplantation patient survival. Similarly, other scores (donor MELD score, balance of risk score) that have been developed to predict transplant outcomes have not gained widespread use. These scores are typically derived using linear statistical models. This study aimed to compare the performance of traditional statistical models with machine learning approaches for predicting survival following liver transplantation.

Materials and methods
Data were obtained from 785 deceased donor liver transplant recipients enrolled in the Korean Organ Transplant Registry (2014–2019). Five machine learning methods (random forest, artificial neural networks, decision tree, naïve Bayes, and support vector machine) and four traditional statistical models (Cox regression, MELD score, donor MELD score and balance of risk score) were compared to predict survival.

Results
Among the machine learning methods, the random forest yielded the highest area under the receiver operating characteristic curve (AUC-ROC) values (1-month = 0.80; 3-month = 0.85; and 12-month = 0.81) for predicting survival. The AUC-ROC values of the Cox regression analysis were 0.75, 0.86, and 0.77 for 1-month, 3-month, and 12-month post-transplant survival, respectively. However, the AUC-ROC values of the MELD, donor MELD, and balance of risk scores were all below 0.70. Based on the variable importance of the random forest analysis in this study, the major predictors associated with survival were cold ischemia time, donor ICU stay, recipient weight, recipient BMI, recipient age, recipient INR, and recipient albumin level. As with the Cox regression analysis, donor ICU stay, donor bilirubin level, BAR score, and recipient albumin levels were also important factors associated with post-transplant survival in the RF model. The coefficients of these variables were also statistically significant in the Cox model (p < 0.05). The SHAP ranges for selected predictors for the 12-month survival were (−0.02,0.10) for recipient albumin, (−0.05,0.07) for donor bilirubin and (−0.02,0.25) for recipient height. Surprisingly, although not statistically significant in the Cox model, recipient weight, recipient BMI, recipient age, or recipient INR were important factors in our random forest model for predicting post-transplantation survival.

Conclusion
Machine learning algorithms such as the random forest were superior to conventional Cox regression and previously reported survival scores for predicting 1-month, 3-month, and 12-month survival following liver transplantation. Therefore, artificial intelligence may have significant potential in aiding clinical decision-making during liver transplantation, including matching donors and recipients.","Despite a remarkable increase in deceased donors, organ shortage is the main hurdle for organ transplantation in Korea [21,22]. Therefore, liver allocation is a major issue. In the setting of organ scarcity, both urgency and utility or transplant outcomes should be considered. Moreover, reports issued in several countries have described poor post-transplantation outcomes after the adoption of the MELD system [23]. However, the relationship between MELD scores and survival following transplantation remains controversial [21,22]. Although a Korean national-based retrospective study showed the superiority of the MELD system for predicting short-term mortality and its usefulness for allocation priority determinations, the Korean liver allocation system is not perfect and has room for improvement [21,22]. Although several scores have been developed to predict outcomes after liver transplantation, they have all failed to gain widespread use because they are calculated from statistical regression models such as logistic regression and the Cox model, which assume the linear influence of different variables and require the unrealistic assumptions of the traditional statistical models, that is, ceteris paribus, “all other variables held constant” [14,18,19]. A predictive model is required that enables effective organ allocation and considers complex interactions within the data available before transplantation [14,18,19]. In this context, this study compared the performance of traditional statistical models including the MELD score with machine learning algorithms for predicting survival following liver transplantation using multi-center registry data and explored the possibility of applying machine learning algorithms in liver allocation to assist with donor-recipient matching.

During the past 10 years, significant progress has been made in the use of AI in the medical field because of its potential for diagnostic and prognostic applications [13,14]. Hayward et al. corroborated the findings of other studies involving the application of machine learning methods to breast, prostate, and bladder cancers, demonstrating superiority in terms of prediction accuracy compared to traditional logarithmic regression [15,24,25].

Machine learning has also been used to predict outcomes after transplantation. Lau et al. reported a study that combined donor-, recipient-, and transplant-related variables to predict the 30-day risk of graft failure [19]. As in our study, RF yielded better results than several current scoring systems that use either isolated donor/recipient scores or combined donor/recipient factors. In their study, RF applied to 15 donor and recipient characteristics at transplantation resulted in an excellent AUC (0.818; 95% CI, 0.812–0.824) for predicting the risk of graft failure, further confirming the utility of machine learning tools in donor-recipient matching [19]. In our study, the AUC values for RF for predicting survival (1-month = 0.80; 3-month = 0.85; and 12-month = 0.81) were similar or higher, and the RF model maintained its excellent performance throughout the year. This may have been due to the fact that our model was constructed using a larger multicenter registry dataset than that reported by Lau et al., which was constructed from 180 transplants performed at a single center [19].

In a study by Cruz-Ramirez et al., machine learning was used to improve the equity of donor-recipient matching by developing rule-based systems using ANNs [26]. Using a compilation of clinical data from 1003 LT recipients and donors and data from the retrieval and pretransplant process, machine learning algorithms were used to generate separate algorithms for predicting 3-month graft survival [18,26]. Machine learning algorithms also demonstrated superior results compared to several current scoring systems and more accurately predicted 3-month graft survival (AUC, 0.81), similar to the findings of our study (AUC, 0.85) [26].

A variety of different machine learning algorithms, including ANNs, DTs, SVMs, and Bayesian networks, have been widely applied in disease prognosis and prediction [[15], [16], [17]]. RF analysis is essentially a collection of DTs [15,17]. A DT is built on an entire dataset and uses all the features/variables of interest, whereas an RF randomly selects observations/rows and specific features/variables to build multiple DTs and averages the results [17].

Surprisingly, after the different models were tested 25 times using randomly sampled data in our study, the RF model showed the best performance for predicting survival. The advantages of the RF model include its capacity for analyzing both linear and nonlinear relationships between features and outcomes, its robustness of overfitting by design, and built-in insights into feature importance aiding model explainability [17,19]. Specifically, RF can address predictors that are more important for survival following liver transplantation [17,19]. Based on the variable importance of the RF analysis in this study, the major predictors associated with survival were cold ischemia time, donor ICU stay, recipient weight, recipient BMI, recipient age, recipient INR, and recipient albumin level. As with the Cox regression analysis, donor ICU stay, donor bilirubin level, BAR score, and recipient albumin levels were also important factors associated with post-transplant survival in the RF model. The coefficients of these variables were also statistically significant in the Cox model (p < 0.05), which was largely consistent with the results of previous studies [[3], [4], [5], [6], [7]]. However, we did not expect recipient weight, recipient BMI, recipient age, or recipient INR to be important factors in our post-transplantation survival RF model because they were not statistically significant in the Cox model. Our results suggest that these variables should be considered in future studies of post-transplantation survival models. Because the RF algorithm mostly outperformed the other models including the conventional Cox model, the results of this study demonstrated that a machine learning algorithm developed from the experience of multiple liver transplant units can be a valuable decision-support system for determining which risk factors are more significant for survival following liver transplantation and may be able to predict the likelihood of transplant success, potentially allowing for practice evolution.

Despite substantial optimism surrounds the growing use of AI in healthcare, machine learning also has its limitations [17,19]. Machine learning usually requires large amounts of data, which can be difficult to obtain, although the creation of national shared databases may increase data volume; this is questionable because of the dimensionality, missing data, and control of bias, with minority groups often being underrepresented in such databases [26,27]. In addition, machine learning requires high levels of technical skill and can be difficult to engineer, with experts from medical, computing, and data sciences often speaking different so-called languages and having different perspectives, which can inhibit communication and shared understanding and limit its full potential [17,26]. Therefore, clinicians should expand their multidisciplinary teams to include professionals with computing and data science backgrounds and use algorithms developed in conjunction with clinicians and viewed as aids rather than replacements for traditional clinical decision-making [27].

Although the machine learning algorithm showed excellent predictive performance, there were limitations to this study, including its small sample size, short-term follow-up period, heterogeneous patient group, and retrospective nature. Second, it was beyond the scope of this study to combine deep learning and the Cox model for predicting post-transplantation survival. Deep learning can be defined as “a sub-group of an artificial neural network whose number of hidden layers is larger than five, e.g., ten.” [28] The last three years have seen the emergence of new strands of research that combine the Cox model with different deep-learning counterparts [29,30].

To enhance the performance of machine learning algorithms in predicting post-transplantation survival, we are planning to perform future studies with a larger number of patients and longer follow-up periods and also perform external validation. We are also considering to apply deep learning algorithms. Our study showed that machine-learning algorithms based on donor and recipient variables can be used to predict transplant outcomes. The ability to quantify risk may allow for improved confidence with the use of marginal organs and better outcome after transplantation. However, at present, we believe that the decision for donor recipient matching should not be solely based on the results of algorithms. The continued development and application of these cutting-edge approaches are needed to break new ground and bring more profound clinical insight for predicting post-transplantation survival and its major determinants.",https://www-sciencedirect-com.myaccess.library.utoronto.ca/science/article/pii/S174391912200615X#sec4
Bae et al. 2020,Machine learning to predict transplant outcomes: helpful or hype? A national cohort study,"An increasing number of studies claim machine learning (ML) predicts transplant outcomes more accurately. However, these claims were possibly confounded by other factors, namely, supplying new variables to ML models. To better understand the prospects of ML in transplantation, we compared ML to conventional regression in a “common” analytic task: predicting kidney transplant outcomes using national registry data. We studied 133 431 adult deceased-donor kidney transplant recipients between 2005 and 2017. Transplant centers were randomly divided into 70% training set (190 centers/97 787 recipients) and 30% validation set (82 centers/35 644 recipients). Using the training set, we performed regression and ML procedures [gradient boosting (GB) and random forests (RF)] to predict delayed graft function, one-year acute rejection, death-censored graft failure C, all-cause graft failure, and death. Their performances were compared on the validation set using -statistics. In predicting rejection, regression (C = 0.6010.6110.621) actually outperformed GB (C = 0.5810.5910.601) and RF (C = 0.5690.5790.589). For all other outcomes, the C-statistics were nearly identical across methods (delayed graft function, 0.717–0.723; death-censored graft failure, 0.637–0.642; all-cause graft failure, 0.633–0.635; and death, 0.705–0.708). Given its shortcomings in model interpretability and hypothesis testing, ML is advantageous only when it clearly outperforms conventional regression; in the case of transplant outcomes prediction, ML seems more hype than helpful.","In this comparison of ML algorithms versus regression in predicting KT outcomes using large national registry data, ML did not outperform regression-based models. In terms of discrimination, we observed similar C-statistics across regression and ML algorithms in all transplant outcomes, with the exception of one-year AR where logistic regression actually showed a higher C-statistic than ML algorithms. Furthermore, in terms of calibration as measured in the Brier score, regression outperformed ML algorithms in predicting time-to-event outcomes (DCGF, death, and ACGF), whereas regression and ML algorithms showed similar performance in predicting binary outcomes (DGF and AR).

Predicting KT outcomes using the U.S. national registry data is perhaps one of the “generic” analytic tasks that pertain to a wide gamut of transplantation research, ranging from fine-tuning organ allocation policy [29, 30] to informing clinical decision-making [14, 31] to identifying independent effects by correctly adjusting for confounders [32, 33]. The lack of ML's advantage in this setting implies that, despite the recent successes, and recent claims of successes, surrounding ML in many areas of medicine, regression is a valuable, and sometimes a preferable, analytic method in transplantation research.

Our findings are consistent with a study in heart transplantation by Miller et al. [34] that found no meaningful difference in predicting 1-year survival between logistic regression and ML algorithms using the same set of variables, with C-statistics around 0.65 in most methods. We have extended this approach to kidney transplantation, to outcomes beyond 1 year, to Cox regression which is the typical method for evaluating survival, and to nonsurvival outcomes such as DGF and AR. Our findings are also consistent with studies [3-5, 35] that reported only minor performance differences (e.g., C-statistic from 0.706 to 0.724); we extend these studies in the context of a true head-to-head comparison that shows no performance advantage of ML and, actually, some performance advantage of regression with some outcomes.

On the other hand, our findings are contrary to several recent studies that reported high predictive performance of ML algorithms. In some cases, the exact reason for the discrepancy is unclear due to the absence of a head-to-head, same-variable, same-population comparison against regression [7-10]. But, more importantly, many of these studies purposefully explored ML as a tool to incorporate additional clinical information into prediction [6, 13, 16], rather than testing if ML outperforms regression on equal footing. For example, Lau et al. [16] reported that RF and neural network outperformed traditional models such as Donor Risk Index (DRI) in predicting graft survival after liver transplantation. However, the ML models included numerous key variables that are not included in DRI, such as recipient disease category, donor serum albumin level, and geographical location. Therefore, these studies have shown that ML methods identified potentially more influential clinical factors which led to better prediction. Purely in terms of predictive performance, these studies do not indicate that ML alone can achieve a new level of predictive performance that regression cannot reach, because an equally comprehensive regression-based model could have demonstrated similar performance. In that sense, our current study is a necessary follow-up to the previous ML prediction studies.

Although our findings do not support the application of ML on simple prediction of KT outcomes using routinely collected tabular data, there are research questions in organ transplantation that might be well suited for ML. Theoretically, ML methods are capable of handling interactions between predictors in a flexible manner [14, 36], integrating nontabular data such as clinical notes or graft biopsy images with tabular clinical data [13, 37], and analyzing high-dimensional data such as genes or biomarkers [38]. Our study was not focused on evaluating these benefits, and our findings should not discourage future applications of ML on such research endeavors. However, in the context of straightforward outcome prediction, we emphasize that ML does not seem to provide a predictive advantage, yet suffers from a number of weaknesses that risk misleading modeling, limit our ability to assess face validity or test biological hypotheses, and diminish the interpretability of the models themselves.

Our study has several limitations. First, we cannot rule out the possibility that there exists a ML algorithm that outperforms the algorithms investigated in this study. However, considering that we observed nearly identical predictive performance from all three methods including regression, it is not unreasonable to assume that these performance measures are bound by the inherent variability of the data, not by the competency of the methods. Second, our findings are not generalizable to any analyses that include new types of data not present in the transplant national registry, especially nontabular clinical information. As discussed above, ML might be actually advantageous in these cases. Lastly, there could be specific subgroups in which GB or RF outperforms regression models because of their ability to handle interactions without modeling assumptions. However, such effects were not observed in our analyses.

Our findings suggest that ML does not outperform conventional regression-based approaches in predicting various KT outcomes using routinely collected tabular data. Given that regression modeling presents an interpretable model and enables hypothesis testing, the advantage of using ML over regression in simple predictions of KT outcomes is questionable. The lack of ML's advantage in our “generic,” controlled analytic setting implies that, in this case, ML is more hype than helpful.",https://onlinelibrary-wiley-com.myaccess.library.utoronto.ca/doi/full/10.1111/tri.13695
Zhang et al. 2021,An explainable supervised machine learning predictor of acute kidney injury after adult deceased donor liver transplantation,"Background
Early prediction of acute kidney injury (AKI) after liver transplantation (LT) facilitates timely recognition and intervention. We aimed to build a risk predictor of post-LT AKI via supervised machine learning and visualize the mechanism driving within to assist clinical decision-making.

Methods
Data of 894 cases that underwent liver transplantation from January 2015 to September 2019 were collected, covering demographics, donor characteristics, etiology, peri-operative laboratory results, co-morbidities and medications. The primary outcome was new-onset AKI after LT according to Kidney Disease Improving Global Outcomes guidelines. Predicting performance of five classifiers including logistic regression, support vector machine, random forest, gradient boosting machine (GBM) and adaptive boosting were respectively evaluated by the area under the receiver-operating characteristic curve (AUC), accuracy, F1-score, sensitivity and specificity. Model with the best performance was validated in an independent dataset involving 195 adult LT cases from October 2019 to March 2021. SHapley Additive exPlanations (SHAP) method was applied to evaluate feature importance and explain the predictions made by ML algorithms.

Results
430 AKI cases (55.1%) were diagnosed out of 780 included cases. The GBM model achieved the highest AUC (0.76, CI 0.70 to 0.82), F1-score (0.73, CI 0.66 to 0.79) and sensitivity (0.74, CI 0.66 to 0.8) in the internal validation set, and a comparable AUC (0.75, CI 0.67 to 0.81) in the external validation set. High preoperative indirect bilirubin, low intraoperative urine output, long anesthesia time, low preoperative platelets, and graft steatosis graded NASH CRN 1 and above were revealed by SHAP method the top 5 important variables contributing to the diagnosis of post-LT AKI made by GBM model.

Conclusions
Our GBM-based predictor of post-LT AKI provides a highly interoperable tool across institutions to assist decision-making after LT.","Interpretation
The cause of post-LT AKI is multifaceted. Patients with end-stage liver disease tend to have preoperative intravascular volume depletion and coagulation deficiency that predispose them to greater intraoperative blood loss and low renal perfusion [25]. Besides, the technique of LT involves partial or side cross-clamping of venous flow above the renal vein during anhepatic phase, which contributes to renal congestion and impairs urine output. The 14 predictors incorporated in our model are mainly indicators of preoperative liver dysfunction, intraoperative volume depletion, graft quality and difficulty of the surgery, which were carefully selected by univariate test and subsequent LASSO regression analysis from a series of variables that had been documented as potential risk factors associated with AKI. Moreover, their correlation with AKI were further demonstrated by SHAP summary plot and dependence plot, in which their distribution in relation to the AKI diagnosis were in line with the pathophysiology mentioned above, adding clinical credibility to our model.

We can also tell from these correlations uncovered by ML algorithm that optimization of potentially modifiable variables exerting high importance in predicting AKI, such as intraoperative urine output, preoperative PLT and time under anesthesia, should be given higher priority pre- and intra-operatively. For instance, higher sentinel level of urine output might be considered in patients receiving LT. As has been shown in the SHAP dependence plot, SHAP values distribution tend to be divided around an average urine output of 2.2 ml/(kg·h), which indicates that this might be a potential threshold for physicians to intervene. On the other hand, the criteria in KDIGO guideline requires merely an urine output below 0.5 ml/(kg·h) for at least 6 h to diagnose AKI. Although we did not use this criteria in our research since serum SCr was a more sensitive biomarker to diagnose post-LT AKI in the regimen we adopted, the correlation recognized by ML algorithms illuminate that a higher cut-off point of intraoperative urine output may serve to remind the physicians of renal-protective intervention in advance.

Similarly, our results also indicate that higher PLT transfusion threshold and early extubation shall be preferred in patients receiving LT. Moreover, while graft steatosis of NASH CRN 1 (steatosis involving 5% to 33% of hepatocytes) is accepted in non-urgent LT due to worldwide scarcity of organ donation, it has been identified as a risk predictor of moderate importance by ML algorithms. More strict preliminary graft assessment or lower tolerance in steatosis threshold may be evaluated in the upcoming studies.

Attempts to predict AKI after LT have been made by implementing either novel ML algorithms or conventional statistical technique [5, 6, 9], yet one commonly recognized state-of-the-art prediction system specifically for post-LT AKI setting is currently lacking. Lee, H et al. used a total of 72 pre- and intra-operative variables and also demonstrated that GBM-based model showed best statistical performance to predict post-LT AKI [9]. Nevertheless, the disparities in techniques like use of venovenous bypass and femoral artery pressure make it hard to use our data set to externally validate this model. Yin Z. et al. identified that CIT (> 7 h), donor WIT (> 10 min), blood loss (> 2500 ml), SCr (> 354 μmol/L), treatment period with dopamine (> 6 days) and overexposure to calcineurin inhibitor (CNI) may be potential risk factors of AKI in Chinese liver transplantation cohort [6]. Nevertheless, in our cohort we discovered that the majority of post-LT AKI cases were diagnosed during the first 24 h postoperatively even with delayed Tacrolimus introduction. Meanwhile, a growing proportion of DBD donors without donor WIT has altered the graft characteristics of the cohort. Therefore the power in risk stratification of these factors should be reconsidered and re-analyzed.

Finally we decided to use Kalisvaart’ s AKI prediction score as a benchmark because of our similarity in statistical performance and immunosuppression therapy [5]. As a result, our GBM-based predictor demonstrated higher AUC and F1-score compared to AKI prediction score, either in our original internal validation set or the subset conforming to their criteria that excluded patients requiring preoperative CRRT. We agreed to include patients with preoperative renal injury because these patients have a high possibility of renal recovery after transplantation [20], and are likely to be elevated in the waiting list. Early identification of deterioration in renal function in these patients would be of greater value compared to patients without preoperative renal injury. Considering the preciousness of liver graft and detrimental outcomes associated with AKI, we valued model sensitivity, that is, the ability to find out as much as possible the occurrence of AKI, over model specificity. Comparing to other ML models, boosting algorithms like GBM and ADA achieved generally highest precision and sensitivity, which is consistent with their performance of other studies [26, 27].

Go to:
Limitations
One limitation of the current study is that it is a single center study. Liver transplantation is a highly specialized and complicated technique. Only by joint effort made by multiple centers can we build a larger data set. However, multi-center validation calls for unification in feature availability and standardized perioperative treatment. Nevertheless, we utilized the data of a temporally independent cohort to validate our model. Temporal validation is a type of external validation in which data of new cases, though are from the same institution as in the development sample, come in a different (preferably later) time period. And it is considered to be a kind of arguable but acceptable external validation in the TRIPOD statement (Type2b), an intermediary between internal and external validation [19]. It was worth noting that our development set and the temporal validation set demonstrated a bit of heterogeneity in several predictors, such as steatosis grade of donor liver, time under general anesthesia, estimated blood loss, use of colloid, bicarbonate and cryoprecipitate. These changes mainly arose from the improvement of surgical techniques and aggravated scarcity of non-steatotic donors. The incidence of AKI tended to be lower but the drop was not significant. We believe that these significant differences to some extent reflect the effectiveness of our temporal external validation result, as well as the robustness of our model. On the other hand, as for geographical external validation, the features utilized in our model are all regularly recorded or tested in OLT cases in most transplant centers, and multicenter cooperation can be achieved once authorization of data usage is approved.

Another possible limitation is that the statistical metrics of our model might not be as high as those presented in similar researches [9, 28]. However, many of these studies built their ML models upon high dimensional features, running the risk of over-fitting. After careful feature elimination, we built our predicting model with merely 14 features, aiming for practical external validation in the future. In this way it was worthy trading statistical accuracy for model applicability. Moreover, the path of decision made by our model in each individual can be illustrated as SHAP decision plot, offering richer information in feature importance or even in potential drawbacks of the model. With such visualized explanation, physicians can interpret the model output easily and timely adjust their decisions.

Implications
Our research is a solid and generalizable work to build an applicable predictor of post-LT AKI with supervised ML, which covers the prediction of AKI in patients requiring preoperative renal replacement therapy. The GBM-based model we developed consists of variables with high clinical credibility that are interoperable across institutions, and demonstrates satisfactory statistical validity and reasonable relational interpretability revealed by SHAP method.

As an emerging tool of explanatory AI, SHAP method can facilitate both local and global interpretations [12, 29]. For local interpretation, each case has its own set of SHAP values. So it can explain how each feature contributes to the prediction of a certain case, as has been illustrated in our SHAP decision plot and force plot, which increases transparency and helps clinicians analyze the credibility of the prediction model. For global interpretability, the aggregate value of SHAP shows the importance of each predicting variable. Compared with traditional methods to evaluate feature importance such as the weight of RF, the SHAP value holds better consistency and can present the positive or negative relationship of each predictor.

The potential application of this model lies in its integration with the EMRs system to guide early diagnosis and interventions after LT. Since the features we selected are all easily accessible right at the end of the surgery, this GBM-based predictor of post-transplant AKI would be a convenient predicting tool that can maintain transparency of the decision-making process to clinical physicians, enabling them to adjust the final decision according to their own experience.",https://www-ncbi-nlm-nih-gov.myaccess.library.utoronto.ca/pmc/articles/PMC8317304/
Wheless et al. 2020,Development of Phenotyping Algorithms for the Identification of Organ Transplant Recipients: Cohort Study,"Background
Studies involving organ transplant recipients (OTRs) are often limited to the variables collected in the national Scientific Registry of Transplant Recipients database. Electronic health records contain additional variables that can augment this data source if OTRs can be identified accurately.

Objective
The aim of this study was to develop phenotyping algorithms to identify OTRs from electronic health records.

Methods
We used Vanderbilt’s deidentified version of its electronic health record database, which contains nearly 3 million subjects, to develop algorithms to identify OTRs. We identified all 19,817 individuals with at least one International Classification of Diseases (ICD) or Current Procedural Terminology (CPT) code for organ transplantation. We performed a chart review on 1350 randomly selected individuals to determine the transplant status. We constructed machine learning models to calculate positive predictive values and sensitivity for combinations of codes by using classification and regression trees, random forest, and extreme gradient boosting algorithms.

Results
Of the 1350 reviewed patient charts, 827 were organ transplant recipients while 511 had no record of a transplant, and 12 were equivocal. Most patients with only 1 or 2 transplant codes did not have a transplant. The most common reasons for being labeled a nontransplant patient were the lack of data (229/511, 44.8%) or the patient being evaluated for an organ transplant (174/511, 34.1%). All 3 machine learning algorithms identified OTRs with overall >90% positive predictive value and >88% sensitivity.

Conclusions
Electronic health records linked to biobanks are increasingly used to conduct large-scale studies but have not been well-utilized in organ transplantation research. We present rigorously evaluated methods for phenotyping OTRs from electronic health records that will enable the use of the full spectrum of clinical data in transplant research. Using several different machine learning algorithms, we were able to identify transplant cases with high accuracy by using only ICD and CPT codes.","In this study, we developed and validated phenotyping algorithms for identifying OTRs from the EHR. Using several different rule-based and machine learning methods, we were able to identify OTRs overall with 90% PPV and sensitivity and higher values for several individual organ types. The algorithms all performed comparably well, although RF tended to be the most consistent. The development of these phenotyping algorithms was necessary as the PPV for using at least one transplant code to identify OTRs was only 60%, indicating that studies based on the presence of only one of these codes may have biased results.

The SRTR of the United Network for Organ Sharing and the Organ Procurement and Transplant Network is the primary national database for transplant recipient outcomes research. Because the SRTR is not linked directly to patient records in EHRs, it is not possible to collect data on additional variables not captured by the data entry forms. As a result, many important variables and outcomes are completely omitted. Indeed, a recent study of cardiac transplants using SRTR data found that advanced machine learning methods did not outperform the more traditional prediction models for 1-year survival, with the authors concluding that the methods were hindered by limited data in the registry [21]. By developing validated algorithms to identify OTRs from the EHR, a broader range of studies can be conducted using the data in the full clinical record.

Large reviews of the accuracy of diagnostic and procedural codes show <90% concordance with true diagnoses in inpatient and outpatient settings, both in the United States and other countries [9,22,23]. In a study from Canada, the use of ICD codes alone to identify kidney donors had only 60% sensitivity and 78% PPV, which were similar to our findings for transplant recipients [9]. While the primary diagnosis for a visit is less likely to be missed, secondary diagnoses were more likely to be omitted from the coding. In the United States, up to 12 diagnoses can be entered for an encounter, though only 4 are allowed to be linked to an individual service, with the codes generating the highest reimbursements being prioritized by the medical coders. As a result, transplant patients seen for critical illnesses or procedures may have been less likely to have a transplant code listed.

Many of the charts we reviewed contained only 1 or 2 transplant codes. In addition, these charts often had only ICD and CPT codes but no documents, medications, or laboratory data. Two possible explanations for this lack of data are that handwritten notes and outside records are not scanned into the Synthetic Derivative, and patients with sparse data that could make them potentially identifiable are redacted more often than those with deeper coverage of their records. Regardless of the reason for lack of data, these patients were all called nontransplant patients, and therefore, our algorithm might underestimate the PPV for those with few codes. We attempted to improve our accuracy in classifying these individuals with few transplant codes. First, after identifying this problem in our preliminary analyses, we increased our initial sample by 67% with oversampling of those with only 1 or 2 codes to provide the models with more data points with which to learn to classify them. We also considered adding medications to our algorithms as well as applying NLP to the documents in the EHR. Although these strategies might have augmented the PPV and sensitivity, the gains would have been minimal as those individuals with data besides ICD and CPT codes tended to have a higher number of transplant codes, and therefore, the algorithms had more accurate classification of these patients without the extra data. Moreover, classifying individuals with sparse data as non-OTRs eliminates even those true OTRs who would be excluded from later analyses due to missing data. The true transplant cases that were misclassified were almost exclusively those who had only a single presentation to VUMC with no additional follow-up. Thus, they tended to have only 1 or 2 diagnostic or procedural codes. From a broader standpoint, these were patients who also had little data to contribute to any downstream analyses of the cohort. Therefore, while the models excluded some cases, the overall information loss was low.

There was notable variation in the model metrics both within and between organ types. The reason for the different performance was likely 2-fold. First, there were low numbers for lung transplant recipients (n=81) compared to kidney transplant recipients (n=259); therefore, it is not surprising that the kidney models performed better. Second, the number of different codes contributing to a specific organ type also played a role. For example, although there were 249 stem cell or bone marrow transplant patients, there were 50 different ICD and CPT codes for this type of transplant. Therefore, it is not surprising that the bone marrow models tended to perform worse than the other organ types that had far fewer codes associated, as there were likely subsets within the cross-validations that did not include certain codes. Each code is used in different clinical settings and can be subject to individual coding preferences; therefore, this variability would be expected across institutions.

This study had several limitations. All the data were from a single medical center and coding practices may differ among institutions. Any center wishing to use this approach would need to perform a validation step to confirm the models’ performance, although EHR algorithms have been shown to have good portability between populations [24]. VUMC is a high-volume transplant center, and as a result, many patients are seen there for either transplant surgery alone or for follow-up after receiving a transplant elsewhere. This fragmentation of care can limit the available data. Our models consistently predicted slightly greater numbers of OTRs than the number of transplant procedures that have been performed at VUMC. These numbers suggest that we are in fact correctly labeling the majority of those transplants performed at VUMC, while also capturing those whose transplants were performed elsewhere but have been seen in follow-up at VUMC. More than half of the possible OTRs in our EHR had >10 transplant codes, indicating high-density data for these individuals. If we had used >10 transplant codes as our cutoff for OTR determination, the PPV would be 98.5% and the sensitivity would still be 72.3%. Conversely, a large proportion of our cohort had low numbers of transplant codes, which can correlate with the duration of the follow-up. Although the cases identified with low numbers of codes could have easily been excluded a priori by requiring a set number of total codes, doing so would falsely inflate our sensitivity measures, as many true cases would not have been investigated and confirmed on chart review. Our goal was to provide accurate estimates of the algorithm’s overall performance, even if many of the identified cases would ultimately be excluded due to missing data in subsequent analyses. Many patients had no available text data from notes. This deficiency likely was the outcome of handwritten notes not being included in the Synthetic Derivative. Thus, we were not able to add NLP to our algorithms, which potentially could have improved our models. EHRs can be a powerful tool for investigating outcomes not captured by large registries.

In this study, we have validated algorithms for identifying OTR overall and OTRs receiving specific organs by using only ICD and CPT codes. Single variable phenotyping algorithms based on code counts alone perform well but can be improved by using RFs. These algorithms can be used to construct EHR-based cohorts to broaden the range of clinical and translational studies conducted on organ transplants.",https://www-ncbi-nlm-nih-gov.myaccess.library.utoronto.ca/pmc/articles/PMC7759442/
Hsich et al. 2019,Variables of importance in the Scientific Registry of Transplant Recipients database predictive of heart transplant waitlist mortality,"The pre-listing variables essential for creating an accurate heart transplant allocation score based on survival are unknown. To identify these we studied mortality of adults on the active heart transplant waiting list in the Scientific Registry of Transplant Recipients database from January 1, 2004-August 31, 2015. There were 33,069 candidates awaiting heart transplantation: 7,681 UNOS Status 1A, 13,027 Status 1B, and 12,361 Status 2. During a median waitlist follow-up of 4.3 months, 5514 candidates died. Variables of importance for waitlist mortality were identified by machine learning using Random Survival Forests. Strong correlates predicting survival were estimated glomerular filtration rate (eGFR), serum albumin, extracorporeal membrane oxygenation, ventricular assist device, mechanical ventilation, peak oxygen capacity, hemodynamics, inotrope support, and type of heart disease with less predictive variables including antiarrhythmic agents, history of stroke, vascular disease, prior malignancy, and prior tobacco use. Complex interactions were identified such as an additive risk in mortality based on renal function and serum albumin, and sex-differences in mortality when eGFR >40 mL/min/1.73m. Most predictive variables for waitlist mortality are in the current tiered allocation system except for eGFR and serum albumin which have an additive risk and complex interactions.","The THEMIS Investigators (Transplantation of HEarts to MaxImize Survival) were recently awarded NIH support to reduce heart transplant waitlist mortality and minimize organ wastage by identifying risk factors for disparities in survival. Utilizing the SRTR national database we found many strong and some weak correlates predictive of waitlist mortality. Among the most predictive variables were eGFR, serum albumin, ECMO, VADs, mechanical ventilation, peak oxygen capacity, year, hemodynamic parameters and inotrope support. Weaker correlates for predicting waitlist mortality included antiarrhythmic agents, history of stroke, vascular disease, prior malignancy, history of tobacco, blood type and race. Our analysis supports usage of the 3 tiered and recently accepted 6 tiered heart allocation system, which prioritizes allocation to those most at risk for waitlist mortality. Our study also emphasizes the fact that other variables like renal function and serum albumin are highly predictive of waitlist mortality and not included in the heart transplant allocation system. Finally, with Random Survival Forest this analysis rapidly identified variables of importance and can be utilized for assessment of complex interactions.

The current and recently approved tiered allocation system defines medical urgency for heart transplantation based on devices, inotropes, mechanical ventilation, functional capacity, and heart disease. All of these variables were highly predictive of waitlist mortality in our study. In fact need for ECMO support currently places candidates into the highest tier for transplantation and was more predictive of mortality in our analysis compared to other devices. Inotropes and intra-aortic balloon pumps were less predictive of mortality than VADs and ECMO but still were important. It is possible that candidates in these subgroups had fewer deaths than those having VADs because they were transplanted faster (reduced outcomes) or bridged with a device after a period of time (delayed outcome). Year of waitlist was very predictive of mortality for UNOS Status 1A and 1B but not Status 2 candidates most likely because of the FDA approval over time of smaller and better mechanical circulatory support devices.11 Most interesting was the discovery that eGFR and serum albumin were more predictive of waitlist mortality than most devices but are not currently among the criteria for heart transplantation.

Renal function is known to be an important predictor of mortality among patients with heart failure.20–24 In a large meta-analysis that included 43 heart failure survival models predicting mortality, renal function was one of the most predictive variables.20 eGFR using the MDRD equation was also shown to predict post-heart transplant mortality.21 Despite these findings, the major issue with using renal function for heart transplant allocation in the future is that it is a dynamic variable affected by diuretic usage and hydration. Nevertheless, dynamic changes in renal function substantially alters instantaneous estimated mortality on the waitlist.25

Serum albumin has been shown in several studies to be an independent predictor of mortality.24,26–30 In a single center study involving 438 patients admitted for acute decompensated heart failure, serum albumin <3.4 g/dL was one of the strongest predictors of 1 year mortality (aHR=2.05, 95% CI 1.10–3.81, P=0.001). 26 Hypoalbuminemia was also predictive of length of post-operative stay and acute renal failure among LVAD recipients.24,27,28 Serum albumin is a reflection of nutritional state, hepatic synthetic function, catabolic state, inflammation, and protein losing conditions. The significance of these factors needs further evaluation, but it is interesting to note that a few studies have shown that the predictive power of hypoalbuminemia is independent of the presence of cachexia or malnutrition.26,27

Several variables in our study were deemed less predictive of waitlist mortality. Should they be eliminated from the OPTN/UNOS database to make room for more essential variables? Not based on this analysis alone. The decision to retain or eliminate a variable depends on its importance for ensuring a proper donor/recipient match, predictive value for post-transplant outcome, necessity to ensure fair distribution of organs, and any significant interactions with other variable that affect outcome. For instance, blood type and body mass index were not important predictors of waitlist mortality in our analysis but are essential for matching donor organs and recipient. History of tobacco use was also not highly predictive of waitlist mortality but has been shown to predict coronary allograft vasculopathy, graft dysfunction, and mortality post-heart transplantation.31,32 Race was among the weak correlates predicting waitlist mortality in our study but is important for tracking fair distribution of organs and outcome for specific races.33 Finally, the importance of a variable also depends on its ability to modify the effect of another variable. For instance, sex was modestly predictive of waitlist mortality, but more substantially affected survival when associated with renal function among candidates with eGFR > 40 mL/min/1.73m2. Sex has also been shown to interact with many variables of importance affecting waitlist survival including serum albumin, hemodynamics, VAD, and peak oxygen consumption.11 Among candidates bridged to transplant with VADs, factors affecting post-transplant survival included sex, mechanical ventilation, history of hemodialysis, history of coronary artery bypass surgery, serum creatinine, and serum bilirubin obtained around the time of transplantation.34 In fact, death is often the result of many factors, making it imperative that we use statistical methods like RSF to handle complex interactions and to create allocation scores.

This analysis has several limitations. The SRTR database is a large national database that is limited to the data elements prospectively entered and subject to human error during data entry. Although each transplant center routinely collects a lot of information and rigorously evaluates potential candidates, only selected data at specific time points are entered into the national database. These data elements do not include natriuretic peptides, serum sodium, heart rate, and blood pressure that are known to have prognostic significance.2,3,35,36 It also does not include serum bilirubin at time of listing although this variable is collected at time of transplant. In addition, values for known prognostic risk factors like peak oxygen consumption were often not entered into the database. Despite concerns, human error is minimized in SRTR by edit checks, validation of data at time of entry, and internal verification when there are outliers. Potential problems are reviewed by data quality specialist who resolve discrepant data by verifying the information with the involved transplant center. Data quality is further improved by UNOS and CMS audit checks done routinely every few years at every transplant center.6 Missing data is also less of a concern when using RSF. RSF performs excellently even with heavy missingness and when missing data are not missing completely at random.14 RSF also can identify complex interactions but cannot determine how much the integrative interaction effect improves prediction performance. However, this limitation does not preclude developing a coarse tiered allocation score derived from the random survival forest prognostic model8,9 nor does it prevent using RSF to quickly derive a survival curve for an individual patient.10 Finally, it is important to mention data quality. As an example, due to lack of standardization, until recently we were not able to utilize panel of reactive antibody in this analysis.

In conclusion, we found many strong and weak correlates predicting heart transplant waitlist mortality in a large national registry with RSF machine learning statistical methods. Most variables highly predictive of waitlist mortality are incorporated into the current and/or new tiered heart allocation system except for eGFR and serum albumin. To create an allocation score for the future, variables not available in the OPTN/UNOS database will need to be compared with existing variables to determine best predictors of waitlist mortality and the factors affecting post-transplant survival. Allocation may also need to vary with demographics, type of heart disease, type of devices, and other factors because complex interactions predicting survival exist. Machine learning technology like RSF can be used to develop a data-driven allocation system similar to what was created for staging of esophageal cancer.8 RSF is a robust, non-parametric algorithmic statistical method that can identify risk factors without prior knowledge of any possible parametric relationship (linear or nonlinear) to mortalityand can handle complex interactions and large amounts of missingness unlike conventional statistical methods like Cox proportional hazards models. Although RSF is not an additive procedure, variables derived from RSF can be used to create an additive allocation score to better predict survival and is worthy of future research.","https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6591021/#:~:text=Strong%20correlates%20predicting%20survival%20were,with%20less%20predictive%20variables%20including"
Davis et al. 2015,Improving geographic equity in kidney transplantation using alternative kidney sharing and optimization modeling,"The national demand for kidney transplantation far outweighs the supply of kidney organs. Currently, a patient's ability to receive a kidney transplant varies depending on where he or she seeks transplantation. This reality is in direct conflict with a federal mandate from the Department of Health and Human Services. We analyze current kidney allocation and develop an alternative kidney sharing strategy using a multiperiod linear optimization model, KSHARE. KSHARE aims to improve geographic equity in kidney transplantation while also respecting transplant system constraints and priorities. KSHARE is tested against actual 2000-2009 kidney allocation using Organ Procurement and Transplant Network data. Geographic equity is represented by minimizing the range in kidney transplant rates around local areas of the country. In 2009, less than 25% of standard criteria donor kidneys were allocated beyond the local area of procurement, and Donor Service Area kidney transplantation rates varied from 3.0% to 30.0%, for an overall range of 27.0%. Given optimal sharing of kidneys within 600 miles of procurement for 2000-2009, kidney transplant rates vary from 5.0% to 12.5% around the country for an overall kidney transplant range of 7.5%. Nationally sharing kidneys optimally between local areas only further decreases the transplant rate range by 1.7%. Enhancing the practice of sharing kidneys by the KSHARE model may increase geographic equity in kidney transplantation.","The preliminary results show that directed kidney sharing between DSAs can lessen the range in DSA kidney transplant rates from 26.9% to 7.5% over a 10-year period. This result is achieved by only sharing between DSAs within 600 miles of each other rather than requiring national kidney sharing, which only further reduces the range in kidney transplant rates to 5.8% by 2009. To validate the results in this article, we performed a simulation study using the (KSIM) model described in Davis and others.45 KSIM is a DSA-level discrete-event simulation of the US transplantation system. It models additions and removals from the kidney transplant list at each DSA and also the procurement of organs in each DSA. Using retrospective data, it simulates the acceptance and subsequent transplantation of each procured organ in accordance with the UNOS hierarchy (i.e., local, regional, and national allocation). One hundred replications of the KSIM model were performed. The rightmost columns of Table 2 summarize the disparity in the actual system and the mean disparity of the KSIM model taken over 100 replications. A comparison of the results in Table 2 shows that the disparity observed from the deterministic optimization model, as well as that observed from the simulated system using the parameters of the deterministic optimization model, is similar at the end of the 10-year period. Using the parameters recommended by the optimization model, we also calculated in-state (when multiple DSA are present within a state) and within-region disparity in transplant rates. The transplant rate disparity was calculated by taking the ratio of the maximum transplant rate to the minimum transplant rate at a DSA within a state (region). The results of the actual system and those obtained from KSHARE are given in Tables 3 and 4, respectively. These results show that the KSHARE disparity in all states except Florida is better than the actual and that it has improved consistently. As mentioned previously, Florida and Tennessee both received a statewide sharing variance in 1991 and 1992, respectively. Consequently, the DSAs within Florida and Tennessee were already sharing kidneys during 1991–1999 within the state prior to regional allocation. Note that the total number of transplants in the new system does not change. A reason for improvements in kidney transplant rate equity is that KSHARE directs more kidneys to the DSAs with lower transplant rates, instead of allowing any other DSA within the region or the nation to have access to the kidney if not used locally. Consequently, it provides a better balance between the supply and the demand at each DSA. Since the model only allows the sharing of a limited fraction of the kidneys that would have been shared since no matched recipient was found locally, the improvement in equity takes place gradually over time. Nevertheless, the results suggest that improvement in inequity can be achieved without requiring a major realignment of regions or placing increased sharing burden on any one area of the country. In creating stable DSA sharing partnerships between nearby DSAs, we may also increase the efficiency of kidney organ placements over time and thereby improve kidney transplant recipient outcomes due to faster kidney placement.15 One may suspect that KSHARE approach will cluster the DSAs. The KSHARE framework will not formally cluster DSAs into sets, because in an actual practical implementation, the policy derived from the KSHARE model can be adapted over time after the equity issue has been resolved. More advanced policies based on advanced randomized versions of KSHARE are possible. For example, only a fraction of organs shared with a DSA will be shared by generating a “random number,” while the remaining will be used as usual. We also performed a sensitivity analysis for different values of (10–2 to 10–10) and found no changes in the achieved disparity levels.",https://journals.sagepub.com/doi/10.1177/0272989X14557696?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub%20%200pubmed
Liu et al. 2020,Predicting short-term survival after liver transplantation using machine learning,"Liver transplantation is one of the most effective treatments for end-stage liver disease, but the demand for livers is much higher than the available donor livers. Model for End-stage Liver Disease (MELD) score is a commonly used approach to prioritize patients, but previous studies have indicated that MELD score may fail to predict well for the postoperative patients. This work proposes to use data-driven approach to devise a predictive model to predict postoperative survival within 30 days based on patient’s preoperative physiological measurement values. We use random forest (RF) to select important features, including clinically used features and new features discovered from physiological measurement values. Moreover, we propose a new imputation method to deal with the problem of missing values and the results show that it outperforms the other alternatives. In the predictive model, we use patients’ blood test data within 1–9 days before surgery to construct the model to predict postoperative patients’ survival. The experimental results on a real data set indicate that RF outperforms the other alternatives. The experimental results on the temporal validation set show that our proposed model achieves area under the curve (AUC) of 0.771 and specificity of 0.815, showing superior discrimination power in predicting postoperative survival.","In this study, we apply feature selection technique to select important features from the physiological measurement items. The top features presented in Fig. 1 are obtained from RF as well as step-wise selection, and the selected features include INR, Lymphocyte, Platelets, WBC, Mg, Na, age and BMI. Moreover, missing data in medical research is a common problem, and we propose an imputation method based on the characteristics of the features to deal with this problem. Next, detailed discussion regarding the findings is presented below. Missing data in medical research is a common problem, and we propose an imputation method based on the characteristics of the features to deal with this problem, in which we use a conservative strategy to replace the missing values. We compare our proposed imputation method with the other alternatives, including imputation with mean, maximum, median and minimum, respectively. The results are presented in Table 3, indicating that the proposed method achieves the best predictive performance. The proposed method considers the characteristics of features, and we believe that is the main reason why the proposed method outperforms the other alternatives. This work uses the data 10 days before the surgery as the data source to construct the first model. Subsequently, we conduct experiments to find out which period of data before surgery has the most impact on the prediction results. The experiments use RF as the machine learning algorithm, and different ranges of data as the data sources to train different predictive models. The experimental results are presented in Fig. 2. The results show that the data from day 1 to day 9 before surgery is more important than the other ranges. This result conforms to the intuition as day 10 is the day farthest from the surgery in the range. MELD score is a formula involving bilirubin, INR and creatinine. Thus, MELD score could be considered as a combination of the three features. To compare the performance impacts brought by the features, we use RF with MELD score and several features to learn a predictive model. The features used by the comparison model includes MELD score, hepatitis, HCC, DX1, age, gender, and BMI. The results are presented in the top of Table 4, indicating that our proposed model outperforms the alternative model for predicting postoperative survival. Besides, we apply cox proportional hazards model with the two combinations of features to perform survival analysis within 30 days and the results are listed in the bottom of Table 4. Both of them are statistically significant, but the features selected by RF achieves higher Concordance Index (0.85) than those used by MELD score. Moreover, the same experiments are applied to temporal validation set to investigate the generalization capability of our proposed model, and the results are presented in Table 5. The experimental results indicate that the features selected by RF could provide more discriminative capability than the features used by MELD score in predicting survival outcome after liver transplantation. Besides the above analysis, the hazard ratios (HR) from cox proportional hazards model are presented in Fig. 3, which only shows the basic features and the blood test data of day 9 owing to the limit of paper length. Significant features comprise INR, Platelets and age, which conform to the bedside experience of the domain expert. The medical data used in this study is imbalanced as presented in most medical studies. We propose to use RF29 to construct the predictive model, which provides not only accurate performance, but also the capability of dealing with imbalanced data. This is because RF uses a technology called bagging that can reduce or mitigate bias for imbalance data33,34,35. Bagging approach uses bootstrap sampling technique to sample enormous sub-samples with replacement from the initial data set, each of the sub-samples is used to train a predictive model. In RF, each model is a classification and regression tree (CART)32, which is a decision tree algorithm. The final model is obtained by averaging all these models, and majority vote rule is a typical approach in determining the final results. The bagging approach provides a way to eliminate bias caused by unstable models. The experimental results indicate that RF works well on imbalanced data used in this study.

In conclusion, the analysis of experimental results presents two findings. First, among these important features, most of the features are blood test items and have been clinically proven that those features have a certain impact on survival outcome after liver transplantation except Mg. Our experimental results show that Mg is also an important feature which has impact on survival outcome after liver transplantation. Second, the experimental results show that RF is robust on imbalanced data. Most medical data sets are characterized by imbalanced property, and many medical applications are interested in the risk factors that lead to the results. Thus, RF is a very good machine learning model in medical domain.

Although previous research has indicated that patients who have undergone orthotopic liver transplantation may be a group especially predisposed to hypomagnesaemia36, the domain experts pointed out that Mg has not been used clinically to predict survival outcome after liver transplantation. However, blood magnesium ion concentration indeed is a very important electrolyte. If the patient has malabsorption or used diuretics, he/she would be considered as being in a high-risk group for hypomagnesemia. When the blood magnesium ion concentration is too low, it would directly affect the recovery of many other electrolytes. Moreover, the previous research has indicated that Mg has a direct relationship with heart function37, which associates Mg with mortality38,39, such as the association between hypomagnesemia and fatal cardiac arrhythmia40. Thus, another benefit of using data-driven approach to devise a predictive model is that one may discover the factors that are not directly related to the organs we are focusing on.

In summary, this work considers the characteristics of features to propose an imputation method to deal with missing values, and the results point out that the proposed method works well. Central to this study is using machine learning to predict short-term survival which can detect the high risk patients in early phase after liver transplantation, and discover important factors that are essential in liver transplantation, in which we argue machine learning could help the physicians make decisions. Once the higher risk patients are identified by the model, several treatment options could be given to these patients. For example, the immunosuprpesion drug should be admitted earlier or in relative high concentration, to avoiding the trigger of acute rejection and causing the vulnerable complications, such as acute kidney injection, and secondary bacterial infection.",https://www.scopus.com/inward/record.url?eid=2-s2.0-85082534937&partnerID=10&rel=R3.0.0
Amini et al. 2022,An explanatory analytics model for identifying factors indicative of long-versus short-term survival after lung transplantation,"Due to the shortage of available organs compared to the number of patients on waitlists, the organ allocation process has always been challenging and calls for an equitable and optimized allocation system. This system demands minimizing the waitlist mortality and improving transplantation benefits (e.g., survival time and quality of life). According to prior research, lung recipients’ long-term survival time is lower than other solid organs recipients. This paper proposes and implements an explanatory analytics framework to study the most prominent factors contributing to long-term survival after lung transplantation. We collect data from the United Network for Organ Sharing registry database. It contains more than 44 thousand unique patients undergoing lung transplantation in the US since 1987. In our proposed framework, we first employ several machine learning (ML) algorithms for classification and choose the best-performing one for further analysis of factors. Then, we utilize a state-of-the-art explainable artificial intelligence method with the chosen ML algorithm for the model explanation and interpretation. The proposed framework lists the most critical factors influencing long-term survival after lung transplantation and their corresponding importance measures. For instance, our results suggest that Hepatitis B surface antibody (HBV_SURF) and forced expiratory volume in one second (FEV1) are among the important factors but have not been well examined by lung transplant researchers. Our framework is also able to provide the main contributing factors for each patient individually. Medical scholars and practitioners can use our findings for further analysis and improvement of the lung allocation system.
","In this paper, we studied factors indicative of patients’ long- versus short-term survival after LTx by proposing an explanatory analytics framework. We collected a comprehensive dataset of LTx operations from the United Network for Organ Sharing (UNOS) registry database, including recipient, donor, and LTx procedure characteristics. Based on the survival time of lung recipients, we defined two categories of patients representing short- and long-term survival after LTx. After going through a comprehensive data cleaning and preprocessing in the proposed framework, we trained and tested various ML algorithms to find the best predictive one for our binary classification problem and preprocessed data in hand. We found RF as the one with the best predictive performance and chose it for further analysis. In the next step, to interpret and explain the predictions, we employed a novel explainable AI method accompanied by RF to understand the most critical factors affecting the long-term survival after LTx. As a result, our proposed framework provided a list of the top 20 important factors and their corresponding importance measures. Policymakers can use our findings in improving the lung allocation system. In addition, our framework can predict and explain the individual long-term survival of LTx candidates and can be utilized as a recommendation system for organ matching and allocation.

One limitation of this study is that the data is limited to only US LTx patients. Although we believe that our data set is large enough to obtain robust results, our framework and results could be validated by scholars who have access to larger organ transplant registries’ data like ISHLT.

We also did not have access to the patients’ medical history and underlying condition before the transplant. This variable and others such as gender and race could be controlled for in future studies. Moreover, each of the top variables in Table 2 calls for a more focused investigation through statistical and causal models.

Finally, from a broader perspective, LTx has brought patients hope for prolonged survival and enhanced health-related quality of life (HRQoL). Although these hopes come true for many patients, some die earlier than they would have without LTx and a considerable proportion of lung recipients face comorbidities and adverse health events. Hence, survival analysis alone cannot provide an adequate measure of the level of success or failure of LTx, and studying the HRQoL of lung recipients is an essential task. Previous studies on HRQoL after LTx often relied on survey data from living and well enough lung recipients. The major limitation of these studies is that the data are not missing at random [51]. Thus, one interesting direction of research is assessing the contributing factors to HRQoL after LTx using transplant registries’ follow-up data and ML/AL methods.",https://www.sciencedirect.com/science/article/pii/S2772662222000194
Brown et al. 2012,Bayesian modeling of pretransplant variables accurately predicts kidney graft survival,"Introduction: Machine learning can enable the development of predictive models that incorporate multiple variables for a systems approach to organ allocation. We explored the principle of Bayesian Belief Network (BBN) to determine whether a predictive model of graft survival can be derived using pretransplant variables. Our hypothesis was that pretransplant donor and recipient variables, when considered together as a network, add incremental value to the classification of graft survival.

Methods: We performed a retrospective analysis of 5,144 randomly selected patients (age ≥18, deceased donor kidney only, first-time recipients) from the United States Renal Data System database between 2000 and 2001. Using this dataset, we developed a machine-learned BBN that functions as a pretransplant organ-matching tool.

Results: A network of 48 clinical variables was constructed and externally validated using an additional 2,204 patients of matching demographic characteristics. This model was able to predict graft failure within the first year or within 3 years (sensitivity 40%; specificity 80%; area under the curve, AUC, 0.63). Recipient BMI, gender, race, and donor age were amongst the pretransplant variables with strongest association to outcome. A 10-fold internal cross-validation showed similar results for 1-year (sensitivity 24%; specificity 80%; AUC 0.59) and 3-year (sensitivity 31%; specificity 80%; AUC 0.60) graft failure.

Conclusion: We found recipient BMI, gender, race, and donor age to be influential predictors of outcome, while wait time and human leukocyte antigen matching were much less associated with outcome. BBN enabled us to examine variables from a large database to develop a robust predictive model.","Currently, kidneys are allocated based on HLA matching and time on list. Nomograms, neural networks, and decision trees have become popular methods for creating more objective ways to predict transplant outcomes [3,5,12,13,14]. This is the first study that uses BBN to predict outcomes in deceased kidney transplantation, and places donor age as one of the most important pretransplant predictors of outcome [15,16,17]. Recipient BMI, gender and race are also influential predictors of outcome in the model, while wait time and HLA matching are much less associated with outcome. This is illustrated in the model graphic as donor age is a primary variable of, or shares an arc with, graft survival greater than 3 years (graft survival over3yr), while recipient gender (RSEX) and BMI (BMI) are secondary variables to a minimum of one year graft survival (graft survival 1yr). The influence of combinations of these factors on 1- and 3-year outcome can be seen explicitly in online supplementary table 1 (for all online suppl. material, see www.karger.com?doi=10.1159/000345552). BBN was able to weigh how each variable in the network, major or minor, influences each other to affect outcome, in contrast to other more traditional nomograms. In essence, BBN takes raw data in the form of individual probability distributions and refines that into a fluid network that accurately predicts renal transplant outcomes.

This model accurately predicts those donor-recipient matches that will have a poor 1-year outcome as illustrated by the sensitivity of 40%. Furthermore, for those donor-recipient pairs that were already a good match, our model did not overly predict them incorrectly as failures, as seen by the high specificity of 80%. In other words, our model would be able to reclaim or reallocate two fifths of the renal allografts that may have been lost in the first year due to a less than ideal recipient selection. Another benefit of the bayesian model is that, while individual variables such as recipient gender may not reliably predict outcome, these same variables populating a network can accurately predict graft outcomes.

While our model’s performance decreases slightly with time from transplant, it maintains a high survival predictive value (>87%) as well as a high specificity (>77%). As we used threshold values that would provide for at least 40% identification of failed grafts, we interpret this as correctly predicting those grafts that would fail while not incorrectly classifying good matches as failures. Although we are not able to capture all of the grafts that failed under the current allocation system, we are able to improve the identification of poor matches by 40% for 1-year and 3-year failures, and potentially warrant reallocation to another qualified recipient. Even though there is a low percent failure rate for cadaveric donor transplants within the first few years, avoiding graft failure in an additional 40% of those that failed within the first year translates into ∼500 additional grafts annually, and thus, a potentially significant reduction in the number of recipients returning to the wait list.

Several efforts have been made to better predict transplant outcomes in order to better allocate organs. An early study, using data from the Collaborative Transplant Study and multivariate analysis, created a computer model to predict 1-year graft survival [18]. Another study using artificial neural network resulted in a model with a positive predictive value for graft survival superior to that of the nomogram (82.1 vs. 43.5%) [3]. However, that study was limited to a single medical center. Yet another study used logistic regression and decision tree modeling to identify predictors of 3-year allograft survival using information from the UNOS database. This information was then used to create predictive models, which had a 76% positive predictive value for graft survival [4]. A similar study was done using tree-based modeling to predict 1, 3, 5, 7, and 10 years’ survival [5]. They showed good performance as measured by area under the ROC curve. However, none of these approaches have yet been widely adopted for organ allocation [19].

The practical effect of using the BBN as a decision making tool in renal allograft allocation may be multifold; less organ waste and reduced cold ischemic times. Currently, nearly 20% of all donor kidneys are discarded, the majority being of marginal quality [20]. However, with this model, the pretransplant characteristics of the particular donor and the proposed recipients can be compared for a prediction of outcome. In addition, as this model can be deployed in XML format, a center could enter the known donor characteristics into a web-based interface and compare the risk of failure based on each prospective recipient’s characteristics.

For example, with a kidney donation from a 44-year-old White male without a history of diabetes and with a BMI of 30, a 39-year-old White female without diabetes and with a BMI of 22 (each of these values, or evidence, is entered directly into the model) is associated with 83.45% probability of greater than 3 years’ graft survival. A 55-year-old Black male with non-insulin-dependent diabetes and a BMI of 29 is associated with 74.67% probability. Finally, a 39-year-old Black female with non-insulin-dependent diabetes and a BMI of 32 is associated with a 72.5% probability of graft survival over 3 years.

There are clear limitations to using an administrative dataset such as the USRDS. We examined a total of 793 pre- and posttransplant variables, which were ultimately narrowed to 52 variables based on clinical expertise, global modeling, and excluding those variables collected during follow-up appointments that did not directly describe outcome. We may have eliminated variables typically thought to have bearing on outcome, such as length of pretransplant dialysis and serum albumin, during the complexity minimization process. Other predictive models and nomograms, neural networks, and decision trees have become popular methods for creating more objective ways to predict transplant outcomes [3,5,12,13,14].

Our model with a sensitivity of approximately 40% and specificity of 80% in predicting transplant outcome for the first 3 years may not be impressive. However, our effort presents a work in which we have shown that the BBN may be superior to existing methods as it has the ability to handle incomplete variables and considers the importance of the combination of variables rather than the contribution of any single variable. Our modeling is one of the many possible means of improving donor-recipient pairing. Future studies will have to compare various neural networks to predict transplant outcomes.

We propose using our model to augment the current allocation system. The current UNOS system should continue to be used to generate the ‘short list’ of candidate recipients matching a particular donor organ. Our model would then be applied as a ‘mathematical equation’ that uses the donor’s and recipients’ information to determine which match would result in the longest-term outcome. Our hypothesis is that the proposed method of matching may have the potential to save more than 40% of grafts that fail within their first year. However, the next step towards implementation is to test the BBN prospectively within a single organ procurement organization before moving to a nationwide evaluation.
",https://karger.com/ajn/article/36/6/561/34469/Bayesian-Modeling-of-Pretransplant-Variables
Banerjee et al. 2003,Predicting mortality in patients with cirrhosis of liver with application of neural network technology,"Background: Prediction of mortality from cirrhosis is important in planning optimal timing of liver transplantation and other interventions. We evaluated the role of the Artificial Neural Network (ANN), which uses non-linear statistics for pattern recognition in predicting one-year liver disease-related mortality using information available during initial clinical evaluation.

Methods: The ANN was constructed using software with data from a training set (n = 46) selected at random from a cohort of adult cirrhotics (n = 92). After training, validation was performed in the remaining patients (n = 46) whose outcome in terms of one-year mortality was unknown to the network. The performance of ANN was compared to those of a logistic regression model (LRM) and Child-Pugh's score (CPS). Death (related to cirrhosis/its complications) within one year of inclusion was the outcome variable. The ANN was also tested in an external validation sample (EVS, n = 62) from another hospital.

Results: Patients in the EVS were younger (mean age, 41 vs 45 years), infrequently of alcoholic etiology (5% vs 49%), had less severe disease (mean CPS 6.6 vs 10.8), and had lower one-year mortality (13 vs 46%). In the internal validation sample, ANN's accuracy was 91%, sensitivity 90% and specificity 92% in prediction of one-year mortality; area under the receiver-operating characteristic (ROC) curve was 0.94. The performance of the LRM (accuracy 74%) and the CPS (accuracy 55%) was significantly worse than ANN (P < 0.05, McNemar's test). Despite differences in the characteristics of the two groups, the ANN performed fairly well in the EVS (accuracy of 90%, area under curve 0.85).

Conclusions: ANN can accurately predict one-year mortality in cirrhosis and is superior to CPS and LRM.","In the present study, ANN was superior to the Child-Pugh scoring system in predicting one-year mortality in patients with cirrhosis. The logistic regression model was also inferior to the ANN in predictive accuracy. The inferior performance of these two instruments may be partially explained by the fact that compared to the ANN these two instruments used a lower number of input variables to make predictions. In contrast, the ANN used 22 variables to make a particular prediction. The ability of the ANN to process more information in the context of multidimensionality of a biological process to a large extent explains the superiority of ANN as a prognostic tool. In addition, ANN has an innate advantage over other regression-based models in that, unlike logistic regression, ANN can absorb and use new information. ANN is a dynamic model and with each new patient the model back-propagates and checks the data with an error-minimization function and re-adjusts hidden weights to improve predictive accuracy (Fig. 1). Thus, as more and more patients’ data are entered into the model, self-training and error correction by back-propagation and in turn predictive accuracy, improves progressively.16 ANN models simulate a clinical assessment; as a clinician tries to gather and assimilate as much information as possible to arrive at a particular conclusion, the ANN model used had input variables which are typically available to the clinician in evaluating a patient with cirrhosis. Once all the variables are entered and the ANN model undergoes training and testing, the built-in ‘intelligent problem solver software’ tries to prune down the number of useful variables to the best combination of input variables that yields the highest predictive accuracy. This is achieved via a process of sequential inclusion and elimination whereby the more predictive variable is retained compared to the input variable that contributes the least to the overall predictive accuracy. In our study, out of the 37 input variables, 22 were retained in the final model. We used all the 37 variables including those which were not direct consequences of liver disease (e.g. cardiac, metabolic and respiratory diseases) as all these could influence mortality in a patient seriously ill due to cirrhosis of the liver. Poor performance of Child-Pugh scoring might be related to the use of a lower number of parameters for prediction of mortality and exclusion of parameters not directly related to liver disease which could also influence mortality of a patient. For example, a patient with cirrhosis of the liver with SBP and septicemia is more likely to die if he had renal failure and uncontrolled diabetes mellitus than if he did not have the latter two associated diseases.

We used Child's scoring system for comparison as this scoring system is most popularly used by clinicians. However, the Child's scoring system uses much less information and the ANN provides more accurate prediction probably because, being computer-based, it can handle more clinical information to arrive at a particular decision.

An important limitation of our study is the short follow-up period of our patients. However, most mortality in our study was in the group with Child's C and at least in the groups used for training and internal validation, one year follow-up might be reasonable. Further, we were interested in one-year mortality as this is most important in the perspective of liver transplantation. Child's score is good for predicting long-term morta-lity, but poor for short-term mortality prediction. Obviously, for liver transplantation, short-term mortality prediction is more important than long-term prediction.

While the current study was in progress, Kamath et al. reported the MELD score as a measure of mortality in patients with cirrhosis.17 Because the majority of our patients did not have INR values, we could not directly compare our ANN-based model with the MELD score. While the MELD scoring system effectively deals with several deficits of the Child-Pugh scoring system, it is a regression-based survival model and suffers from limitations, as pointed out earlier. Unlike the MELD score, which is based mostly on patients with end stage liver disease, the ANN model can be used in both early and advanced liver disease. The MELD score is dependent on laboratory parameters and also the etiology of chronic liver disease, which often requires extensive evaluation. However, from the point-of-view of a clinician evaluating a patient with cirrhosis, it will be very useful to be able to make reasonably accurate assessment of one-year mortality from history, physical examination and simple laboratory tests. The ANN-based model has the potential to be a simple clinical instrument in this context. In fact, when trained and retested after deleting all laboratory-based variables, the ANN was able to predict one-year mortality reasonably well, purely on the basis of physical examination and clinical history.

In another study, neural networks using clinical and laboratory data had a higher prognostic accuracy than that of Maddrey's discriminant function in predicting mortality in patients with severe alcoholic liver disease.18 However, that study did not have an external validation component. For any prediction tool to be accepted into clinical practice, it has to be portable, that is, its performance should not deteriorate when used in a different cohort. In the present study we had the opportunity to test our model in an off-site sample of patients with cirrhosis. It should be noted that our external validation group significantly differed from the training and the internal validation groups. In general, the patients in the external validation group had less advanced cirrhosis with a much lower mortality. This difference is largely related to the inclusion of patients for training and internal validation set from a super-specialty referral institution. In contrast, patients of the off-site validation group were from a large general hospital. ANN performed remarkably well in the internal validation sample. In spite of significant differences in the characteristics of the external validation sample, the ANN performed reasonably well in this group as well. However, positive predictive value was substantially lower (58%) in the external validation sample. The positive predictive value is likely to be a function of the lower mortality rate in the external validation sample and it has implications for the utility of the ANN model. It implies that the present model may not be a good predictor of mortality in a population of patients with well-compensated cirrhosis of the liver with low mortality. Therefore, we believe that the present model should be viewed with caution when the clinician is dealing with patients who are having compensated cirrhosis and therefore, low one-year mortality. But this may not create much of a problem with the clinical use of our model as the question of liver transplantation would be unlikely in such patients.

Although the neural network software allowed us to identify the specific input variables that were the most valuable with respect to accuracy of prediction, these should not be viewed as independent predictor variables as perceived by a clinician. Rather they should be interpreted as part of the global function of the ANN model, reflecting the elaborate, multidimensional and variable nature of interactions between clinical factors. The superiority of ANN-based models rests on their ability to identify complex, non-linear relationships among clinical variables in a global analysis. ANN-based models process data in a non-linear fashion and the network's logic of prediction cannot often be broken down into pieces of clinical reasoning. Thus, concern exists for the so-called, ‘black box approach’, used in neural network-based predictive models; direct cause and effect relationships between input and output variables in these models are often unclear.19 Small sample size is a limitation of our study, particularly considering the large number of potential predictors. Therefore, further studies including a larger number of patients are essential. If the testing population is very different from the training sample, retraining the model may improve its performance. If the testing population is somewhat similar to the training sample, which often would be the case, the ANN model is expected to perform reasonably well even without retraining. This is a major advantage of the ANN-based model over a logistic regression model. However, because during retraining a greater number of patients with known outcomes are fed into the model, performance of ANN-based models would improve to a variable degree until it reaches a plateau, as ANN learns with every example.

Computer-based decision support systems have recently received considerable attention and are increasingly being used clinically for decision-making and improving patient care.20 The commercially available neural network software can be used with any standard desktop computer. Once the neural network model is designed and stored, further data entry and generation of prediction probabilities for individual patients requires reasonably acceptable time and can even be performed by support personnel.

In conclusion, in the present study ANN was able to predict one-year mortality accurately in patients with cirrhosis using clinical information and simple laboratory tests. Prospective clinical studies will be needed to evaluate the use of such models in decision-making and patient management in actual clinical practice.",https://onlinelibrary.wiley.com/doi/full/10.1046/j.1440-1746.2003.03123.x?sid=nlm%3Apubmed
Akl et al. 2008,Prediction of Graft Survival of Living-Donor Kidney Transplantation: Nomograms or Artificial Neural Networks?,"Background. 
An artificial neural networks (ANNs) model was developed to predict 5-year graft survival of living-donor kidney transplants. Predictions from the validated ANNs were compared with Cox regression-based nomogram.

Methods. 
Out of 1900 patients with living-donor kidney transplant; 1581 patients were used for training of the ANNs (training group), the remainder 319 patients were used for its validation (testing group). Many variables were correlated with the graft survival by univariate analysis. Significant ones were used for ANNs construction of a predictive model. The same variables were subjected to a multivariate statistics using Cox regression model; their result was the basis of a nomogram construction. The ANNs predictive model and the nomogram were used to predict the graft survival of the testing group. The predicted probability(s) was compared with the actual survival estimates.

Results. 
The ANNs sensitivity was 88.43% (95% confidence interval [CI] 86.4-90.3), specificity was 73.26% (95% CI 70-76.3), and predictive accuracy was 88% (95% CI 87-90) in the testing group, whereas nomogram sensitivity was 61.84% (95% CI 50-72.8) with 74.9% (95% CI 69-80.2) specificity and predictive accuracy was 72% (95% CI 67-77). The positive predictive value of graft survival was 82.1% and 43.5% for the ANNs and Cox regression-based nomogram, respectively, and the negative predictive value was 82% and 86.3% for the ANNs and Cox regression-based nomogram, respectively. Predictions by both models fitted well with the observed findings.

Conclusions. 
These results suggest that ANNs was more accurate and sensitive than Cox regression-based nomogram in predicting 5-year graft survival.","he importance of having a possibility to predict the outcome after renal transplantation does not need emphasis. This would allow the choice of the best possible kidney donor and the optimum immunosuppressive therapy for a given patient.

Several methods have been used to construct a prognostic model. A multivariate analysis was used to predict the outcome of renal transplantation from deceased donor in an attempt to optimize the allocation of the recovery of organs (7). In another study, multivariate analysis was used to predict creatinine levels in recipients of kidneys from living donors (8). The probability of deceased donor-graft survival was studied using tree regression model (9, 10). ANNs were used to predict the possibility of delayed graft function after deceased donor renal transplantation (11).

To generate an accurate prediction model, several conditions should be fulfilled: use of a robust dataset that represents a large patient population, and the incorporation of prognostically significant variables into the model (12). In addition, the generated model should be validated using an independent testing group (13–16).

ANNs are complex computational systems that can perform a large number of complex tasks. It is claimed that ANNs have many advantages over conventional statistics. They require less statistical training. They can detect and deal with non-linear relationships as well as the possible interactions among the various variables. Furthermore the input variables can be cross-correlated. Nevertheless, ANNs have some disadvantages: its black-box nature, the requirement of more sophisticated computational process, and a tendency for over fitting.

In our study, the accuracy of ANNs was compared with that of a nomogram that we have developed by traditional statistics (5). Although this nomogram was simple to interpret by clinicians, but the linear nature of Cox regression may limit its handling power of complex tasks.

For the development of our ANNs, we have opted to use a feed-forward with back-propagation model since it is known for its stability and tendency not to over fit (17). The ANNs sensitivity was 88.43 (86.4-90.3) %, specificity was 73.26 (70-76.3) %, and its predictions was 16% significantly more accurate than the Cox regression-based nomogram (P<0.001). The Cox regression-based nomogram sensitivity was 61.84% (50-72.8) with 74.9% (69-80.2) specificity and area under ROC curve was 72% (67-77).

The predicted graft survival probabilities of both models responded well to the observed graft survival. The negative predictive value for the ANNs and Cox regression-based nomogram was 82% and 86.3%, respectively. Eighty-two percent of the ANNs predicted graft survival probabilities were responding to observed graft survival, whereas only 43.5% of the Cox regression-based nomogram predicted graft survival probabilities were responding to their observed graft survival.

The ANNs and nomogram calibration plot gave virtually ideal predictions as the predicted graft survival closely paralleled the observed ones and virtually corresponded to the 45° line. Both model predictions fitted well with the observed findings as Hosmer and Lemeshow test was insignificant.

In conclusion, the predictive accuracy of the ANNs prognostic model was superior to that of the nomogram in predicting 5-year graft survival. In a future project, we shall implement the ANNs model that was developed in this study in a form of software to make it available to estimate survival and prognosticate individual transplant recipients outcomes.",https://journals.lww.com/transplantjournal/fulltext/2008/11270/prediction_of_graft_survival_of_living_donor.12.aspx
Quinino et al. 2023,A Machine Learning Prediction Model for Immediate Graft Function After Deceased Donor Kidney Transplantation,"Background. 
After kidney transplantation (KTx), the graft can evolve from excellent immediate graft function (IGF) to total absence of function requiring dialysis. Recipients with IGF do not seem to benefit from using machine perfusion, an expensive procedure, in the long term when compared with cold storage. This study proposes to develop a prediction model for IGF in KTx deceased donor patients using machine learning algorithms.

Methods. 
Unsensitized recipients who received their first KTx deceased donor between January 1, 2010, and December 31, 2019, were classified according to the conduct of renal function after transplantation. Variables related to the donor, recipient, kidney preservation, and immunology were used. The patients were randomly divided into 2 groups: 70% were assigned to the training and 30% to the test group. Popular machine learning algorithms were used: eXtreme Gradient Boosting (XGBoost), Light Gradient Boosting Machine, Gradient Boosting classifier, Logistic Regression, CatBoost classifier, AdaBoost classifier, and Random Forest classifier. Comparative performance analysis on the test dataset was performed using the results of the AUC values, sensitivity, specificity, positive predictive value, negative predictive value, and F1 score.

Results. 
Of the 859 patients, 21.7% (n = 186) had IGF. The best predictive performance resulted from the eXtreme Gradient Boosting model (AUC, 0.78; 95% CI, 0.71–0.84; sensitivity, 0.64; specificity, 0.78). Five variables with the highest predictive value were identified.

Conclusions. 
Our results indicated the possibility of creating a model for the prediction of IGF, enhancing the selection of patients who would benefit from an expensive treatment, as in the case of machine perfusion preservation.","This single-center, retrospective study analyzed a homogeneous population of 859 unsensitized patients undergoing their first KTx with kidneys from deceased donors and compared several ML algorithms for the prediction of IGF. An ML model was developed that identified patients who will develop IGF after KTx.

Popular algorithms were used to develop the model. When all available variables were used, the algorithm that presented the best prediction of IGF was XGBoost (AUC, 0.78; sensitivity, 0.64; specificity, 0.78; PPV, 0.44; NPV, 0.92). The other algorithms had similar AUCs, which demonstrates the predictive ability of the dataset, even if with a lower NPV. It is important to emphasize that ML models are capable of learning complex relationships between variables. XGBoost is a learning-by-set method that brings together decision trees as building blocks to construct a strong “learner” that can discover nonlinear relationships between predictors and the outcome. Recently, XGBoost has been shown to have a better predictive performance than that in other ML algorithms in various contexts.61-63

Several predictive models developed by conventional statistics have been published with good performance in the population in which they were developed.20-23 However, in validation studies, these models were not generalizable, with the exception of the model developed by Irish et al. 24-26 However, this is a complex nomogram that uses many variables not frequently available and may not be applicable in clinical practice.20 More recently, studies developed by ML were published compared with those of classic statistics models to predict DGF and concluded that ML was superior.37,38

The novelty of our study lies in the prediction of IGF instead of DGF. The models described in the literature, either using conventional statistics or ML, used the DGF as an outcome. The prediction of DGF can be complex because of different definitions, but most of them are based on the need for dialysis in the first week after KTx.

Universal indications for dialysis are few (extreme hypervolemia, severe hyperkalemia, and very high BUN levels). However, several indications may change according to the nephrologist’s discretion. The indication according to the BUN level varies among centers. Economic issues also exist, such as dialysis fees or limiting it due to the maximum number of dialysis that can be reimbursed by the health system. In addition, the incidence of dialysis in the first week also varies if dialysis is performed in the morning of the 8th day instead of the night of the 7th day due, for example, to the availability of the dialysis facility, only to mention a few. For these reasons, models described in the literature do not perform well when applied to other centers.

Conversely, IGF defined as patients presenting good diuresis from the beginning, with a daily drop in SCr levels and rapid renal function recovery adds no margins for doubts, despite the nephrologist who evaluates the patient or the logistic conditions of the hospital. Besides, it seems that there is no long-term (3-y) improvement in graft survival by machine perfusion in patients who do not develop DGF.64

According to the importance of variables reported as a result of SHAP in the complete model (Figure 4), as well as for the analysis of a minimum set of variables with Boruta’s variable selection algorithm (described in Table S2, SDC, https://links.lww.com/TP/C674) the most relevant variables for predicting IGF were donor final serum Cr, KDRI, donor mean blood pressure, donor 24-h diuresis and donor age. In the literature, there are several predictive models for DGF, but we did not find any reference to the prediction of IGF. Younger donors and lower CIT are described in a study published by Siénko et al, which describes factors associated with IGF.65 On the other hand, although not described in the literature as factors associated with IGF, donor serum creatinine, donor mean blood pressure, and diuresis reflect maintenance conditions before organ procurement.

By identifying patients with a possible outcome of IGF, patients with a higher chance of developing either DGF or SGF were combined. It seems that patients with SGF have long-term outcomes comparable to those of the DGF group.8-10 Therefore, both SGF and DGF groups may be considered unique groups that may need special assistance to improve their outcome, for example, with the use of hypothermic MP.

MP is an expensive procedure (the cost in Brazil is US $2000.00/kit). Besides requiring special logistics, MP is a high-cost procedure, especially for developing countries like Brazil, where MP is not reimbursable by the health system.

The idea, therefore, is to use MP only in kidneys with a higher chance of developing DGF/SGF. Specifically, in this condition, there is a need to present to the health authorities that this extra expense is cost effective, reducing hospital stays, days at the intensive care units, exams, biopsies, etc.18 With MP, it has been reported that cost reduction was approximately US $2626 per procedure.66

A Brazilian collaborative, randomized study of kidney recipients from the same deceased donor showed a 16% reduction in DGF (61% versus 45%) with MP versus cold storage (CS).67

Another major study comparing CS with MP in kidneys from the same donor showed a small reduction (5.7% with marginal statistical significance, P = 0.05) in the incidence of DGF with no differences in the duration of DGF, creatinine clearance, acute rejection episodes, and length of hospital stay.68

In a 3-y analysis of the same cohort, overall, 3-y graft survival was better for machine-perfused kidneys (91% versus 87%; adjusted hazard ratio for graft failure, 0.60; P = 0.04). However, when evaluating survival in patients who did not develop DGF, there were no differences in the 3-y graft survival.64

Therefore, the results of MP in terms of DGF rate and costs are not striking to introduce the procedure worldwide. Including kidney transplant recipients who possibly will develop IGF (around 20%–25% of patients) in the MP studies may reduce the power of these studies in demonstrating a cost-effective benefit of MP in brain-dead kidney donors. By excluding patients with a higher chance of IGF, the statistical results of studies comparing MP with CS may be enriched. This is the next step after developing our model.

The recorded incidence of DGF in our country is high, with rates much higher than those reported in other countries. In this study, the incidence of DGF was 49%, higher when compared with the rates described in the United States and Europe, but comparable to what has been described in Brazil and even lower when compared to the incidence of some Brazilian centers that describe rates of up to 82% and 87.7%.69

The exact reason for the high incidence of DGF in Brazil is not known, but it is likely to be related to the maintenance of the potential donor before organ procurement and prolonged CIT due to procurement and organ distribution in a large country.36

Our study has several limitations. This study was conducted at a single center with a very “homogeneous” population (first KTx, nonsensitized patients) designed to avoid biases that could increase the incidence of DGF. For this reason, it is possible that the model may not be generalizable to the entire transplant cohort, which includes sensitized recipients and retransplants. Although access to data from another research center could be used to test the generalization of the model, there would be a need to retrain the model with local data given the change in the distribution of variables, which would lead to inadequate extrapolation. For this reason, it is possible that the model cannot be directly extended to the entire transplant population including sensitized recipients or retransplants. However, a single-center–based model may be more accurate and practical for clinical applicability in its specific population. The small number of patients can also be considered a limitation when compared with other ML models recently published in the literature38,39 and thus, the high NPV found for this study could be different in centers with low DGF prevalence. Nevertheless, it can be looked at as a pilot study to develop a larger analysis at a multicenter level.

In summary, an ML model was developed to identify transplanted patients with a higher chance of developing IGF after KTx. By excluding them, a prospective study comparing CS and MP in patients with a higher chance of developing DGF will be developed to demonstrate a higher cost-effective benefit of MP, which might largely overcome the costs of the MP procedure.",https://journals.lww.com/transplantjournal/fulltext/2023/06000/a_machine_learning_prediction_model_for_immediate.29.aspx
Greco et al. 2010,Decisional Trees in Renal Transplant Follow-up,"Introduction
The predictive potentialities of application of data mining algorithms to medical research are well known. In this article, we have applied to a transplant population classification trees to build predictive models of graft failure, evaluating the interactions between body mass index (BMI) and other risk factors. The decision trees have been widely used to represent classification rules in a population by a hierarchical sequential structure.

Patients and methods
We retrospectively studied 194 renal transplant patients with 5 years of follow-up (128 males, 66 females, mean age at time of transplant of 43.9 ± 12.5 years). Exclusion criteria were: age < 18 years, multiorgan transplant, and retransplant. The BMI was calculated at the time of transplantation. In the classification algorithm, we considered the following parameters: age, sex, time on dialysis, donor type, donor age, HLA mismatches, delayed graft function (DGF), acute rejection episode (ARE), and chronic allograft nephropathy (CAN). The primary endpoint was graft loss within 5-years follow-up.

Results
The classification algorithm produced a decision tree that allowed us to evaluate the interactions between ARE, DGF, CAN, and BMI on graft outcomes, producing a validation set with 88.2% sensitivity and 73.8% specificity. Our model was able to highlight that subjects at risk of graft loss experienced one or more events of ARE, developed DGF and CAN, or has a BMI > 24.8 kg/m2 and CAN.

Conclusions
The use of decision trees in clinical practice may be a suitable alternative to the traditional statistical methods, since it may allow one to analyze interactions between various risk factors beyond the previous knowledge.","Machine learning is a group of techniques (algorithms) to obtain knowledge from a data set.1, 3 These automatic learning algorithms are divided in three types:
1
Supervised learning, comprising algorithms like decision trees, decision rules, and support vector machines in which the data are pattern input/output returning <output> functions of the analysis of input observations.1, 2

2
Unsupervised learning, comprising algorithms like clustering in which the data are represented by a sample without any associated target value. These algorithms provide, for example, probability density, cluster classes, and so on.2, 3, 4

3
Reinforcement learning: algorithms like neural networks or genetic algorithms that are able to learn and to adapt themselves to ambient mutations; they implement the strategies to the “reward” magnification.5, 6


Medical diagnoses are problems usually faced with a supervised approach. Between several automatic supervised learning algorithms, we decided to use decision trees not only for their ability to provide models that are easily understood by clinicians, but also for the possibility to analyze the classification structures, validating their accuracy by knowledge and medical experience. The structure of a typical decision tree is recursively built and constituted by: leaf nodes; nonleafs (internal leafs or test leafs); branches.

The nonleafs are labeled by attribute nouns, while the leaf nodes are labeled by the class value, and the branches, by values, setting the instance routing toward a specific leaf node (decisional rules). A decision tree includes as input an element x ∈ X described by a property set (attribute-value couples) and produces a binary decision (yes–no) as the output.

Hence, decision trees are useful tools to represent the classification rules of objects (described by a group of attributes and by a class label), by a hierarchical and sequential structure that shares the data recursively and by groups based on values assumed by the variables.

The aim of our study was to build predictive models of graft failure able to find interactions between BMI and other risk factors involved in graft failure.7, 8, 9, 10, 11 The machine learning (supervised learning), by an easily understandable model like a decision tree, confirmed the hypothesis of predisposition to graft failure among patients who experienced particular events (DGF, ARE, CAN) or particular conditions (high BMI).

In conclusion, use of decision trees in clinical practice may be a suitable alternative to traditional statistical methods, since it may allow analysis of the interactions between various risk factors beyond previous knowledge. In our article, we have demonstrated a statistically significant trend among the data set. The most important contribution provided by our work was the observation of usefulness of classification trees in the follow-up of patients with renal transplantations.",https://www.sciencedirect.com/science/article/pii/S0041134510003490
Senanayake et al. 2010,Using machine learning techniques to develop risk prediction models to predict graft failure following kidney transplantation: protocol for a retrospective cohort study,"Background: A mechanism to predict graft failure before the actual kidney transplantation occurs is crucial to clinical management of chronic kidney disease patients.  Several kidney graft outcome prediction models, developed using machine learning methods, are available in the literature.  However, most of those models used small datasets and none of the machine learning-based prediction models available in the medical literature modelled time-to-event (survival) information, but instead used the binary outcome of failure or not. The objective of this study is to develop two separate machine learning-based predictive models to predict graft failure following live and deceased donor kidney transplant, using time-to-event data in a large national dataset from Australia.  

Methods: The dataset provided by the Australia and New Zealand Dialysis and Transplant Registry will be used for the analysis. This retrospective dataset contains the cohort of patients who underwent a kidney transplant in Australia from January 1 st, 2007, to December 31 st, 2017. This included 3,758 live donor transplants and 7,365 deceased donor transplants. Three machine learning methods (survival tree, random survival forest and survival support vector machine) and one traditional regression method, Cox proportional regression, will be used to develop the two predictive models (for live donor and deceased donor transplants). The best predictive model will be selected based on the model’s performance.

Discussion: This protocol describes the development of two separate machine learning-based predictive models to predict graft failure following live and deceased donor kidney transplant, using a large national dataset from Australia. Furthermore, these two models will be the most comprehensive kidney graft failure predictive models that have used survival data to model using machine learning techniques. Thus, these models are expected to provide valuable insight into the complex interactions between graft failure and donor and recipient characteristics.","This protocol describes the development of two separate machine learning-based predictive models to predict graft failure following live and deceased donor kidney transplant, using a large national dataset from Australia. The live donor risk prediction model will be the first machine learning based predictive model developed using a large national dataset, and the deceased donor risk prediction model will be the only machine learning based predictive model that used more than 7,000 patient records outside the United States. Furthermore, these two models will be the only two predictive models which used post kidney transplant graft survival data to model using machine learning techniques. Thus, the two predictive models are expected to provide valuable insight into the complex interactions between graft failure and donor and recipient characteristics.

The dataset necessary for the study was provided by ANZDATA. ANZDATA collects and reports the incidence, prevalence and outcome of dialysis treatment and kidney transplantation for patients with end-stage kidney disease across Australia and New Zealand. This registry started in 1977 and since then all the kidney transplant activities in Australia and New Zealand have been captured in the registry, including the transplants in the private sector. The inclusion of all kidney transplants in Australia and New Zealand, and the availability and longevity of follow-up information have been the strength of this registry 42. Thus, this registry has been the source of information for numerous high-impact publications 43– 45.

The current study proposes to use four methods, namely: Cox proportional regression, survival support vector machine, random survival forest and survival tree. The best machine learning technique available to develop a predictive model is currently being discussed widely 46. Most are of the view that no single technique fits all datasets, and it depends on the complexity of the data 47. Thus, it is imperative that different machine learning methods are used to develop predictive models on a single dataset, so that the best could be chosen using validation parameters.

This project will have some limitations. According to medical literature, there are an abundance of risk factors for graft failure following a kidney transplant. However, the proposed predictive models will be only based on the variables which have already collected by ANZDATA, thus a complete risk factor profile may not be captured. The graft failure is linked to genetic 48 and socio-economic factors 49 of the transplant population. Thus, generalisability of the proposed models to other settings outside Australia, needs to be assessed further after they have been developed.

Go to:",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7199287/
Lee et al. 2018,Prediction of Acute Kidney Injury after Liver Transplantation: Machine Learning Approaches vs. Logistic Regression Model,"Acute kidney injury (AKI) after liver transplantation has been reported to be associated with increased mortality. Recently, machine learning approaches were reported to have better predictive ability than the classic statistical analysis. We compared the performance of machine learning approaches with that of logistic regression analysis to predict AKI after liver transplantation. We reviewed 1211 patients and preoperative and intraoperative anesthesia and surgery-related variables were obtained. The primary outcome was postoperative AKI defined by acute kidney injury network criteria. The following machine learning techniques were used: decision tree, random forest, gradient boosting machine, support vector machine, naïve Bayes, multilayer perceptron, and deep belief networks. These techniques were compared with logistic regression analysis regarding the area under the receiver-operating characteristic curve (AUROC). AKI developed in 365 patients (30.1%). The performance in terms of AUROC was best in gradient boosting machine among all analyses to predict AKI of all stages (0.90, 95% confidence interval [CI] 0.86–0.93) or stage 2 or 3 AKI. The AUROC of logistic regression analysis was 0.61 (95% CI 0.56–0.66). Decision tree and random forest techniques showed moderate performance (AUROC 0.86 and 0.85, respectively). The AUROC of support the vector machine, naïve Bayes, neural network, and deep belief network was smaller than that of the other models. In our comparison of seven machine learning approaches with logistic regression analysis, the gradient boosting machine showed the best performance with the highest AUROC. An internet-based risk estimator was developed based on our model of gradient boosting. However, prospective studies are required to validate our results.","We compared the predictive ability of 7 machine learning models and a logistic regression model to predict AKI after liver transplantation. The result showed that gradient boosting machine has the largest AUROC and highest accuracy to predict both AKI of all stages and stage 2 or 3 AKI. The authors previously reported the superior predictive ability by the gradient boosting for AKI after cardiac surgery [26]. Our study results are consistent with the previous study with different analysis tool. We also developed an internet-based risk estimator based on our gradient boosting model. This estimator should be prospectively validated for its clinical use to determine the risk of AKI at the end of liver transplantation surgery. Further prospective multicenter trials are required to validate the better performance of gradient boosting.
For the prediction of a non-linear relationship, the gradient boosting machine builds a sequential series of decision trees, where each tree corrects the residuals in the predictions made by the previous tress. After each step of boosting, the algorithm scales the newly added weights, which balances the influence of each tree. In addition, the gradient boosting machine uses techniques that aim to reduce overfitting by only a random subset of descriptors in building a tree [23]. The impact of gradient boosting machine has been recognized in a number of machine learning and data mining challenges. Our results also demonstrated that the gradient boosting machine appears to be a very effective and efficient machine learning method.
Although its predictive ability was less than that of the gradient boosting machine, the performance of random forest was also better than that of logistic regression in our dataset. Random forest is an extension of traditional decision tree classifiers with an ensemble technique [27]. Each tree is constructed using a random subset of the original training data and a random subset of the explanatory variables. Random forest can minimize overfitting by making the decision by voting of these randomly generated trees [28]. However, this advantage of random forests seems much more effective when many variables and a large number of datasets are used for learning. The results of the present study showed no significant performance gain over the simple decision tree model.
Simple decision tree analysis showed a good performance in our study. A decision tree is a hierarchical model that recursively splits the dataset based on the Gini impurity or entropy [29,30,31]. A decision tree could have better prediction ability than logistic regression model under certain circumstances because it uses different variable and threshold in every branch. The greatest advantage of the decision tree model is that it gives interpretable decision rules after training. However, the deeper the depth of the tree, the more difficult it is to interpret, and the risk of overfitting also increases.
Although multilayer perceptron showed a good performance to predict in-hospital mortality in a previous study [1], the performance of the neural network and deep belief network in our study was inferior to those of all other machine learning techniques. The reason for this may be explained by the fact that the relationship between the explaining variables and outcome variable is largely non-linear. Although multilayer perceptron is able to approximate any nonlinear function, a large number of learning data are required. Therefore, our dataset might be not large enough to train the multilayer perceptron [32].
There have been some reports that performance of the machine learning techniques are not superior to conventional risk score or the logistic regression model to predict mortality [1,33]. However, in these studies, they compared the performance to predict in-hospital mortality in a study sample with a low incidence (<1%) [1,33]. Therefore, the difference might not be statistically proven. In this study, it could be compared easily with a higher incidence of the outcome variable (30.3%).
Cold ischemic time and intraoperative mean SvO2 were considered to be the most important variables to classify the development of AKI by the gradient boosting machine and decision tree [34,35]. This may be attributed to the fact that our study sample consisted of mixed populations including both DDLT and LDLT patients. The wide variability in cold ischemic time between deceased donor and living donor and a higher incidence of AKI in DDLT may have produced the high discriminative power of cold ischemic time. Therefore, cold ischemic time may be less important for prediction of AKI after either DDLT or LDLT. A low SvO2 suggests poor oxygen delivery to the major organs including the kidney [36]. During liver transplantation, clamping of inferior vena cava and intraoperative bleeding decrease cardiac output and major organ perfusion, resulting in poor oxygen delivery, especially during graft reperfusion [37]. Therefore, SvO2 may be an important hemodynamic goal. Further prospective trials may evaluate the effect of the optimizing intraoperative SvO2 to reduce the risk of AKI after liver transplantation.
Our study has several limitations. First, our analysis used only a small number of cases from data derived from an Asian single-center with mixed living and deceased donor transplantation. The performance of machine learning techniques might be different when they are applied to a sample of different institution with a different distribution of covariates. The race difference might be a significant predictor of AKI, which could not be evaluated in our dataset. External validity of our prediction model may be limited. By machine learning approach using each institution’s specific data set for training, each institution could obtain their own specific model best fit for their institution. We uploaded the source code for learning the gradient boosting model used in this study as a Supplemental Text S2. Therefore, each institution may develop their own prediction model by machine learning approach using their historical electronic medical record data and update their model periodically. The real-time processing of patient data would yield risk prediction for each patient after surgery. Second, the results from our machine learning models are more difficult to interpret than the results from the logistic regression model [3]. However, gradient boosting machine and decision tree provided for the modest interpretability through the variance importance plot and the inspection of the decision rule in tree nodes. Third, it is not certain that our results could translate into improved clinical outcomes for the patients undergoing liver transplantation. Many of our variables of importance are not clinically modifiable, and accurate risk prediction may not be followed by improved patient outcomes. However, a further prospective trial should evaluate whether the adjustment of hemodynamic variables including SvO2 could decrease the incidence of AKI.",https://www.mdpi.com/2077-0383/7/11/428
Chen et al. 2021,Development and performance assessment of novel machine learning models to predict pneumonia after liver transplantation,"Background
Pneumonia is the most frequently encountered postoperative pulmonary complications (PPC) after orthotopic liver transplantation (OLT), which cause high morbidity and mortality rates. We aimed to develop a model to predict postoperative pneumonia in OLT patients using machine learning (ML) methods.

Methods
Data of 786
Data of 786 adult patients underwent OLT at the Third Affiliated Hospital of Sun Yat-sen University from January 2015 to September 2019 was retrospectively extracted from electronic medical records and randomly subdivided into a training set and a testing set. With the training set, six ML models including logistic regression (LR), support vector machine (SVM), random forest (RF), adaptive boosting (AdaBoost), extreme gradient boosting (XGBoost) and gradient boosting machine (GBM) were developed. These models were assessed by the area under curve (AUC) of receiver operating characteristic on the testing set. The related risk factors and outcomes of pneumonia were also probed based on the chosen model.

Results
591 OLT patients were eventually included and 253 (42.81%) were diagnosed with postoperative pneumonia, which was associated with increased postoperative hospitalization and mortality (P < 0.05). Among the six ML models, XGBoost model performed best. The AUC of XGBoost model on the testing set was 0.734 (sensitivity: 52.6%; specificity: 77.5%). Pneumonia was notably associated with 14 items features: INR, HCT, PLT, ALB, ALT, FIB, WBC, PT, serum Na+, TBIL, anesthesia time, preoperative length of stay, total fluid transfusion and operation time.

Conclusion
Our study firstly demonstrated that the XGBoost model with 14 common variables might predict postoperative pneumonia in OLT patients.
","Early detection of postoperative pneumonia is critical for timely interventions to prevent the onset of the complication. Until now, the predication of postoperative pneumonia has been challenging, and there is need for reliable and accurate predictive model for patients after liver transplantation. This study, based upon large volume of data and ML methods, has the following major novel findings: (1) The incidence of postoperative pneumonia was high in patients after OLT, and the occurrence was significantly associated with prolonged hospital stay and increased mortality after liver transplantation; (2) A total of 14 factors were identified to be significantly correlated with postoperative pneumonia after OLT, including INR, HCT, PLT, ALB, ALT, FIB, WBC, PT, serum Na+, TBIL, anesthesia time, preoperative length of hospital stay, total fluid transfusion, and operation time; (3) The XGBoost model exhibited the best overall performance in predicting postoperative pneumonia among the developed ML models, with the value of AUC of 0.794, sensitivity of 52.6%, and specificity of 77.5%; (4) Multiple lines of evidence support that the XGBoost model holds promise for future clinical application in predicting postoperative pneumonia in patients after liver transplantation.

XGBoost model is recognized as an efficient and scalable tree boosting system [26], and it has performed well in the ML competitions, especially the simplicity in use and the accuracy in prediction [27, 28]. In the present study, we developed a total of six ML models, of these, XGBoost model had the best overall performance, with a specificity of 77.5% and a sensitivity of 52.6% in predicting postoperative pneumonia in OLT patients. In the study, the AUC values of LR, SVM, and MLP were relatively lower than other three ensemble machine learning models including XGboost, RF and GBM, whose accuracy and robustness might be attributed to their nature of integrating multiple base classifiers or learners. However, RF is a bagging ensemble, and it needs to train a large amount of decision trees and aggregate them. As a result, it usually takes much more time to trade numerous random computations for high accuracy, compared with GBM and XGboost, which both belong to boosting ensemble method. Moreover, compared to GBM, XGboost leverages second order derivative and implements sampling method in each iteration to alleviate overfitting and speed up computation.

Considering the high prevalence of multi-drug resistant bacteria in post-transplant patients induced by the excessive use of antibiotics [4], high specificity is especially necessary in clinical practice to avoid an unnecessary and overuse of antibiotics in low-risk patients. By contrast, all patients received peri-operative antibiotic therapy for 72 h, and this has posed considerable challenge in predicting pneumonia at an early stage [29]. Therefore, the novel XGBoost model as established in this study may assist clinicians in making optimal interventions and treatments, and eventually improve care for affected patients.

It has been reported that a number of risk factors, including age of recipient, liver dysfunction score, indication for OLT, perioperative transfusions especially the blood and fresh frozen plasma units, restrictive preoperative pulmonary testing pattern and INR measured prior OLT, are significantly associated with post-liver transplant pneumonia [3, 30, 31]. However, these factors are limited for its underutilization of within-category information, causing a loss of information [32]. For instance, patients above or below the optimal cut-point value had been equally considered in the risk-factor prediction, yet the risk of post-transplant pneumonia may vary considerably. As the risk-factor prediction is developed with neither combining all factors together nor weighting difference between different factors, it is not widely used in clinical practice. In addition, the traditional scores were given on the basis of the assumption that all misclassification errors have equal costs. In fact, this assumption is indefensible if apply in real-world applications [33]. In this study, we applied RFE feature selection method on 33 features which were statistically significant, of which 14 best features with the highest sensitivity score, including preoperative laboratory results of INR, HCT, PLT, ALB, ALT, FIB, WBC, PT, serum Na+, TBIL, anesthesia time; preoperative length of hospital stay, total fluid transfusion, and operation time. We found that most of the factors have been reported to be associated with pneumonia and PPCs except for PLT and serum Na+ [18, 30, 31, 34, 35]. As the risk factors reported in different literatures are quite different and this may be attributed to different population and definition of pneumonia and PPCs, we think it just reflects the advantage of ML models to capture previously unknown correlations in big data. Although the underlying mechanism remained unclear, the high clinical relevance of these factors laid a solid foundation for the consequent ML process and made the conclusion more practical and clinically valuable [36]. Moreover, we found the 14 features in ML model were all routinely recorded and widely used, and no factors need special instrument or equipment to obtain, indicating that our models are feasible and can be widely used in hospitals.

To date, ML models have shown outstanding performance in prediction of diseases and clinical conditions, for which these models can be helpful in decision-making about the use of interventions and medications [33]. For example, ML models can generate an individualized probability for each patient. Additionally, implementation of sophisticated computer algorithms at the bedside has become a reality since the popularity of EPR systems and wide availability of structured patient data. In our study, the EPR systems included HIS, LIS, PACS, and Docare Anesthesia System, which allowed us to integrate medical data generated during admission, covering demographic data, daily documentation, laboratory and imaging results, anesthesia records and thorough record of medication, and treatment. In addition, we separated the patients 1000 times (70% train and 30% test) into 1000 different pairs of train and test sets and this could minimize accidental error and enhance the accuracy of the current ML models. This result showed that in predicting post-transplant pneumonia, we should not apply only one of the ML model.

In the study, we found that patients with hepatic malignancy, better hepatic function before surgery, and longer hospital stay before surgery were significantly associated with lower risk of developing postoperative pneumonia. We postulated that this could be attributed to the better preoperative treatment and preparation, suggested that interventions should be implemented to improve the patients’ overall preoperative conditions. In consistence with previous reports [37, 38], we identified that a number of intraoperative factors, such as the longer operation and anesthesia time, excessive blood product transfusion, and fluid transfusion, were significantly related to postoperative pneumonia in patients following liver transplantation. By contrast, we found that there was an association between the use of telipressin and dopamine and decreased incidence of postoperative pneumonia in patients after liver transplantation. These findings are clinically important for the intraoperative anesthetic management and help improving the clinical outcomes.

The study may have several limitations. Firstly, the ML models are developed on the basis of a single-center cohort study, and future multi-center study will be needed for external validation. Secondly, this study is performed retrospectively, for which collection and entry bias, as well as possible residual confounding may occur. Thirdly, we were unable to incorporate the metrics of liver donors as training variables in our study, due to the lack of donor information in the EPR systems of our hospital.
",https://respiratory-research.biomedcentral.com/articles/10.1186/s12931-021-01690-3#Sec15
Medved et al. 2023,Simulating the Outcome of Heart Allocation Policies Using Deep Neural Networks,"Abstract— We created a system to simulate the heart allocation process in a transplant queue, using a discrete event model
and a neural network algorithm, which we named the Lund
Deep Learning Transplant Algorithm (LuDeLTA). LuDeLTA is
utilized to predict the survival of the patients both in the queue
and after transplant. We tried four different allocation policies:
wait time, clinical rules and allocating the patients using either
LuDeLTA or The International Heart Transplant Survival
Algorithm (IHTSA) model. Both IHTSA and LuDeLTA were
used to evaluate the results. The predicted mean survival for
allocating according to wait time was about 4,300 days, clinical
rules 4,300 days and using neural networks 4,700 days.","We used two predictive models to mitigate the bias
introduced by using the same predictive model to both
prioritize the patients during allocation and then to evaluate
the survival for the same set of patients.
The hyperparameters of the LuDeLTA models such as
topology, activation function and drop out, were chosen by
empirical testing of the models using 5-fold cross-validation
on the training data, to maximize the performance metrics.
We chose to only use half of the patients from the time
period in the simulation, this is to minimize the bias of
using the same patients in both training and validation of the
models. This means that all the results obtained in Table III
represent a queue with half of the patient, in contrast to
the real historic UNOS queue. This could influence metrics
such a mean wait time or mean survival time using a
neural network as the allocation method. The latter because
the potential number of recipient-donor pairs is lower to
maximize the predicted survival on.
The reason that only the clinical rules discarded hearts
for transplant, was that the only requirement for a transplant
to occur for the others was blood group compatibility. The
waiting list was sufficiently large to always have a compatible
recipient for the donors.
In this paper, we have shown that an organ transplant
queue can be simulated by utilizing neural networks to predict survival, both pre- and post-transplant. Additionally
we have shown that using neural networks as the allocation
policy, could possibly result in longer survival post-transplant
for the patients.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8513637
Nam et al. 2020,Novel Model to Predict HCC Recurrence after Liver Transplantation Obtained Using Deep Learning: A Multicenter Study,"Several models have been developed using conventional regression approaches to extend the criteria for liver transplantation (LT) in hepatocellular carcinoma (HCC) beyond the Milan criteria. We aimed to develop a novel model to predict tumor recurrence after LT by adopting artificial intelligence (MoRAL-AI). This study included 563 patients who underwent LT for HCC at three large LT centers in Korea. Derivation (n = 349) and validation (n = 214) cohorts were independently established. The primary outcome was time-to-recurrence after LT. A MoRAL-AI was derived from the derivation cohort with a residual block-based deep neural network. The median follow-up duration was 74.7 months (interquartile-range, 18.5–107.4); 204 patients (36.2%) had HCC beyond the Milan criteria. The optimal model consisted of seven layers including two residual blocks. In the validation cohort, the MoRAL-AI showed significantly better discrimination function (c-index = 0.75) than the Milan (c-index = 0.64), MoRAL (c-index = 0.69), University of California San Francisco (c-index = 0.62), up-to-seven (c-index = 0.50), and Kyoto (c-index = 0.50) criteria (all p < 0.001). The largest weighted parameter in the MoRAL-AI was tumor diameter, followed by alpha-fetoprotein, age, and protein induced by vitamin K absence-II. The MoRAL-AI had better predictability of tumor recurrence after LT than conventional models. The MoRAL-AI can also evolve with further data.","In this study, we developed and validated a novel prediction model, called MoRAL-AI, for tumor recurrence after LT in patients with HCC. To the best of our knowledge, this is the first prediction model based on deep learning algorithms. The performance of MoRAL-AI was confirmed by using an independent validation cohort and was better than that of the MC, currently the most widely used criteria, as well as other prediction models. The MoRAL-AI is served through the website, and it can evolve with further data accumulation from various cohort groups. With this evolution, we can establish more evolved criteria of LT.
For LT as a treatment option for HCC, the underlying hepatic function of the recipient is not an important factor. While the recipient usually has liver cirrhosis before LT, the recipient liver is completely replaced by the donor liver and the severity of the pre-LT cirrhosis might thus not affect the post-LT tumor recurrence [18]. Therefore, predictive factors that are related to the post-LT HCC recurrence might include only the tumor burden and the biological aggressiveness of tumor cells before LT. Imaging studies can provide tumor-related information (e.g., tumor number, maximum tumor diameter, and vascular tumor invasion). On the other hand, serum levels of tumor markers (AFP and PIVKA-II) can reflect both tumor burden and biological aggressiveness because AFP and PIVKA-II had a significantly positive correlation with histological aggressive findings (microvascular invasion, perineural invasion, and serosal invasion) as well as tumor burden. Among previous models, the MC, UCSF, and up-to-seven models comprise only the factors related to imaging-based tumor burden, whereas the MoRAL score consists of only serum tumor markers [1,3,5,6]. The Hangzhou criteria and Kyoto criteria consider both imaging-based tumor burden and tumor markers [4,19]. The MoRAL-AI was developed based on both imaging-based tumor burden and biochemical tumor markers to maximize its performance.
The general disadvantage of the deep learning method is that it typically requires a large amount of data; it has previously been applied to the analysis of medical images such as plain X-ray, CT, or histology images, whose data can be abundantly obtained since a number of images are being taken during daily clinical practice [20,21]. However, when a specific disease is being analyzed, the size of the available data set is generally limited. For example, in Korea, only 1400 cases of LT were performed in 2015 [22], whereas approximately 265,000,000 plain X-rays were performed [23]. Moreover, it is generally complicated to collect demographic information and clinical results via a unified form due to the different data forms used by hospitals. Thus, data from fewer than 1000 cases have been analyzed with conventional statistical methods that can evaluate a relatively small dataset. However, our current model was derived from a relatively small derivation cohort containing 349 patients and showed a better predictive power than previous models. This result might suggest that DNN models can be developed even with relatively few data points if the potential prognostic factors for the prediction model have previously been well identified. Indeed, the predictive factors used in this study, such as tumor diameter [1,3,4,5,19], tumor number [1,3,4,5], AFP [6,19], PIVKA-II [4,6], and portal vein invasion [1,3,4,5,19,24], were well-identified parameters in previous studies.
The DNN model has several strengths compared with conventional statistical modeling. First, by applying a deep learning method to the prediction model, it is possible to derive more accurate continuous probability results. In the conventional statistical method, a scoring system is established to divide risk groups and continuous variables are stratified according to arbitrary cutoffs. However, DNN models can use continuous data rather than converting them into categorical or binary variables. Thus, they can provide more accurate and individualized results, which means that they can calculate the individual tumor recurrence probability at any time point after an LT according to baseline clinical information. Second, because DNN models were originally designed to be calculated by computer, there is no need to focus on ease of use. In contrast, previous conventional models had to be intuitive and easy to use and simple models comprising fewer factors were thus preferred. However, because DNN models involve an automatic calculation on a web application, there is no need to limit the number of factors or to consider the complexity of the formulae. Third, DNN models can evolve continuously with further data accumulation [25]. Previous models, such as the MC system which was developed in 1996, have not been changed, despite the accumulation of new data. However, DNN models can continuously improve their performances through additional data training.
This study has several limitations. First, it is impossible to understand the outcome operations resulting from deep learning. This is a general shortcoming of deep learning methods. Second, because this model was developed from Asian patients who underwent living donor LTs and whose underlying liver diseases is predominantly chronic hepatitis B, further validation in Western countries and deceased donor LT cohorts is warranted. Our model can provide additional options to select for deceased or living donor LT in our web application with further validation. Third, PIVKA-II is generally measured in Asian countries before LT, but is less commonly measured in Western countries. With further data training with other data sets, it may be possible to develop another model with high performance power without a certain factor like PIVKA-II.",https://www.mdpi.com/2072-6694/12/10/2791
He et al. 2022,An Imageomics and Multi-Network based Deep Learning Model for Risk Assessment of Liver Transplantation for Hepatocellular Cancer,"Introduction: 
Liver transplantation (LT) is an effective treatment for hepatocellular carcinoma (HCC), the most common type of primary liver cancer. Patients with small HCC (<5 cm) are given priority over others for transplantation due to clinical allocation policies based on tumor size. Attempting to shift from the prevalent paradigm that successful transplantation and longer disease-free survival can only be achieved in patients with small HCC to expanding the transplantation option to patients with HCC of the highest tumor burden (>5 cm), we developed a convergent artificial intelligence (AI) model that combines transient clinical data with quantitative histologic and radiomic features for more objective risk assessment of liver transplantation for HCC patients.

Methods:
Patients who received a LT for HCC between 2008-2019 were eligible for inclusion in the analysis. All patients with post-LT recurrence were included, and those without recurrence were randomly selected for inclusion in the deep learning model. Pre- and post-transplant magnetic resonance imaging (MRI) scans and reports were compressed using CapsNet networks and natural language processing, respectively, as input for a multiple feature radial basis function network. We applied a histological image analysis algorithm to detect pathologic areas of interest from explant tissue of patients who recurred. The multilayer perceptron was designed as a feed-forward, supervised neural network topology, with the final assessment of recurrence risk. We used area under the curve (AUC) and F-1 score to assess the predictability of different network combinations.

Results:
A total of 109 patients were included (87 in the training group, 22 in the testing group), of which 20 were positive for cancer recurrence. Seven models (AUC; F-1 score) were generated, including clinical features only (0.55; 0.52), magnetic resonance imaging (MRI) only (0.64; 0.61), pathological images only (0.64; 0.61), MRI plus pathology (0.68; 0.65), MRI plus clinical (0.78, 0.75), pathology plus clinical (0.77; 0.73), and a combination of clinical, MRI, and pathology features (0.87; 0.84). The final combined model showed 80% recall and 89% precision. The total accuracy of the implemented model was 82%.

Conclusion:
We validated that the deep learning model combining clinical features and multi-scale histopathologic and radiomic image features can be used to discover risk factors for recurrence beyond tumor size and biomarker analysis. Such a predictive, convergent AI model has the potential to alter the LT allocation system for HCC patients and expand the transplantation treatment option to patients with HCC of the highest tumor burden.","Liver transplantation is currently the most successful treatment for HCC despite the risk of recurrence in patients with large HCC. To our knowledge, this i-RAPIT model is the first decision support tool for HCC-recurrence risk stratification of liver transplant patients using convergent AI techniques and improves patient-physician engagement in making an informed decision on whether or not there is a high risk of HCC recurrence after liver transplant. We presented the results of applying the i-RAPIT model to stratify recurrence risk for post-liver transplantation patients. Our risk prediction model demonstrates high sensitivity and specificity.

The advantages of the proposed multi-network model include the following. First, the i-RAPIT model can seamlessly learn from both annotated data and a large number of unannotated clinical image data. Combining labeled data and large amounts of unannotated data can help the deep learning model to better generalize than using labeled data alone. Second, the model uses multiple types of input information including images and clinical report features, whereas the traditional deep networks, such as CNNs [37], can only use a single type of data. Third, it makes use of a high-performance and data-efficient deep network, which our prior work has shown to outperform state-of-the-art deep networks terms of classification accuracy while using an order of magnitude fewer data [37]. Fourth, it employs an effective Bayesian deep learning method to estimate the certainty of its prediction besides the final recurrence risk calculation.

In the i-RAPIT model, CapsNet network extracts the features from MR images and pathological images, and these features contain the spatial information that is fundamentally different from CNN features. Previous work [48] did the activation maximization analysis and showed that CapsNet network features could be used to better describe all facets of tumor objects or cancer cell conditions than CNN features. The researchers believed that the explainable features used for the machine learning model could improve prediction performance in clinics [50]. The CapsNet network used in i-RAPIT model with routing and reconstruction contains the object spatial information across all capsule vectors. The capsule vectors could indeed capture clinical parameters of tumor objects or cancer cells, which is a major benefit over CNN features. Combining with the clinical signatures, i-RAPIT model explores multiple types of data to identify clinical markers that predict recurrence after liver transplant for HCC.

The i-RAPIT model was built, refined, and validated with patient data from a single large urban hospital system in one of the most ethnically diverse cities (Houston, TX) and the multi-site testing was performed with data from hospitals still restricted to South Texas. This may affect generalizability of the study. A wider geographic spread, multi-center study is being planned to assess the scalability of the risk prediction model. This risk assessment is to support liver transplantation decision making for recipients with HCC and not a definitive diagnosis tool. For example, patients assessed as ‘intermediate risk’ (model output is close to 0.5) by the i-RAPIT model would still need to be engaged by their healthcare provider to arrive at an informed decision on whether to proceed with a liver transplantation.",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8054468/
Dag et al. 2017,Predicting heart transplantation outcomes through data analytics,"Predicting the survival of heart transplant patients is an important, yet challenging problem since it plays a crucial role in understanding the matching procedure between a donor and a recipient. Data mining models can be used to effectively analyze and extract novel information from large/complex transplantation datasets. The objective of this study is to predict the 1-, 5-, and 9-year patient's graft survival following a heart transplant surgery via the deployment of analytical models that are based on four powerful classification algorithms (i.e. decision trees, artificial neural networks, support vector machines, and logistic regression). Since the datasets used in this study has a much larger number of survival cases than deaths for 1- and 5-year survival analysis and vice versa for 9-year survival analysis, random under sampling (RUS) and synthetic minority over-sampling (SMOTE) are employed to overcome the data-imbalance problems. The results indicate that logistic regression combined with SMOTE achieves the best classification for the 1-, 5-, and 9-year outcome prediction, with area-under-the-curve (AUC) values of 0.624, 0.676, and 0.838, respectively. By applying sensitivity analysis to the data analytical models, the most important predictors and their associated contribution for the 1-, 5-, and 9-year graft survival of heart transplant patients are identified. By doing so, variables, whose importance changes over time, are differentiated. Not only this proposed hybrid approach gives superior results over the literature but also the models and identification of the variables present important retrospective findings, which can be the basis for a prospective medical study.","The main objectives of this paper were to develop an assumption-free, data-driven approach to predict the 1- (short-term), 5- (medium-term), and 9-year (long-term) outcomes for heart transplantation patients, and understand how the importance of the predictor variables change with these three time-points. To achieve these objectives, a data analytic methodology (framework), consisting of five phases, has been proposed. The proposed approach was used to investigate a large, feature-rich dataset obtained from UNOS, containing all recorded information on heart transplant operations that were performed in the U.S. between October 1, 1987 and December 31, 2012. In this current analysis of the UNOS dataset, the following research questions regarding heart-transplantation have been addressed:

1.
Can we develop an individualized patient model, with high AUC performance, for predicting heart transplantation outcomes?

2.
What predictive factors contribute to the outcome of a heart transplant for a given follow-up time-point?

3.
How does the contribution/importance of each of these factors change over time (based on 1-, 5-, and 9-year time-points for analysis)?

4.
Is it possible to group these variables whose effect change over time? If so, how can this be done to potentially provide insights to medical practitioners and/or different stakeholders?


It is important to note that Questions 2–4 have not been addressed previously in the heart transplantation literature. The following medically-relevant results were obtained: A) Our proposed approach can predict the 1-, 5-, and 9-year outcomes with a mean AUC score of 0.624, 0.676, and 0.838, respectively. Thus, our methodology can potentially assist medical-decision makers in evaluating the suitability of a donor heart for a given patient (and more generally to surgical patients' prioritization [68]); and B) We have identified 43 pre-operative variables that contribute to outcome prediction for at least one of the three time-points. We have also grouped these predictors into 7 groups, which reflect how the importance of these predictors change over time.

Based on this work, we believe that are some more general takeaways that can be shared with the business and data analytics communities. First, the importance of “asking the right questions”. Most of the previous studies, where data mining methods were applied to transplantation problems, addressed the following question: “What is the best model to predict the outcome at x years post-transplant ?” In our opinion, the innovation in these methods is based on using somewhat sophisticated modeling approaches. Second, we believe that any data analytics study/application should address a set of sequential questions. After all, the goal is to uncover hidden patterns and generate insights that are interesting and transforming. As an illustration, in the context of transplantation research, after determining the best model for x years, there are several opportunities for asking some additional questions:
•
With respect to the best model, are there other models that present somewhat similar results but can be more advantageous in practice? For example, these models may require less predictors, and/or can be easier to implement. We refer the reader to Ref. [12] for an example in the context of heart transplants.

•
What happens to the model if we change the number of years? This question has two dimensions: a) What happens in terms of the predictive performance? and b) Do the selected/significant predictor variables remain unchanged?

•
How can we communicate these results (or alternatively, how do uncover patterns from our results)? For example, in this paper, we resorted to a simple graph (i.e., Fig. 2) to communicate the change in the importance of the variables over time.

•
If the analysis is implemented in practice, does the model perform as expected. If not, is due to a technology change (e.g., a major breakthrough in transplantation research) or does it reflect some potential issues with the model?


The reader should note that these type of questions are inherent in the data analytic process. For example, the CRISP-DM (CRoss Industry Standard Process for Data Mining) framework emphasizes the sequential nature of data mining applications [69]. However, in our estimation, the literature does not necessarily reflect that.

In summary, this paper demonstrates how data analytic approaches can be used to generate new knowledge (in the context of heart transplantation research). Our approach can be used for applications where the classification problem is temporal (i.e., we want to predict a binary outcome over time). Obvious examples of this include other organ transplantation problems, especially since UNOS provides information on other organ transplants. Other examples can include: reliability/maintenance applications (predicting whether a system will fail at different time-points), customer relationship management (i.e. attrition of customers over time), and other health-analytics applications (e.g., detecting cancer in individuals). Finally, it should be noted that the analysis presented in this paper may inform new prospective studies that can test hypotheses based on the groupings of the variables from the hybrid model.",https://www.sciencedirect.com/science/article/pii/S0167923616301816
Nematollahi et al. 2017,Classification Models to Predict Survival of Kidney Transplant Recipients Using Two Intelligent Techniques of Data Mining and Logistic Regression,"Kidney transplantation is the treatment of choice for patients with end-stage renal disease (ESRD). Prediction of the transplant survival is of paramount importance. The objective of this study was to develop a model for predicting survival in kidney transplant recipients. In a cross-sectional study, 717 patients with ESRD admitted to Nemazee Hospital during 2008–2012 for renal transplantation were studied and the transplant survival was predicted for 5 years. The multilayer perceptron of artificial neural networks (MLP-ANN), logistic regression (LR), Support Vector Machine (SVM), and evaluation tools were used to verify the determinant models of the predictions and determine the independent predictors. The accuracy, area under curve (AUC), sensitivity, and specificity of SVM, MLP-ANN, and LR models were 90.4%, 86.5%, 98.2%, and 49.6%; 85.9%, 76.9%, 97.3%, and 26.1%; and 84.7%, 77.4%, 97.5%, and 17.4%, respectively. Meanwhile, the independent predictors were discharge time creatinine level, recipient age, donor age, donor blood group, cause of ESRD, recipient hypertension after transplantation, and duration of dialysis before transplantation. SVM and MLP-ANN models could efficiently be used for determining survival prediction in kidney transplant recipients.","We found SVM, MLP-ANN, and LR the most appropriate models for prediction of renal transplant recipient survival. Hoot [19] conducted a study to predict the graft survival rate of liver transplant recipients. The main limitation of this study was that it used only a small number of variables and it had only 67% accuracy. Brier [8] also found an overall prediction accuracy of 64% for LR and 63% for MLP-ANN. ANNs were most closely related to LR results for prediction and discriminant analysis for classification. In that study, only one factor, transplantation of kidney from a white donor to black recipient, was associated with a statistically significant risk factor [8]. MLP-ANN predictors have been shown to offer a more flexible modeling environment than other statistical methods [20].

In general, determining the accuracy of the predictive models to predict particular medical issues is very complicated. This complexity can be caused by factors such as lack of collecting critical data in appropriate time and location. Many previous studies in the area of survival predictions have been performed using different statistical techniques and ANN, which is a subset of the data mining techniques. Neural networks are one of the most widely used techniques in the field of medical survey data [4, 8, 20]. In this study, after SVM, the MLP-ANN model with an accuracy of 85.9%, was suitable for predicting the survival of transplantation. The results of this study were consistent with those of another study [7] with an accuracy of 78.5%. The SVM is another model based on the accuracy discussed in this study. The prediction accuracy of this model was higher than other models used. One of the reasons was that this method is a good technique to differentiate samples or boundary points. A study [13] demonstrated the usefulness of this technique in predicting survival, but the accuracy of the model has not been mentioned.

In conclusion, SVM and MLP-ANN models can efficiently be used to predict renal transplant recipient survival. Discharge time creatinine level, recipient age, donor age, donor blood group, cause of ESRD, recipient hypertension after transplantation, and duration of dialysis before transplantation were independent predictors for survival of kidney transplant recipients. Attention to the condition of dialysis before transplantation, control of high blood pressure at the discharge time and the cause of ESRD could efficiently be used for determining survival prediction in kidney transplant recipients. The results of this study were comparable with those from statistical models.",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5611541/
Oztekin et al. 2009,Predicting the graft survival for heart–lung transplantation patients: An integrated Data-Mining methodology,"Background: Predicting the survival of heart-lung transplant patients has the potential to play a critical role in understanding and improving the matching procedure between the recipient and graft. Although voluminous data related to the transplantation procedures is being collected and stored, only a small subset of the predictive factors has been used in modeling heart-lung transplantation outcomes. The previous studies have mainly focused on applying statistical techniques to a small set of factors selected by the domain-experts in order to reveal the simple linear relationships between the factors and survival. The collection of methods known as 'data mining' offers significant advantages over conventional statistical techniques in dealing with the latter's limitations such as normality assumption of observations, independence of observations from each other, and linearity of the relationship between the observations and the output measure(s). There are statistical methods that overcome these limitations. Yet, they are computationally more expensive and do not provide fast and flexible solutions as do data mining techniques in large datasets.

Purpose: The main objective of this study is to improve the prediction of outcomes following combined heart-lung transplantation by proposing an integrated data-mining methodology.

Methods: A large and feature-rich dataset (16,604 cases with 283 variables) is used to (1) develop machine learning based predictive models and (2) extract the most important predictive factors. Then, using three different variable selection methods, namely, (i) machine learning methods driven variables-using decision trees, neural networks, logistic regression, (ii) the literature review-based expert-defined variables, and (iii) common sense-based interaction variables, a consolidated set of factors is generated and used to develop Cox regression models for heart-lung graft survival.

Results: The predictive models' performance in terms of 10-fold cross-validation accuracy rates for two multi-imputed datasets ranged from 79% to 86% for neural networks, from 78% to 86% for logistic regression, and from 71% to 79% for decision trees. The results indicate that the proposed integrated data mining methodology using Cox hazard models better predicted the graft survival with different variables than the conventional approaches commonly used in the literature. This result is validated by the comparison of the corresponding Gains charts for our proposed methodology and the literature review based Cox results, and by the comparison of Akaike information criteria (AIC) values received from each.

Conclusions: Data mining-based methodology proposed in this study reveals that there are undiscovered relationships (i.e. interactions of the existing variables) among the survival-related variables, which helps better predict the survival of the heart-lung transplants. It also brings a different set of variables into the scene to be evaluated by the domain-experts and be considered prior to the organ transplantation.","This study suggests that when modeling combined heart–lung transplantation procedures a data-mining-driven methodology should be used to augment the variable selection process rather than focusing on mere expert-selected predictor variables. The human expert's input cannot be ignored in modeling combined heart–lung transplantation (nor can be in any area of medicine) but should be (and as shown in this study, could be) strengthen with the knowledge that can be discovered from data. In order to make use of voluminous datasets, it may be useful to apply the data mining models to extract previously unknown patterns and relationships among the predictor variables. Thus, a small set of effective variables (predictors) could be identified for analysis instead of the original large number of variables, which enables more effective and efficient analyses. This study proposes that the data mining models select the significant variables as the first step. Thereafter, potential variable sets from domain experts will be integrated in the process. In the subsequent analysis, the medical experts should especially be referred to interpret the results that this methodology reveals in combined heart–lung transplantation. The medical experts are to evaluate the patterns and the newly introduced predictor variables as to their significance and if they bring new actionable and logical directions in transplant area. An example is the GINT variable which was created in this study, which is shown to be important in predicting graft survival. The medical professionals who have years of experience in the combined heart–lung transplant area will be expected to decide if it is medically important to assign an organ to a recipient who is the same gender as the donor.

Because of its ability to model highly complex data-rich phenomenon, predictive data mining is destined to become an essential instrument for researchers in medical informatics. Due to the increasingly more effective and efficient data collection and storage mechanisms in a variety of medical fields coupled with the enormity of ever more complex problems, data mining applications will continue to gain popularity. Future research efforts will involve extension of the data mining analysis for UNOS thoracic dataset along with the follow-up datasets. This perspective will hopefully open a new window to observe patients’ medical condition after the combined heart–lung transplant has been performed. A critical prognostic index can be devised, which categorizes the transplant patients in terms of various risk groups, namely low, medium, and high. Apart from that, more sophisticated statistical modeling approaches such as Bayesian model averaging will be incorporated to evaluate various data mining model outcomes simultaneously.",https://www.sciencedirect.com/science/article/pii/S1386505609000707?via%3Dihub
Petrovsky et al. 2002,Use of artificial neural networks in improving renal transplantation outcomes,"Recent advances in renal transplantation, including the matching of major histocompatibility complex or new immunosuppressants, have improved 1-year survival of cadaver
kidney grafts to more than 85%. Further optimization of kidney transplant outcomes is
necessary to enhance both the graft survival time and the quality of life. Techniques derived from the artificial intelligence enable better prediction of graft outcomes by using
donor and recipient data. The authors used an artificial neural network (ANN) to model
kidney graft rejection and trained it with data on 1542 kidney transplants. The ANN correctly predicted 85% of successful and 72% of failed transplants. Also, ANN correctly
predicted the type of rejection (hyperacute, acute, subacute, and chronic) for approximately 60% of the failed transplants. These results indicate that the ANN-based approach is useful for prediction of both the general outcomes of kidney transplants and
the prediction of the type of rejection.","The biggest problem in improving renal transplant allocation may ironically not be the development of more reliable graft survival predictions but,
rather, the gaining of acceptance of such predictions by the organ allocation bodies. These bodies
are by their nature conservative and may be resistant to the adoption of new technology, particularly
if it is seen to supplant their decision-making
process. Such bodies are likely to require prospective validation of the technology, although testing
with blinded retrospective data should be sufficient to determine the predictive power of the technology and would obviate several years of delay in
its implementation. The impartiality of an AIbased allocation system should ultimately be its
greatest strength, as it would prevent bias creeping into organ allocation. The existence of such
bias is suggested by data such as evidence that black
patients in the United States are half as likely as
white patients to be rated as suitable candidates for
transplantation when assessed by expert panels.
Even when assessed suitable for transplantation,
black patients are less likely to be referred for evaluation, placed on a waiting list, or transplanted.35
Clearly, no system that is required to balance the
conflicting needs for utility and equity is ever going
to be easy to develop and implement. Such difficulties should not stop us, however, from improving the current organ allocation system by incorporating more accurate assessment of the likely graft
outcome. The technological advances supported by
the AI methods for graft allocation will ultimately
benefit the many patients currently awaiting organ
transplants.",https://scholar.google.com/scholar_lookup?journal=+Graft&title=Use+of+artificial+neural+networks+in+improving+renal+transplantation+outcomes&author=N+Petrovsky&author=SK+Tam&author=V+Brusic&volume=5&publication_year=2002&pages=6&
Brier et al. 2003,Prediction of delayed renal allograft function using an artificial neural network,"Background. Delayed graft function (DGF) is one of the most important complications in the post-transplant period, having an adverse effect on both the immediate and long-term graft survival. In this study, an artificial neural network was used to predict the occurrence of DGF and compared with traditional logistical regression models for prediction of DGF.

Methods. A total of 304 cadaveric renal transplants performed at the Jewish Hospital, Louisville were included in the study. Covariate analysis by artificial neural networks and traditional logistical regression were done to predict the occurrence of DGF.

Results. The incidence of DGF in this study was 38%. Logistic regression analysis was more sensitive to prediction of no DGF (91 vs 70%), while the neural network was more sensitive to prediction of yes for DGF (56 vs 37%). Overall prediction accuracy for both logistic regression and the neural network was 64 and 63%, respectively. Logistic regression was 36.5% sensitive and 90.7% specific. The neural network was 63.5% sensitive and 64.8% specific. The only covariate with a P < 0.001 was the transplant of a white donor kidney to a black recipient. Cox proportional hazard regression was used to test for the negative effect of DGF on long-term graft survival. One year graft survival in patients without DGF was 92 ± 2% vs 81 ± 3% in patients with DGF. The 5-year graft survival was not affected by DGF in this study.

Conclusion. Artificial neural networks may be used for prediction of DGF in cadaveric renal transplants. This method is more sensitive but less specific than logistic regression methods.","The goal of this work was to test methods that would allow the choice of initial immunosuppression to be guided by a prediction of the risk of DGF in an individual transplant recipient. We evaluated artificial neural networks as a tool to predict DGF. Artificial neural networks are an extension of traditional statistical techniques. They are most closely related to logistic regression for prediction and discriminant analysis for classification. Neural network predictors have been shown to offer a more flexible modelling environment than any of the traditional approaches, including other statistical methods [16]. Predictions from the neural network were compared with a standard technique of logistic regression, which also allows us to evaluate the data for specific risk factors in this population for DGF. In general, both methods provided similar overall predictive accuracy. Both the neural network and logistic regression methods provided accurate predictions in ∼64% of cases evaluated. A more sensitive analysis of the data using ROC curves also reaches the same conclusion, that there is no difference between predictions.

Analysis of the complete data set allowed us to determine individual risk factors for DGF in our patient population. In the present analysis, only one factor was associated with a statistically significant risk and that is transplantation of kidneys from a white donor to black recipients. The relative risk of DGF for a black recipient receiving a kidney from a white donor was 12.64. The higher incidence of poor outcome in black recipients has been reported by other authors; Ojo et al. [6] and Feldman et al. [5] with an odds ratio of 1.63–2.17.

Other risk factors associated with DGF as reported by Boom et al. [4] include recipient pre-transplant mean arterial pressure <100, female donor to a male recipient, cold ischaemia time >28 h, peak panel-reactive antibody (PRA) >50% and donor age >50 years.

In this analysis, we did not find a relationship of cold ischaemia time and HLA mismatch with DGF. The absence of any influence of cold ischaemia and HLA mismatch may be a centre effect. The overall HLA match among our patients was good (AB match 1.8 ± 0.8; DR match 1.1 ± 0.5) and cold ischaemia times are limited (1353 ± 453 min) because the majority of the kidneys were procured locally.

Poor outcome among black recipients has been reported by various authors. Feldman et al. [5] in a series of 325 cadaveric allografts reported that 62.6% of 91 black recipients had DGF compared with 48.3% of 234 white recipients. Sanfillipo and colleagues [15] in a series of 3800 cadaveric renal transplants in the pre-cyclosporin era found black race to be a significant predictor of DGF. Cacciarelli et al. [16] in a study of 495 cadaveric allograft recipients between 1983 and 1987 found that 58% of 176 black recipients required dialysis post-transplant compared with 46% of 221 white recipients, a difference that was statistically significant.

Postulated explanations for the poorer outcome in black recipients of a cadaveric renal transplant have been a greater number of HLA mismatches, greater immune responsiveness, less bioavailability of cyclosporin, prolonged cold ischaemia time, lengthy duration of prior dialysis therapy, prior blood transfusion and a higher peak PRA [4].

Graft survival at 1 and 5 years has also been found to have a poorer outcome. A recent UNOS scientific renal transplant registry reports that blacks have the worst 1- and 5-year graft survival compared with all other ethnic groups, 79 and 48%, respectively [17]. Ojo et al. [6] in an analysis of the USRDS data reported that African Americans had a relative risk of 1.34 for a poorer 5-year graft survival obtained by a Cox non-proportional hazard regression analysis.

The increased risk of DGF in black patients that we observed cannot be readily explained. Immunological differences and cold ischaemia may be aetiological factors for DGF in black patients, but in our data set cold ischaemia and HLA match were equivalent between white and black recipients. Thus, the aetiology of increased DGF in black patients in our centre is not clear. However, our data do support the hypothesis that DGF may significantly contribute to poorer overall graft survival in black patients.

We conclude that the application of computer modelling to predict outcomes in renal transplantation is promising. In this first application of an artificial neural network to predictions in renal transplantation, a neural network provided predictions that are at least equivalent to those of logistic models which are a more traditional technique. The combination of neural networks and logistic regression offers the promise of highly sensitive and specific prediction of ARF following kidney transplantation. Also, logistic regression analysis revealed an individual factor highly correlative with DGF; the markedly increased risk of black transplant recipients for DGF. These findings indicate that the use of computers to predict DGF may be useful in guiding the selective use of anti-lymphocyte therapy, particularly in black patients who are at a markedly increased risk of DGF.",https://pubmed.ncbi.nlm.nih.gov/14605292
Yahav & Shmueli 2010,Predicting potential survival rates of kidney transplant candidates from databases with existing allocation policies,"Under the current kidney allocation system in the United States, kidneys are allocated
to patients primarily through a combination of tissue matching, sensitization level, and
waiting time. However, due to recent trends in medicine and the shortfall of kidney
supply, the current system fails to match donors and recipients well. In an effort to
improve the allocation system, the United Network of Organ Sharing (UNOS) defined
principle factors that would determine a new allocation policy. The most prominent
factor is patients’ potential remaining lifetime.
Estimating “potential remaining lifetime” is complicated for several reasons. First, the
characteristics of candidates in the waitlist are different from kidney recipients, implying
that the mortality of candidates does not represent the mortality that would have occurred
among recipients, had they not received a transplant. Second, treatment methods of
patients without a transplant have changed over the last two decades, making lifetime
predictions less certain. Lastly, the lifetime model should extrapolate future survival
beyond the duration of the data.
In this paper, we use a parametric Weibull Accelerated Failure Time (AFT) model to
predict survival rates and show its advantage over common in terms of prediction accuracy. We also use data mining methods, and in particular, classification and regression
trees, to tackle recipients’ selection bias and bias caused by changes in medical treatment.","This paper presents a model for predicting survival rates of kidney transplant candidates from databases with existing allocation policies. Our approach consists of a preprocessing procedure to impute recipients’ lifetime information and a parametric survival analysis that predicts patients’ survival rates. The highlights of this procedure are provided below:
Data imputation: Imputing recipients’ potential lifetime (had they not received transplant) in two steps:
Step 1: Imputing death incidences among recipients
Step 2: Completing recipients’ lifetime, conditional on the event of death
Survival analysis: Using a parametric Accelerated Failure Time (AFT) model to predict patients’ survival rates based on the completed dataset
In future work we plan to test additional data imputation techniques and to evaluate
them based on their direct predictive accuracy, that is, predicting lifetime and death incidences of candidates in holdout set, and their indirect predictive accuracy, or in other
words, predicting survival rates from the imputed dataset. We will compare the performance of our survival model to the weighted proportional hazards regression model,
proposed in [3].",https://scholar.google.com/scholar_lookup?journal=Predicting+potential+survival+rates+of+kidney+transplant+candidates+from+databases+with+existing+allocation+policies&author=I+Yahav&author=G+Shmueli&author=+editors&publication_year=2010&
Koyuncugil & Ozgulbas 2010,Donor research and matching system based on Data-Mining in organ transplantation,"It is very important to identify the appropriate donor in organ transplantation under the time constraint. Clearly, adequate time must be spent in appropriate donor research in that kind of vital operation. On the other hand, time is very important to search for other alternatives in case of inappropriate donor. However, the possibility for determining the most probable donors as fast as possible has an great importance in using time efficiently. From this point view, the main objective of this paper is developing a system which provides probabilistic prior information in donor transplantation via data mining. While the sytem development process, the basic element is the data of successful organ transplantations. Then, the hidden information and patterns will be discovered from this data. Therefore, this process requires the data mining methods from its definition. In this study, an appropriate donor detection system design based on data mining is suggested.
","At the beginning of this study an implementation phase with real data has been planned as an example for the clarification of the System’s accuracy. Therefore, it would be possible to show the real world results of the System. But at the application process Ministry of Health of Turkey (MOHT) was rejected to provide real data according to the Turkish Statistics Law Article 13 which is given below.

Statistics Law of Turkey (Number of the Law: 5429, Date of Approval: 10/11/2005; Published Official Gazette Date and Number: 18/11/2005 and 25997) [33].

Article 13—Confidential data can be accessed only by the ones involved in the production of official statistics, to the extent that they need for performing their duties properly. If the number of the statistical unit in any cell of the data table formed by aggregating the individual data is less than three or one or two of the statistical units are dominant even if the number of units is three or more, the data in the concerned cell is considered confidential.

The confidential data compiled, processed and preserved for the production of official statistics cannot be delivered to any administrative, judiciary or military authority or person, can not be used for purposes other than statistics or as an instrument of proof. Civil servants and other staff in charge of compiling and processing these data are obliged to comply with this rule. This obligation continues after the related personnel leave their duties and posts.

The rulers of the institutions and organisations producing official statistics shall take all measures to prevent any illicit access, use or revelation of the confidential data. Data or information obtained from sources that are open to all people shall not be deemed confidential.

Data confidentiality ceases when a statistical unit gives written approval for the revelation of confidential data concerning itself.

Confidential data can be published only as combined with other data so as not to allow any direct or indirect identification.

Principles and procedures relating to data confidentiality and security shall be regulated through statutes to be issued in line with national and international principles and by soliciting the opinion of relevant institutions and organisations.

The authors of this study then decided to ask data from research hospitals for a Pilot study according to the reviewers comments but unfortunately this applications were rejected with the same reason of the MOHT.

Policy implications
In December 2003, the government of Turkey announced a new and comprehensive reform program, titled ‘Health in Transition’. The health reform has aimed at organizing, financing and delivering the health services in an effective, productive and equal way. The major components of this program comprised the institutional strengthening of Ministry of Health, introduction of a universal health insurance, a reorganization of the health service delivery, development of the human resources in the health sector, quality assurance and accreditation of health facilities; establishment of a National Pharmaceutical and Medical Devices Agency and the development of a sound information system for the national health service [34].

Quality in health care, effectiveness, equity and patient safety are the primary goals of reform studies in health. When these objectives were considered in terms of organ transplantation and the number of patients in need of organ transplants, it was seen that Turkey needs a new system for donor research and matching. Donor Research and Matching System Based on Data Mining presented in this paper will ensure an equitable nationwide system for the distribution of transplantable organs. Beside these, this System provided some other advantages for the policy makers and health reform studies:

Data warehouse and statistical analysis infrastructure,

High quality data for strategic decision making process,

Fast, accurate and reliable reporting base for health system,

Evidence based medical processes,

Measurable, objective, comparable and transparent results for higher patients safety,

Ability to perform performance measures and benchmarking for the evaluation of transplantation centers by using System’s data.

Therefore, this system should take into consideration in the manner of Organ Tranplantation component of the information system for Health Transition program.

In addition, Turkish Statistics Law Article 13 or its implementation process should be changed for scientifical studies. Especially in health studies personal data always becomes the vital base. Therefore, restrictions on using personal data should be ignored at least in health studies.",https://pubmed.ncbi.nlm.nih.gov/20503609
Ram et al. 2023,CT-based Machine Learning for Donor Lung Screening Prior to Transplantation,"Background:
Assessment and selection
Assessment and selection of donor lungs remains largely subjective and experience based. Criteria to accept or decline lungs are poorly standardized and are not compliant with the current donor pool. Using ex vivo CT images, we investigated the use of a CT-based machine learning algorithm for screening donor lungs prior to transplantation.

Methods:
Clinical measures and ex-situ CT scans were collected from 100 cases as part of a prospective clinical trial. Following procurement, donor lungs were inflated, placed on ice according to routine clinical practice, and imaged using a clinical CT scanner prior to transplantation while stored in the icebox. We trained and tested a supervised machine learning method called dictionary learning, which uses CT scans and learns specific image patterns and features pertaining to each class for a classification task. The results were evaluated with donor and recipient clinical measures.

Results:
Of the 100 lung pairs donated, 70 were considered acceptable for transplantation (based on standard clinical assessment) prior to CT screening and were consequently implanted. The remaining 30 pairs were screened but not transplanted. Our machine learning algorithm was able to detect pulmonary abnormalities on the CT scans. Among the patients who received donor lungs, our algorithm identified recipients who had extended stays in the ICU and were at 19 times higher risk of developing CLAD within 2 years post-transplant.

Conclusions:
We have created a strategy to ex vivo screen donor lungs using a CT-based machine learning algorithm. As the use of suboptimal donor lungs rises, it is important to have in place objective techniques that will assist physicians in accurately screening donor lungs to identify recipients most at risk of post-transplant complications.","Lung transplantation is presently the only viable cure for end-stage lung diseases such as COPD (Chronic Obstructive Pulmonary Disease) and IPF (Idiopathic Pulmonary Fibrosis). In this proof-of-concept study, we demonstrated a strategy to screen donor lungs ex-situ using computed tomography and machine learning. By leveraging the high resolution and air-tissue contrast of CT and enhanced feature-based detection of a machine learning algorithm, we demonstrated the benefits of this unique strategy for lung screening. In our single center study, we found that our method predicted ICU stay and the odds of a PGD score of 3 in transplant recipients. Our results suggest that this CT-ML strategy, which on average takes only 5 minutes, may serve as a complementary step in the screening process of donor lungs for transplantation.

It is important to note that while CT is not the only tool that can assist with transplantation decisions, it has potential as an accessible, valuable method for selecting viable donor lungs. Donor history, blood gases of the pulmonary veins and in situ inspection remain critical factors in clinical decision making; however, in cases where there is uncertainty about the quality of a donor lung, CT scans may reveal insights that facilitate this process.24,25 To the best of our knowledge, this is the first study to evaluate the use of CT in conjunction with machine learning to assess donor lungs used for transplantation. This provided a unique opportunity to test the potential of our approach for predicting post-transplant outcomes. In our previous work, we obtained CT scans from declined donor lungs and found that CT examination of these specimens by a trained thoracic radiologist provided detailed information of interstitial changes otherwise obscured during routine donor lung assessment.12,13 However, manual screening of CT scans is hampered by interobserver variability, as well as delays due to accessibility to radiologists. Importantly, time constraints must be minimized to effectively incorporate our strategy of applying CT scanning to donor lung screening. For the present study, we therefore developed a fully automated process to screen CT scans of donor lungs.

An important attribute of our ML screening method is its ability to focus exclusively on the features presented in CT scans without requiring additional information such as donor or recipient characteristics or clinical data. Due to the novelty of our method, i.e., using clinical CT scans to screen donor lungs, our data came only from this single center study. While our dictionary learning model was trained only on 14 cases (7 accepted and 7 declined), it still provided associations with clinically meaningful measures. In fact, our model predicted ICU stay in lung transplant recipients (Figure 3). Further, we observed significant differences in hospital stay between transplant recipients with donor lungs classified as “accepted” and “declined” (p = 0.039; Table 2). PGD scores are used in the early post-lung transplant period (immediately post-transplant to 72 hours post-transplant) to predict early outcomes. Through our strategy of screening donor lungs using CT and ML, we not only demonstrated that recipients who received a “ML Declined” donor lung, as classified by our approach, were 5.25 times more likely to generate a PGD score of 3 but were 19.12 times more likely to develop CLAD in 2 years. It is important to reiterate that no prior knowledge of the donor or recipient, other than the ex-situ CT scan, was used to train our ML model. It is also important to note that the training set, whether accepted or declined, consists primarily of healthy lung tissue. To account for this bias, we developed our algorithm to detect and remove redundancies between dictionaries, such that patches in class 1 comprise of normal lung and class 2 abnormal lung. We identified one case in our training set declined due to logistics, though it was a healthy lung. However, this would not affect our ML algorithm as it would automatically associate normal patches with class 1 irrespective of the case delineation.",https://pubmed.ncbi.nlm.nih.gov/37034670/
Shaikhina et al. 2019,Decision tree and random forest models for outcome prediction in antibody incompatible kidney transplantation,"Clinical datasets are commonly limited in size, thus restraining applications of Machine Learning (ML) techniques for predictive modelling in clinical research and organ transplantation. We explored the potential of Decision Tree (DT) and Random Forest (RF) classification models, in the context of small dataset of 80 samples, for outcome prediction in high-risk kidney transplantation. The DT and RF models identified the key risk factors associated with acute rejection: the levels of the donor specific IgG antibodies, the levels of IgG4 subclass and the number of human leucocyte antigen mismatches between the donor and recipient. Furthermore, the DT model determined dangerous levels of donor-specific IgG subclass antibodies, thus demonstrating the potential of discovering new properties in the data when traditional statistical tools are unable to capture them. The DT and RF classifiers developed in this work predicted early transplant rejection with accuracy of 85%, thus offering an accurate decision support tool for doctors tasked with predicting outcomes of kidney transplantation in advance of the clinical intervention.","The test classification accuracy of 85% achieved by the models demonstrates that the ML approach can be effectively applied to predictive modelling in renal transplantation despite the small number of observations and heterogeneous input parameters. Based on only 80 cases, our DT model achieved a similarly high level of performance for acute ABMR as the model of Krikov et al. [9] for kidney allograft survival, which was built on a national database of 92,844 patient records. The proposed models outperform in their accuracy (AUC of 0.819 for RF and 0.854 for DT) some of the highest-performing models in the area of kidney transplantation discussed in Section 1 [13], [14], [16], [17].

Our DT model, chosen from the group of DT models with the highest prediction power, was able to successfully determine the optimal set of parameters associated with early rejection. The 6 key predictors identified by the DT are confirmed by previously developed logistic regression likelihood multivariate model [23], which is a tool of choice in medical statistics for binary classification [24], [33], [34]. The superiority of the DT model is that it was also able to determine the level of antibodies associated with ABMR, which conventional statistical methods were unable to provide. It is important to note that it has been intuitively known by transplant doctors that harmful highest IgG antibody levels were at around 1000 MFI [35], [36], [37] which our model confirmed to be at 1062 MFI (Fig. 1). Additionally, the harmful levels of IgG4 were identified to be at 80 MFI (Fig. 1) addressing the clinical aim as set in the Introduction.

The RF model provides an extension to the DT model with the purpose of improving the robustness of the classification tool. A RF is so-called black-box model and less interpretable than DT, but allows for better consistency of results and robustness of predictions. Our DT and RF are equally well-equipped to handle partially missing data and managed to classify correctly the 3 cases with incomplete data.

Tree-based models can be implemented in the electronic decision support system by means of standard computational resources. When used for clinical decision support, our models can provide a simulation tool to explore various clinical scenarios and identify patients at risk of ABMR prior to the operation, and thus leaving more time to make essential adjustment to treatment.

It is important to state that outcomes from this single-centre study may not generalise on a larger population in and outside of the United Kingdom. They may be affected by institutional bias, and therefore a further work comparing the results on extended datasets from other centres would be beneficial. Despite this limitation, the achieved outcomes remain significant to the area of kidney transplantation.

To our best knowledge, this study is the only work aimed at developing ML models for prediction of acute ABMR based on specific MFI levels of IgG subclasses. Predictions made by our DT and RF models are patient-specific yielding an accurate and robust tool for ABMR risk stratification preceding transplantation.",https://www.sciencedirect.com/science/article/pii/S1746809417300204
Lau et al. 2017,Machine-Learning Algorithms Predict Graft Failure After Liver Transplantation,"Background: The ability to predict graft failure or primary nonfunction at liver transplant decision time assists utilization of scarce resource of donor livers, while ensuring that patients who are urgently requiring a liver transplant are prioritized. An index that is derived to predict graft failure using donor and recipient factors, based on local data sets, will be more beneficial in the Australian context.

Methods: Liver transplant data from the Austin Hospital, Melbourne, Australia, from 2010 to 2013 has been included in the study. The top 15 donor, recipient, and transplant factors influencing the outcome of graft failure within 30 days were selected using a machine learning methodology. An algorithm predicting the outcome of interest was developed using those factors.

Results: Donor Risk Index predicts the outcome with an area under the receiver operating characteristic curve (AUC-ROC) value of 0.680 (95% confidence interval [CI], 0.669-0.690). The combination of the factors used in Donor Risk Index with the model for end-stage liver disease score yields an AUC-ROC of 0.764 (95% CI, 0.756-0.771), whereas survival outcomes after liver transplantation score obtains an AUC-ROC of 0.638 (95% CI, 0.632-0.645). The top 15 donor and recipient characteristics within random forests results in an AUC-ROC of 0.818 (95% CI, 0.812-0.824).

Conclusions: Using donor, transplant, and recipient characteristics known at the decision time of a transplant, high accuracy in matching donors and recipients can be achieved, potentially providing assistance with clinical decision making.","This study is a proof-of-concept that machine-learning algorithms can be an invaluable tool, supporting the decision-making process for liver transplant organ allocation. This is particularly relevant in the current high-stakes environment where suboptimal organ utility leads to either increased waiting list mortality or patient mortality after transplantation.

The results of this study revealed that using 15 of the top-ranking donor and recipient variables available before transplantation were the best predictors of outcome with an average AUC-ROC of 0.818 with the random forest algorithm and 0.835 with artificial neural networks. Both machine learning techniques showed significant improvements in AUC-ROC with characteristic selection. This was followed by training the random forest classifier with the variables used to calculate DRI plus MELD score (AUC-ROC, 0.764). Using the random forest classifier with the factors used to calculate DRI improved the discrimination of DRI from 0.680 to 0.697. SOFT score achieved an average AUC-ROC of 0.638. Assessing the predictive accuracy of the final models with top 15 factors, as trained for 30 day outcome, for graft failure at 3 months, the AUC-ROC value decreased from 0.818 to 0.715 with random forests and 0.835 to 0.559 with neural networks. By comparison, DRI prediction of 3 month graft failure was 0.595.

There are many machine-learning paradigms, of which 2 of the most widely used are artificial neural networks and random forest classifiers. In a recent landmark article where the performance of 179 different machine-learning classifiers were used to classify all 121 data sets, representing the entire University of California Irvine Machine Learning Repository, random forest classifiers were found to be the most accurate.27 There are 4 reports using artificial neural networks to predict transplant outcome in literature.28–31 The present study is the first report using a random forest machine-learning algorithm for predicting outcome after liver transplantation.

There are multiple theoretical advantages with the use of random forest algorithms in this application. It is well known in machine learning literature that artificial neural networks are prone to overfitting and learning noise in data, resulting in unstable models with poor generalization ability.32–35 However, by design, random forest classifiers are less prone to overfitting producing more stable models.36–38 In medical data sets, there is frequently a large degree of missing data because the data are often not collected for research purposes, and some tests are not routinely performed even though they may be highly prognostic (eg, donor liver biopsy for assessment of steatosis). Simply excluding these cases may bias the results due to the fact that the “missingness” of the data is not completely at random.39,40 Random forest algorithms are superior in handling data sets missing a significant proportion of input data such as with this study.41 Furthermore, although artificial neural networks are essentially, a “black-box” into which data are inputted and a prediction is outputted, the characteristic importance measure with random forest can indicate the importance of each variable in the data set thereby improving the transparency of the algorithm.38,41,42

Myriad factors interact to influence liver transplant including donor, recipient, and locally specific transplant factors. There have been many attempts to predict graft failure, after liver transplant in literature.7,8,43–48 Some studies looked at predicting graft failure using either donor factors, recipient factors,43 or a combination of both.7,8,45–48 However, these approaches have all failed to gain greater adaptability because they are developed from patient populations which may not be generalizable to other centers due to regional differences in patient, donor or process factors, or changes in practice since their development.5,6 Furthermore, they are calculated from simple multiple regression statistical models which assumes the linear influence of different variables. A predictive model required to enable effective organ allocation needs to be locally and temporally applicable, and account for the complex interactions within the data available before transplantation.

Currently, decisions for organ allocation are largely subjective or based on a recipient “sickest-first” or “waiting-time” approach rather than an outcome-based approach. Machine-learning algorithms are increasingly used for modern clinical decision-making. Compared with current methods, they are data driven, able to accommodate numerous interdependent variables, and specific to the population from which they were trained on. In addition, compared with static indices, they are dynamic, able to “learn” case-by-case with the expansion of the training set.

Using characteristic importance measure, the most influential donor and recipient variables were determined. Most of these factors, such as donor age, whether the offer is after brain death or cardiac death, donor cause of death, donor hospital state (geographical distance), donor alcohol consumption, recipient disease category, and medical status at activation are already known as important factors.4,45,49,50 Donor Hb, protein level and insulin usage were also top-ranking predictive characteristics which make sense clinically. Donor CMV and recipient herpes simplex virus status were also predictive, and although less intuitive, has been shown to be associated with acute viral infection and rejection.51,52 Interestingly, the decision to retrieve the pancreas for islet cell or whole organ transplant was also a top-ranking factor, although the decisions to retrieve kidneys, lungs, or heart were not significant factors. This is likely because the decision for pancreas retrieval is usually more stringent, requiring more ideal donor conditions.

This study highlights the importance of characteristic selection and tailoring in predictive modeling. The predictive accuracy of the well-known DRI was improved when tailored to the specific influences at the Austin Health Liver Transplant Unit. Accuracy was further improved with the addition of recipient MELD characteristic with the best accuracy found with the application of a unit-specific Random Forest algorithm using the top-ranking predictive factors.

The main limitations of machine-learning algorithms are that they are best suited to predicting outcome in the environment from which they are derived. Conversely, this limitation is also its strength, in that it is highly specific to the peculiarities of a particular transplant centre, enabling the best decision for each individual transplant. Therefore, although it is not ideal to export a trained algorithm from 1 transplant center to the next, certainly, the approach, with an algorithm tailored to each transplant center is possible. A further limitation of this algorithm is that although it is trained to predict 30-day graft failure, its predictive accuracy may not extend to other important liver transplant outcomes, such as 3-, 6-, or 12-month graft failure, early graft dysfunction, acute/chronic rejection, infections, immunosuppression, or late biliary strictures. Each of these outcomes might require a separately trained algorithm.

A limitation of this study is that the machine-learning algorithm was derived from an observational database. While the bootstrapping with replacement methodology is well validated for the development of robust predictive machine-learning models,53,54 and our attempts to predict a 3-month graft failure for a separate validation data set looks promising, prospective validation for 30-day graft failure would be valuable to confirm the predictive ability.

This study confirms that machine-learning algorithms based on donor and recipient variables which are known before organ allocation can be utilized to predict transplant outcomes. This approach may be used as a tool for transplant surgeons to improve organ allocation decisions. The ability to quantify risk may allow for improved confidence with the use of marginal organs and better outcome after transplantation.",https://pubmed.ncbi.nlm.nih.gov/27941428/
Abdeltawab et al. 2019,A novel CNN-based CAD system for early assessment of transplanted kidney dysfunction,"This paper introduces a deep-learning based computer-aided diagnostic (CAD) system for the early detection of acute renal transplant rejection. For noninvasive detection of kidney rejection at an early stage, the proposed CAD system is based on the fusion of both imaging markers and clinical biomarkers. The former are derived from diffusion-weighted magnetic resonance imaging (DW-MRI) by estimating the apparent diffusion coefficients (ADC) representing the perfusion of the blood and the diffusion of the water inside the transplanted kidney. The clinical biomarkers, namely: creatinine clearance (CrCl) and serum plasma creatinine (SPCr), are integrated into the proposed CAD system as kidney functionality indexes to enhance its diagnostic performance. The ADC maps are estimated for a user-defined region of interest (ROI) that encompasses the whole kidney. The estimated ADCs are fused with the clinical biomarkers and the fused data is then used as an input to train and test a convolutional neural network (CNN) based classifier. The CAD system is tested on DW-MRI scans collected from 56 subjects from geographically diverse populations and different scanner types/image collection protocols. The overall accuracy of the proposed system is 92.9% with 93.3% sensitivity and 92.3% specificity in distinguishing non-rejected kidney transplants from rejected ones. These results demonstrate the potential of the proposed system for a reliable non-invasive diagnosis of renal transplant status for any DW-MRI scans, regardless of the geographical differences and/or imaging protocol.","Early detection of AR can help physicians with early intervention with appropriate treatment and thus prolong the renal graft function and improve patient outcomes. Generally, there are multiple types of AR, and the selection of the appropriate treatment depends on the rejection type. For example, acute cellular rejection is treated with a high dose of corticosteroids, administrated intravenously as the first line treatment43,44. The most popular regimen is the administration of methylprednisolone for three successive days43. In the case of persistent kidney deficiency with the steroid and/or antithymocyte globulin or the presence of a new defect in renal function after treatment of AR, another biopsy is recommended to discover additional causes of renal dysfunction. T-cell depleting antibodies are suggested for aggressive vascular cellular rejection and AR episodes that do not respond to steroid treatments45. On the other hand, if antibody mediated rejection is the resulting diagnosis, the following alternatives are suggested for treatment: plasmapheresis, immunoadsorption, intravenous immunoglobulin, or monoclonal antibodies46.

It is worth mentioning that most of the clinical research estimates the ADC at a few select b-values17–27, typically one of the lower b-values and one of the higher b-values along with the baseline (b0). In fact, blood flow and complex tissue microstructure create non-monoexponential DW-MRI signal attenuation. The maximal b-value in this study (1000 s mm−2) should not elicit excessive non-Gaussian diffusion. One usually goes to b-value > 2000 s mm−2 for sensitivity to kurtosis, etc. That said, perfusion is measurable at low b-values < 200 s mm−2 47,48. Usually, the low b-values account for blood perfusion, and the high b-values account for the water diffusion47–50. To account for both, we used the 11 different b-values to accurately find the differences in perfusion and diffusion patterns between the non-rejection and acute rejection groups. Additionally, we integrated the decision from the 11 different b-values to get an accurate final decision. It is worth noting that this integration helped with handling any errors that might occur in one or two b-values due to chemical shifts or existing artifacts during the acquisition process.

To summarize, a deep learning-based CAD system for non-invasive assessment of renal transplant status using the fusion of the DW-MR derived markers and clinical biomarkers was developed. Specifically, this fusion process produced well-separated CNN ADC input maps that were used as transplant status discriminatory features, which in turn affected the individual as well as the global diagnostic accuracy of the proposed CAD system. These preliminary results demonstrated the potential of the CAD system as a reliable non-invasive renal transplant diagnostic tool. It is independent of the scanner type and/or imaging protocol that has been used in DW-MRI data collection and the geographical area where the data were collected. The developed technique has also proven that the processing time and complexity can be reduced by avoiding the complex segmentation and registration procedures without affecting the quality of the final diagnosis.

Currently, we are including both lower and higher b-values without sacrificing any of the 11 different b-values to help us gather all possible information that might, one day, lead to the development of an accurate non-invasive alternative diagnostic tool to the renal biopsy. However, we are planning to extend our study by performing a statistical analysis to determine the most informative b-values. Once we reach this point, we could sacrifice some of the b-values that are unsatisfactory informative, which in turn will reduce the DW-MRI acquisition time. Future progress includes the usage of a larger sample size collected at different transplant centers and/or different imaging systems and collection protocols, and the exploration of additional biomarkers, such as genomic information that will augment personalized data in the cohort.",https://pubmed.ncbi.nlm.nih.gov/30976081/
Parkes et al. 2019,An integrated molecular diagnostic report for heart transplant biopsies using an ensemble of diagnostic algorithms,"BACKGROUND
We previously reported
We previously reported a microarray-based diagnostic system for heart transplant endomyocardial biopsies (EMBs), using either 3-archetype (3AA) or 4-archetype (4AA) unsupervised algorithms to estimate rejection. In the present study we examined the stability of machine-learning algorithms in new biopsies, compared 3AA vs 4AA algorithms, assessed supervised binary classifiers trained on histologic or molecular diagnoses, created a report combining many scores into an ensemble of estimates, and examined possible automated sign-outs.

METHODS
We studied 889 EMBs from 454 transplant recipients at 8 centers: the initial cohort (N = 331) and a new cohort (N = 558). Published 3AA algorithms derived in Cohort 331 were tested in Cohort 558, the 3AA and 4AA models were compared, and supervised binary classifiers were created.

RESULTS
Algorithms derived in Cohort 331 performed similarly in new biopsies despite differences in case mix. In the combined cohort, the 4AA model, including a parenchymal injury score, retained correlations with histologic rejection and DSA similar to the 3AA model. Supervised molecular classifiers predicted molecular rejection (areas under the curve [AUCs] >0.87) better than histologic rejection (AUCs <0.78), even when trained on histology diagnoses. A report incorporating many AA and binary classifier scores interpreted by 1 expert showed highly significant agreement with histology (p < 0.001), but with many discrepancies, as expected from the known noise in histology. An automated random forest score closely predicted expert diagnoses, confirming potential for automated signouts.

CONCLUSIONS
Molecular algorithms are stable in new populations and can be assembled into an ensemble that combines many supervised and unsupervised estimates of the molecular disease states.","Having found value in molecular EMB assessment (especially after incorporating injury estimates),13 in the present study we aimed to develop an ensemble of estimates into a biopsy report and examine automated sign-out. We established that machine-learning algorithms trained in one cohort perform similarly in future biopsies despite differences in case mix. We showed that Model 2 incorporating injury demonstrated similar relationships to histologic rejection as Model 1 and correlated with DSA status. We then developed supervised binary classifiers to complement the unsupervised Model 1 and 2 scores. We found that binary molecular classifiers trained on histology diagnoses predicted molecular diagnoses better than histology diagnoses. As expected, the classifiers trained on molecular diagnoses gave better predictions of molecular diagnoses than those trained on histology diagnoses. Indeed, no molecular score predicted histology diagnoses with an AUC >0.78, which was expected given the limits imposed by interobserver disagreements in histology diagnoses reported in the Cardiac Allograft Rejection Gene Expression Observational Study II (CARGO II).1 We assembled a molecular report signed out by an expert, and found significant agreement with histology, but also extensive discrepancies. An automated random forest estimate accurately predicted MMDx expert sign-out of molecular rejection. We conclude that an ensemble of supervised and unsupervised molecular estimates of rejection can form a stable reporting system that avoids undue reliance on single methods, and that combining all scores into an ensemble using random forest assessment makes automated sign-outs feasible for central reporting of EMBs.

The results present a picture of the distribution of rejection phenotypes in the prevalent heart transplant population. In both indication and protocol biopsies, MMDx diagnosed ABMR and no rejection more frequently than histology. Histology diagnosed pTCMR much more frequently than MMDx, but these histology TCMR biopsies often bore little resemblance to TCMR in molecular terms. The molecular results suggest that histologic pTCMR (i.e., Grade 1R) in heart transplantation is seldom true TCMR, as pathologists and clinicians have long suspected. MMDx shows some positive TCMR and ABMR in biopsies where histology is negative, and vice versa. Such discrepancies are the subject of ongoing and detailed analyses in the extension of the INTERHEART study.

Although no diagnostic system is perfect, we consider molecular assessment a more accurate reflection of the true disease state than histology. Molecular assessment utilizes precise measurements of features expressed as continuous numbers, not semi-quantitative or binary scores like histology, and has high technical and biologic reproducibility.33 MMDx machine-learning algorithms are purely data-driven rather than derived through a consensus of opinions. Molecules outperform histology when predicting phenotypes with accurate “gold standards,” such as survival,18, 34 and simulations show that molecular classifiers can outperform flawed gold standards over a wide range of sample labeling errors. Thus, classifiers trained on molecules associated with histology are probably more accurate than the histologic diagnosis because machine learning can overcome noise in histology. MMDx analysis has provided evidence for updating the Banff guidelines for kidney transplants with C4d-negative ABMR.15, 16 The biologic relevance of the gene expression measured by microarrays can often be established.23

The observation that binary molecular classifiers, whether trained on histology diagnoses or molecular diagnoses, could predict molecular diagnosis but could not predict histologic diagnoses with AUCs >0.78 probably reflects the limitations posed by the known noise in histology, including some misclassification caused by injury-induced inflammation masquerading as rejection.13 This is lower than the agreement between MMDx and histology in kidney, as expected given that CARGO II reported lower agreement between pathologists assessing EMBs (Cohen's kappa = 0.28)1 than is usually seen with kidney transplant biopsies,17 probably because EMBs are more challenging to assess histologically than kidney core biopsies. Nevertheless, all machine-learning molecular estimates, including unsupervised archetype scores and supervised binary classifiers trained on histology or MMDx diagnoses, were significantly associated with histology diagnoses, confirming that there is substantial truth in histology.

Caution is warranted in benchmarking the performance of a new molecular system against a standard with known problems such as histology. Seeking maximum agreement between molecular tests and a flawed conventional test would generate a molecular system that retains the errors of the conventional test. The MMDx phenotype is real and reproducible, and analysis of discrepancies with histology will improve understanding. We propose that the MMDx system and the histology system can develop reciprocally, each offering a platform to help develop the other. With this in mind, a detailed analysis of the MMDx‒histology discrepancies is in progress, with the goal of eventually improving both systems. One limitation of this study is that the clinical implications need to be further explored, and will be addressed in the ongoing extension of the INTERHEART study.",https://www.sciencedirect.com/science/article/pii/S1053249819313488
Costa et al. 2020,The impact of deceased donor maintenance on delayed kidney allograft function: A machine learning analysis,"Background: This study evaluated the risk factors for delayed graft function (DGF) in a country where its incidence is high, detailing donor maintenance-related (DMR) variables and using machine learning (ML) methods beyond the traditional regression-based models.

Methods:
Methods: A total of 443 brain dead deceased donor kidney transplants (KT) from two Brazilian centers were retrospectively analyzed and the following DMR were evaluated using predictive modeling: arterial blood gas pH, serum sodium, blood glucose, urine output, mean arterial pressure, vasopressors use, and reversed cardiac arrest.

Results: Most patients (95.7%) received kidneys from standard criteria donors. The incidence of DGF was 53%. In multivariable logistic regression analysis, DMR variables did not impact on DGF occurrence. In post-hoc analysis including only KT with cold ischemia time<21h (n = 220), urine output in 24h prior to recovery surgery (OR = 0.639, 95%CI 0.444-0.919) and serum sodium (OR = 1.030, 95%CI 1.052-1.379) were risk factors for DGF. Using elastic net regularized regression model and ML analysis (decision tree, neural network and support vector machine), urine output and other DMR variables emerged as DGF predictors: mean arterial pressure, ≥ 1 or high dose vasopressors and blood glucose.

Conclusions: Some DMR variables were associated with DGF, suggesting a potential impact of variables reflecting poor clinical and hemodynamic status on the incidence of DGF.","This study suggest that poor donor clinical and hemodynamic status may impact on DGF occurrence, and this might explain the high incidence of DGF in Brazil, where the incidence is significantly higher than that the predicted by available formulas.

In our cohort, DGF incidence was almost 3-fold higher than the predicted by the nomogram described by Irish et al, suggesting an important role of variables not included in the prediction model. In fact, none of the available predictive models includes in final formula variables reflecting donor maintenance. Except for terminal serum creatinine (that could reflect renal consequences of hypovolemia, shock and other causes of acute kidney injury), only the score developed by Chapal et al analyzed some variable related to donor care (type of vasopressor) [24].

Only 4.3% of patients received kidneys from expanded criteria donors and 97.7% had KDPI below 85%, suggesting good structural quality kidneys. On the other hand, 12.2% had a reversed cardiac arrest episode before organ recovery surgery, 47.4% presented acid-base disorders, 61.6% had hypo- or hypernatremia, and 68.6% showed inadequate glycemic control, reflecting poor clinical and hemodynamic conditions. The impact of poor donor maintenance in our country has previously suggested in a study including simultaneous pancreas-kidney transplants. In this cohort, despite favorable demographics, the incidence of delayed kidney graft function was 22.7% and donor hypernatremia was an independent risk factor for DGF [7].

The challenge of properly maintaining potential deceased donors seems not to be exclusive of our population. Previous American and Canadian studies have reported low adherence to donor-care bundles at the time of consent for donation. Importantly, the early achievement of donor management goals was associated with a reduced risk of DGF in these studies [11, 12].

The high CIT was notable and its contribution to DGF is unequivocal. All statistical analysis demonstrated that each additional hour matters, even in KT with CIT < 21h. The large territorial extension, the allocation model predominantly based on HLA compatibility, and the absence of specific allocation policies for “marginal” donors contribute to the long CIT in our country [25]. However, it is noteworthy that almost half of KT with CIT <21h presented DGF, suggesting the contribution of other factors beyond the CIT.

Due to the high negative impact of CIT on DGF incidence, potentially masking other predictors, we performed a post-hoc analysis including a regression model on a sub-sample of patients with CIT inferior to 21h. As hypothesized, variables reflecting donor maintenance now emerged. Additionally, in ML analysis, donor maintenance related variables, such as blood pressure, use of high dose vasopressors, urine output and blood glucose, were also associated with DGF. To further explore the contribution of other than the traditional variables to DGF incidence using a more interpretable model, we performed a sensitivity analysis using elastic net regression. Again, beyond the traditional conditions associated to DGF occurrence, variables reflecting donor management were risk factors (serum Na+, blood glucose) or protective (high dose vasopressors and diuresis).

Recently published studies demonstrated the impact of donor hemodynamics as a predictor of DGF in transplantation from donors after cardiac death [26, 27]. However, evidences are scarce on transplantation of brain dead donors.

Our study has limitations that should be pointed out. First, it was a retrospective cohort; therefore, the capture of variables was limited to those available on medical records. We cannot assure that our results are generalizable to other transplant centers worldwide. Donor maintenance related variables (and DMGs) are not static and we could not evaluate them dynamically. Since we could not follow variables over time and assess adherence to care bundles, it is possible that some clinical and hemodynamic data reflected the severity of the disease that led to brain death and not the patient care. Besides, it’s possible that some clinical and hemodynamic parameters reflected patient situation before they became a consented donor, and thus may not reflect donor maintenance, but general intensive care. Finally, variables reflecting perioperative care were not available.

In conclusion, DGF incidence in Brazil is significantly higher than that predicted by available models. Although our data do not allow us to draw definitive conclusions, this study suggests that donor illness severity and hemodynamic instability might contribute to this scenario. Prospective studies are needed to robustly conclude how donor management impacts on DGF incidence. Additionally, a cohort including machine-perfused grafts may be useful to explore if pumping might mitigate kidney damage secondary to poor donor clinical and hemodynamic status.

We believe bringing this issue up is crucial in our setting. Scant donor care is probably a reflection of the poor economic conditions of our country. However, some educational actions could be taken, focusing on early recognition of potential donors and training staff who care for brain death patients.",https://pubmed.ncbi.nlm.nih.gov/32027717/
Tanaka et al. 2018,Decision tree analysis to stratify risk of de novo non-melanoma skin cancer following liver transplantation,"Purpose
Non-melanoma skin cancer (NMSC) is the most common de novo malignancy in liver transplant (LT) recipients; it behaves more aggressively and it increases mortality. We used decision tree analysis to develop a tool to stratify and quantify risk of NMSC in LT recipients.

Methods
We performed Cox regression analysis to identify which predictive variables to enter into the decision tree analysis. Data were from the Organ Procurement Transplant Network (OPTN) STAR files of September 2016 (n = 102984).

Results
NMSC developed in 4556 of the 105984 recipients, a mean of 5.6 years after transplant. The 5/10/20-year rates of NMSC were 2.9/6.3/13.5%, respectively. Cox regression identified male gender, Caucasian race, age, body mass index (BMI) at LT, and sirolimus use as key predictive or protective factors for NMSC. These factors were entered into a decision tree analysis. The final tree stratified non-Caucasians as low risk (0.8%), and Caucasian males > 47 years, BMI < 40 who did not receive sirolimus, as high risk (7.3% cumulative incidence of NMSC). The predictions in the derivation set were almost identical to those in the validation set (r2 = 0.971, p < 0.0001). Cumulative incidence of NMSC in low, moderate and high risk groups at 5/10/20 year was 0.5/1.2/3.3, 2.1/4.8/11.7 and 5.6/11.6/23.1% (p < 0.0001).

Conclusions
The decision tree model accurately stratifies the risk of developing NMSC in the long-term after LT.","De novo malignancy is one of the most common causes of death after liver transplantation (Watt et al. 2009, 2010). The risk of malignancy is two to four times higher in transplant recipients than in age- and sex-matched control subjects (Lanzino et al. 1997; Kaneko et al. 2013). This may be due to impaired immune surveillance in those on immunosuppression (Swann and Smyth 2007). De novo NMSC is one of the most common malignancies following LT, with an overall incidence of 16–22.5% (Unlu et al. 2015; Herrero et al. 2005). Our data confirms the high cumulative incidence of NMSC which was 13.5% after a mean of 20-years post-transplant. Our data also illustrate the magnitude of this increased risk: there were 1285 NMSC per 100,000 person-years in our liver transplant cohort, versus an expected rate of 38 per 100,000 person-years in the general US adult population (Tejera-Vaquerizo et al. 2016). This high rate is important because patients with NMSC could have worse post-transplant survival than those without NMSC (Herrero et al. 2005; Aberg et al. 2008), especially in case of SCC under immunosuppressive agents (Mithoefer et al. 2002).The incidence of BCC was four times higher than the incidence of SCC amongst the general population in the United States (Christenson et al. 2005), but we observed more cases with SCC than BCC in the dataset. There were several other studies investigating post-SOT (including LT) cohort observed the reverse ratio of BCC to SCC (Mithoefer et al. 2002; Hartevelt et al. 1990; Euvrard et al. 1995; Krynitz et al. 2013), which might be related to the pattern of exposure to ultraviolet radiation with more strict instruction for sunscreen, or again the immunosuppressive treatment in transplant recipients (Naldi et al. 2000).

There are several known risk factors for developing NMSC in liver transplant recipients, including older age, male gender, fair-skin, prior sun exposure, actinic keratosis, smoking, history of autoimmune hepatitis, human papilloma virus infection, and excessive alcohol use (Mithoefer et al. 2002; Bellamy et al. 2001; McCaughan and Vajdic 2013). Garrett et al. (2017) recently reported that increased age, white race, male sex, and thoracic organ transplantation increased the risk of skin cancer post-solid organ transplantation (SOT), and suggested that this information could be used to inform risk stratification and screening guidelines for post-transplant skin cancers. Lowenstein et al. also reviewed prediction tools for NMSC post-kidney transplantation (Lowenstein et al. 2017). However, clinical providers cannot quantify the absolute risk based on the presence or absence of these risk factors alone or in combination.

Decision tree analysis is a method to classify individuals into homogeneous subgroups; it generates a transparent algorithm in the form of a tree-structure that is intuitive and easily used clinically (Many other forms of artificial intelligence generate opaque black box algorithms that cannot be applied clinically) (Leiter et al. 2004; Kurosaki et al. 2012; Garzotto et al. 2005; Valera et al. 2007). We used the independent risk factors that we had initially identified by Cox Regression analysis, to enter in to the Decision tree analysis. These factors were: Caucasian men, age at transplant, BMI, and not receiving sirolimus after transplant. The decision tree analysis ranked the risk factors and found optimal cutoffs to provide the greatest discrimination among subsets. The optimum age was > 47, and the optimum BMI was < 40. The highest risk group had a 7.4% cumulative incidence of NMSC and the lowest risk subgroup had a cumulative incidence of only 0.8%. Many researchers have put a lot of efforts to formulate regression models for prediction of NMSC post-transplant (Lowenstein et al. 2017). These prediction models are useful for identifying high-risk patients but are somewhat complicated to use at the bedside because they require calculations to be performed. Our prediction model is used simply by incorporating basic patients’ data into the decision tree and following the flowchart. These prediction models based on factors easily accessible in routine clinical settings help physicians identify the high-, intermediate- or low-risk individuals and use this stratification to modify surveillance, care and immune suppression, and educate the individual about their specific risk, even since pre-LT.

Sirolimus use was shown to protect patients from getting NMSC in our study to some extent (but not from getting SCC, presumably due to lack of decent sample size in that sub-analysis). This finding is in keeping with many studies showing reduced risk of de novo malignancy in recipients that use the mammalian target of rapamycin (mTOR) instead of a calcineurin inhibitor (CNI) for immune suppression. Data from multiple studies have been synthesized in several meta-analyses: the meta-analysis by Knoll et al. showed that mTOR use reduced NMSC rates in transplant recipients (Knoll et al. 2014), that by Kauffman et al. showed mTOR inhibitors were associated with reduced de novo malignancy across the board, including NMSC in kidney recipients (Kauffman et al. 2005) and the meta-analysis by Liang et al. showed reduce de novo malignancy in LT recipients (Liang et al. 2012). However, this topic is controversial, and some studies did not show a protective effect of mTOR on NMSC. There were no actual data in our dataset regarding the reasons why sirolimus was introduced to those respective recipients, either. Never-the-less our decision tree analysis identified mTOR inhibitor use as one of the 5 most important variables that predict the risk of NMSC. Results from clinical trials comparing CNI to mTOR suggest that patients at high risk for skin cancer derive the greatest benefit from changing to an mTOR early, before they have developed multiple lesions (Geissler 2015). Our results support using an mTOR but in addition, the decision tree can help practitioners identify the high risk patients.

We also identified lower BMI as a risk factor for NMSC. This is in keeping with studies of non-transplant patients that showed that obesity protected against the development of NMSC (Pothiawala et al. 2012; Tang et al. 2013), possibly because obesity is associated with less sun exposure (Rigel 2008; Trost et al. 2002).

Patients with history of alcohol consumption, as the underlying etiology of liver disease, were reported to have higher risk in developing NMSC after transplantation (Bellamy et al. 2001; Modaresi Esfeh et al. 2012), although it was not observed in our current study, presumably because the UNOS STAR file did not include actual daily amount, frequency or length of alcohol consumption prior to LT.

The decision tree analysis provides practitioners with a simple and easy to use tool to accurately stratify and quantify risk of NMSC in liver transplant recipients. Patients at high risk can potentially be managed differently. Current guidelines recommend that SOT recipients have annual skin exam surveillance for NMSC (McCaughan and Vajdic 2013). However high risk patients could potentially be considered for more frequent surveillance or additional interventions, such as retinoid chemoprophylaxis, or changes in intensity or type of immune suppression (Kim et al. 2016). In addition, this result also could be utilized in enrolling patients at high risk in interventional studies to prove safety and efficacy and lack of harm. Further research is warranted in this topic. Those with the predictive index less than 3.0 could have less frequent surveillance based on its high sensitivity and negative predictive value, however even our lowest risk group, with 124 cases/100,000 person-years have an age adjusted incidence of NMSC that is four times more than would be expected in the general population (38 cases per 100,000 person-years) (Tejera-Vaquerizo et al. 2016). Thus, even the low risk group should be followed according to current guidelines, with annual skin examinations for cancer surveillance at this point.

The decision tree model showed that patients with high-risk for de novo NMSC had 7.4% of the development, and 3.1–5.5% in patients with intermediate-risk during the follow up period, however, the Kaplan Meier analysis showed that 10 and 20 years cumulative development rate were much higher than those. This is mainly based on the nature of decision tree analysis (this is not a probability model to represent times-to-event), and the analysis was conducted by the population with median follow up period of 6.7 years. Thus, it probably is reasonable to see much higher rate of NMSC in later post-transplant years. In addition, the patient with Caucasian race who were < 47 years old had lowest development rate of NMSC amongst the Intermediate risk group. It could be argued that the higher rate of the development rate of NMSC in High- and the rest of the Intermediate-risk group was associated with their getting older during the study period, which should increase the risk of NMSC.

A limitation of our study is that model building (derivation) and validation sets were not from entirely independent cohorts. However, the model building and validation sets were randomly derived a priori, with no patients overlapping in the two sets, and the sets had identical distribution of variables: with no statistical differences in any of the variables in the model building and validation sets. A second limitation is that the decision tree analysis was performed using retrospectively and voluntary collected registry- based database (UNOS STAR file), which originally is not designed specifically to track cancer incidence. We were not able to assess pre-existing NMSC as a potential risk factor (Lowenstein et al. 2017) because the data was obtained only in approximately 10% of the cases (n = 11,916). Also we were not able to evaluate smoking history or human papilloma virus (Chockalingam et al. 2015; Euvrard et al. 2003) as the UNOS STAR file do not include such information. We could not control for intensity of immunosuppression over time or be certain of how long patients who were discharged on sirolimus remained on it either. In addition, it did not allow us to accurately identify those who died of NMSC or who lost follow up. A third limitation is that the OPTN does not collect data on everolimus use at discharge. Some subjects who were not using sirolimus may have received everolimus. If we assume that both mTOR inhibitors (everolimus and sirolimus) have similar antineoplastic properties, then those on everolimus would have been protected, but would have been analyzed in the non-sirolimus group. This would weaken the signal of the protective effect of mTOR inhibitors, but would certainly not invalidate the observation that sirolimus protects against developing NMSC.

In conclusion, our study confirms the importance of the known risk factors of Caucasian race, older age, and male gender as risk factors, but identifies the new variables of sirolimus and obesity (both protective) in establishing an individual’s risk of getting NMSC. We have confirmed those with very high risk of developing NMSC after liver transplantation. However, even the lowest risk group has 4 times the risk of the general population. Annual surveillance for skin cancer seems to be an appropriate minimum recommendation for all post-LT patients including the lowest risk group, but the highest group might need more frequent surveillance. We have used decision tree analysis to derive simple and easy to use tool to accurately stratify and quantify risk of NMSC post-LT. It gives specific guidance about age cutoff (> 47) and BMI cutoff (Pothiawala et al. 2012) for optimal risk stratification. This decision tree could help providers to individualize care of their liver transplant recipients.
",https://pubmed.ncbi.nlm.nih.gov/29362916/
Barbosa et al. 2018,Machine learning algorithms utilizing quantitative CT features may predict eventual onset of bronchiolitis obliterans syndrome after lung transplantation,"Rationale and Objectives
Long-term survival after lung transplantation (LTx) is limited by bronchiolitis obliterans syndrome (BOS), defined as a sustained decline in forced expiratory volume in the first second (FEV1) not explained by other causes. We assessed whether machine learning (ML) utilizing quantitative computed tomography (qCT) metrics can predict eventual development of BOS.
Materials and Methods
Paired inspiratory-expiratory CT scans of 71 patients who underwent LTx were analyzed retrospectively (BOS [n = 41] versus non-BOS [n = 30]), using at least two different time points. The BOS cohort experienced a reduction in FEV1 of >10% compared to baseline FEV1 post LTx. Multifactor analysis correlated declining FEV1 with qCT features linked to acute inflammation or BOS onset. Student t test and ML were applied on baseline qCT features to identify lung transplant patients at baseline that eventually developed BOS.
Results
The FEV1 decline in the BOS cohort correlated with an increase in the lung volume (P = .027) and in the central airway volume at functional residual capacity (P = .018), not observed in non-BOS patients, whereas the non-BOS cohort experienced a decrease in the central airway volume at total lung capacity with declining FEV1 (P = .039). Twenty-three baseline qCT parameters could significantly distinguish between non-BOS patients and eventual BOS developers (P < .05), whereas no pulmonary function testing parameters could. Using ML methods (support vector machine), we could identify BOS developers at baseline with an accuracy of 85%, using only three qCT parameters.
Conclusions
ML utilizing qCT could discern distinct mechanisms driving FEV1 decline in BOS and non-BOS LTx patients and predict eventual onset of BOS. This approach may become useful to optimize management of LTx patients.","The current definition of BOS is based on a sustained and otherwise unexplained decline in FEV1. However, pulmonary function tests have limited ability to detect its onset before the development of irreversible decline in lung function, are insensitive to early disease, and can be confounded by mixed changes (eg, combination of restrictive and obstructive pathophysiology). FRI is a validated quantitative imaging technique that has been shown to be more sensitive than pulmonary function tests primarily because of its capacity to obtain regional information and to characterize early structural and functional changes, potentially enabling detection of disease at an earlier stage (15).
Multiple studies have suggested that HRCT could be useful in the diagnosis of BOS after LTx; however, its clinical value has not yet been proven because of inconsistent findings (7, 16, 17). Nonetheless, two more recent publications demonstrate promising results in using advanced quantitative analysis of HRCT in LTx patients to improve the accuracy of diagnosis of BOS (18) and in predicting functional parameters such as FEV1 from qCT metrics specifically obtained from the transplanted lung in unilateral LTx (19). In our study, CT scans obtained after LTx were quantitatively analyzed using FRI to investigate how multiple quantitative imaging parameters correlate with the onset of BOS and to differentiate the mechanism accounting for FEV1 decline, whether related to bronchiolitis obliterans or a reversible cause, such as a pulmonary infection or pulmonary edema.
FRI could distinguish the onset of BOS from other reversible causes in patients that underwent LTx and presented with FEV1 decline. The decrease in FEV1 in non-BOS patients was correlated with a decrease in the central airway volume at TLC. On the other hand, BOS patients experienced an increase in lung volume and central airway volume at FRC with decreasing FEV1.
We hypothesize that these findings reflect an increase in the extent of air trapping and peribronchiolar fibrosis related to the progressive airflow obstruction, which characterizes the BOS phenotype of CLAD after LTx. Previous studies have also shown evidence of air trapping on expiratory CT scans and have pointed this as an indicator of the small airway disease that underlies BOS (1, 16, 20). On the other hand, changes in lung structure and airways for the non-BOS cohort were more evident at TLC and in an opposite direction (decrease in central airway volume), illustrating that for these patients, the nonsustained decline in FEV1 is not associated with obliterative bronchiolitis (as would be expected with BOS), but rather with large airway bronchial wall thickening (resulting in decreases in central airways luminal volume), reflecting bronchitis or interstitial edema, as these changes were by definition reversible (eg, these patients recovered their FEV1 and did not develop BOS).
From a pathophysiological point of view, our results seem to be concordant with these hypotheses. Bronchiolitis obliterans is a small airway disease, which in CT images is often reflected by hyperinflation and bronchodilation at the expiratory (FRC) scan. Respiratory infections (especially acute bronchitis and bronchopneumonia), on the other hand, predominantly affect the central airways leading to less prominent changes at expiration than at inspiration, where bronchial wall thickening with decreased large airway luminal volume may be the predominant effect. These different pathophysiological expressions likely support the differences in behavior of the qCT metrics that we have observed between FRC and TLC scans in LTx patients with a decline in FEV1.
Furthermore, it should be noted that in most bilateral transplantation patients who developed BOS, a general trend of differences in hyperinflation for the left and right lungs could be seen. The representative BOS patient in Figure 6 is a perfect example of this phenomenon. We believe, and this is merely a hypothesis, that internal airflow redistribution due to mechanical properties specific for each lung (compliances) may be correlated with this occurrence. These trends will be subject to further research.
For each of the multidimensional classification tasks, ML with local and global FRI features determined the set of features that are most predictive of BOS outcome. All significant biomarkers at baseline may play a role in eventual outcome. However, ML, as it is used in this study, should be considered a tool to enable the most accurate prediction in each multidimensional space by clustering the different cohorts in the most effective way possible for the given order of dimensions. Classification in higher order dimensional spaces is certainly conceivable; however, our objective was to show that, based on a relative low amount of baseline qCT FRI features (up to 3), highly accurate predictions are still feasible.
Mixed effects models suggest that evolution toward onset of BOS is characterized by increasing lobe and airway volumes at FRC, likely due to increased hyperinflation. However, the ML classification results show that at baseline, lobe and airway volumes in the developing BOS cohort are smaller than normal (%P < < 100%). The onset of BOS could therefore be related to underexpansion of the transplanted lung followed by increases in lobe and airway volumes. Considering that this increase in lobe and airway volumes will bring the parameters closer to the mean of healthy individuals, although not in a sustainable manner, these increased volumes could explain the limited sensitivity of FEV1 to detect early onset of BOS. The fact that baseline FEV1 and FEV1 as percent predicted are not significant in predicting eventual BOS development suggests that in early phases of BOS, the compensatory nature of healthy areas of the transplanted lungs is still able to balance the effect of diseased areas found by FRI at a regional level of analysis. Therefore, these findings support that the examination of regional structural and functional parameters is crucial in the process of early-stage BOS detection. These hypotheses will be tested in future larger scale prospective studies.
There were limitations of the present study, mostly related to its retrospective nature. The CT scans were not obtained after a consistent exam protocol with set intervals, which lead to variability in the number, interval, and frequency of available scans per patient. Scans were obtained as clinically indicated by the transplant pulmonologist. Additionally, no spirometric control was used for the paired inspiratory and expiratory CT acquisition (noting this is standard clinical practice); therefore, the precise phase of the respiratory cycle cannot be absolutely ensured for all scans (noting that poor-quality scans with inadequate respiratory maneuvers were excluded a priori by a board-certified radiologist). The identification of the BOS patients was performed based on FEV1 (which is standard clinical practice), and there might have been variations in PFT technique, given the long span of time pertaining to our data, which could have impacted the sensitivity to detect BOS (17). Finally, the number of patients in our sample was relatively small. It is important to note that, although most of our patients did not have pathologic confirmation of the diagnosis of bronchiolitis obliterans, this is not necessary and is rarely performed in clinical settings. The clinical diagnosis of BOS does not require pathologic confirmation.
Nevertheless, the results illustrate the potential of FRI-derived qCT imaging biomarkers to assist in the early diagnosis of BOS and in the elucidation of the etiologic mechanism accounting for FEV1 decline, whether due to true onset of BOS or from a reversible cause such as a pulmonary infection or edema. The distinct behavior of CT metrics between BOS and non-BOS patients after LTx experiencing FEV1 decline is potentially important for prognostication and may be helpful to guide management and therapy development for the LTx patient population.
As future directions of this research, we intend to explore whether FRI-derived qCT metrics can capture differences in behavior between the native and the transplanted lungs in unilateral LTx, and between unilateral and bilateral LTx. The latter is of interest because the type of transplant operation has an impact on survival, with bilateral transplantation being associated with higher overall survival rate in certain clinical situations (21, 22), however, constrained by the limited supply of donor organs relative to the demand for LTx. If quantitative regional information can be extracted from CT images and linked to the transplanted lung in unilateral LTx, this may provide a more accurate way to diagnose early-stage BOS and could possibly serve as a predictor of future disease. By definition, PFT can only capture global information (the combined pulmonary function of the allograft and native lung in a single LTx situation) and therefore may be particularly insensitive to early functional and structural changes in the unilateral LTx population. By using qCT methods such as FRI, changes in the transplanted and nontransplanted lung can be assessed separately to improve prediction of early BOS onset.",https://pubmed.ncbi.nlm.nih.gov/29472146/
Raji and Safna 2022,Computational methods for predicting the outcome of thoracic transplantation,"Cardiac disease and the death rates due to coronary heart failure and cardiomyopathy are increasing. Thoracic transplantation is now a widely accepted therapeutic option for end-stage cardiac failure. The survival rate after the organ transplantation is crucial. Survival prediction after heart transplantation is a hot area of research. The use of conventional statistical techniques is computationally expensive and does not provide reliable solutions. Artificial Neural Networks based survival prediction helps surgeons make precise decisions and predict the best outcomes. The proposed system implements multi-layer perceptron algorithm, which shows good performance in survival prediction. We also implemented our work in the Radial Basis Function Network model to prove the accuracy of proposed model. For this research study, data were collected from United Network for Organ Sharing database and extracted the relevant thoracic transplantation survival prediction attributes with the help of suitable data mining techniques. We obtained an accuracy of 97.1% from the multi-layer perceptron model with the evaluation of various performance measures. In order to assure the validity of the proposed model we implemented the Radial Basis Function model and obtained an accuracy of 92.37%. We collated the accuracy of proposed survival prediction models with existing systems and proved that the proposed system appeared to be best for survival prediction with higher accuracy compared to 85.9% in the existing system. The outcome of the model will be an asset for the lifesaving procedures in the medical field.","Thoracic transplantation and the survival after the thoracic transplantation is a hot area of research. The donor scarcity is an important problem and hence in such a synopsis, every organ allocation has to be accurate. We all know that survival from Thoracic transplantation is very risky in the medical domain. Hence, we propose a computational model for survival prediction for which we used a relevant live dataset. Data were validated accurately and 25 relevant attributes for survival prediction were extracted and our dataset was implemented in the selected MLP model with back-propagation. The proposed system obtained a higher accuracy of 97.1%. In order to forecast the accuracy of proposed model, we implemented another computational model RBF. It yielded an accuracy of 92.37%, which was less than that of the proposed model. As it is a life problem and in order to prove the accuracy of model, we compared the proposed system with an existing system. The existing system branched off the MLP model and was used to forecast the survival of heart lung transplant patients with another dataset, which produced an accuracy of only 85.9%. Hence, through all of these comparisons, we came to the conclusion that our proposed model with our relevant dataset had a higher accuracy of survival prediction in thoracic transplantation than the existing systems. The results will be very supportable for doctors for undertaking lifesaving procedures for the patients.
",https://pubmed.ncbi.nlm.nih.gov/34155697/
Liu et al. 2021,Machine learning for the prediction of red blood cell transfusion in patients during or after liver transplantation surgery,"Aim: This study aimed to use machine learning algorithms to identify critical preoperative variables and predict the red blood cell (RBC) transfusion during or after liver transplantation surgery.

Study Design and Methods: A total of 1,193 patients undergoing liver transplantation in three large tertiary hospitals in China were examined. Twenty-four preoperative variables were collected, including essential population characteristics, diagnosis, symptoms, and laboratory parameters. The cohort was randomly split into a train set (70%) and a validation set (30%). The Recursive Feature Elimination and eXtreme Gradient Boosting algorithms (XGBOOST) were used to select variables and build machine learning prediction models, respectively. Besides, seven other machine learning models and logistic regression were developed. The area under the receiver operating characteristic (AUROC) was used to compare the prediction performance of different models. The SHapley Additive exPlanations package was applied to interpret the XGBOOST model. Data from 31 patients at one of the hospitals were prospectively collected for model validation.

Results: In this study, 72.1% of patients in the training set and 73.2% in the validation set underwent RBC transfusion during or after the surgery. Nine vital preoperative variables were finally selected, including the presence of portal hypertension, age, hemoglobin, diagnosis, direct bilirubin, activated partial thromboplastin time, globulin, aspartate aminotransferase, and alanine aminotransferase. The XGBOOST model presented significantly better predictive performance (AUROC: 0.813) than other models and also performed well in the prospective dataset (accuracy: 76.9%).

Discussion: A model for predicting RBC transfusion during or after liver transplantation was successfully developed using a machine learning algorithm based on nine preoperative variables, which could guide high-risk patients to take appropriate preventive measures.","This study was novel in using machine learning algorithms to predict RBC transfusion during or after liver transplantation. A machine learning model was built that could accurately predict RBC transfusion during or after liver transplantation before the surgery, better than other models developed in this study. The model established in this study had great discrimination and showed satisfactory specificity and sensitivity. Therefore, the hypothesis proposed in this study was supported by the results.

Several studies showed that RBC transfusion increased complications and was related to a lower 5-year survival rate (3, 26). In addition, costs associated with transfusing a single unit of blood were significantly high, including the cost of treating any adverse effect of transfusion or the associated increased length of hospital stay. These costs far outweighed the lower cost of the use of tranexamic acid, erythropoietin (EPO), oral treatments of anemia, intravenous iron therapy, and cell salvage utilization. As a result, clinicians have taken many measures to reduce RBC transfusions (25, 27). By predicting RBC transfusion before surgeries, high-risk patients could be identified. The management of patients could be improved, thus improving outcomes and reducing morbidity and cost (4, 28, 29). Therefore, it was of great importance to predict RBC transfusion before surgeries and take corresponding preoperative measures.

In this study, a machine learning model was developed to predict RBC transfusion, which could help clinicians identify high-risk patients. If the model identified patients at low probability of transfusion, potentially unnecessary repeat testing was exempt, such as a complete blood count or further preoperative laboratory testing. Therefore, this model might be a valuable tool to avoid wasteful and unnecessary medical tests. Alternatively, identifying patients at high risk for transfusion might improve the efficiency of perioperative blood management and reduce transfusions. It was suspected that for each transfusion avoided, the patient and financial benefit might be significant due to the large number of patients undergoing gynecologic surgery. Future investigations should include measuring the model's impact on patient and cost outcomes.

In addition, two examples were used to visualize how the model could predict RBC transfusion and determine the relative importance of each variable for the clinician. With millions of liver transplants taking place each year, the findings could help surgeons perform liver transplants, while also giving patients information about their probabilities of receiving RBC transfusion before surgery.

Previous studies reported that intraoperative blood loss and postoperative decreased hemoglobin levels were associated with the risk of receiving an RBC transfusion (30–32). However, preoperative information should be used to predict the need for RBC transfusion so as to find other risk features; otherwise, it is too late to take action to determine transfusion risk through intraoperative or postoperative information.

The significance of this study was that it combined preoperative characteristic variables other than hemoglobin to establish a clinical prediction model. Portal hypertension, age, hemoglobin, diagnosis, direct bilirubin, APTT, ALT, AST, and globulin were selected as important variables. Arshad found that portal hypertension was associated with increased blood loss and RBC transfusion in orthotopic liver transplantation (33), which was similar to the result of the present analysis. Fabio Bagante established a nomogram of hepatectomy to predict the risk of transfusion and included total bilirubin among the risk factors for transfusion. However, the present study found that the level of direct bilirubin correlated with the risk of transfusion in patients undergoing liver transplantation (34). Most studies assessing the risk of transfusion also demonstrated a vital role for age and preoperative hemoglobin in predicting transfusion (3, 35, 36). All of the aforementioned studies supported the results of the present study very well. Besides, this study also found other variables that increased the risk of RBC transfusion, including preoperative APTT, AST, ALT, and globulin. APTT reflects the patient's coagulation function; the lower the coagulation function, the greater the likelihood of intraoperative blood loss, thus increasing the risk of RBC transfusion. Therefore, clinical decision-makers should consider using the pro-coagulation treatment and administering drugs that could alter the coagulation state with careful thinking for patients predicted as high-risk groups. An abnormal level of AST, ALT, or globulin reflected the poor state of a patient's liver function, which might indirectly represent a decreased coagulation state and increased risk of transfusion. Focusing solely on hemoglobin to determine whether to transfuse might be of limited utility, and comprehensive inclusion of preoperative patient information could help guide clinical transfusion decisions and more effective blood management. For high-risk patients, clinicians should consider correcting hemoglobin before surgery and provide liver protection treatment to improve liver function, coagulation function, and portal hypertension.

In this study, an RBC transfusion prediction model was developed with great discrimination. This study included multi-center datasets and prospective validation, which was also an advantage compared with other studies; the abundant data allowed rigorous evaluation of the performance of machine learning models. Ultimately, the approach used in the present study can be applied to a variety of problems that arise before and after surgery to make the surgery safe. Furthermore, it can also be applied to other complications and operations, such as sepsis and acute kidney injury (37–41).

This study had several limitations. First, the transfusion criteria were not the same in each institution; therefore, the definition of the transfusion group was different. A vast majority of institutions were based on a restrictive transfusion strategy, where patients were transfused when their hemoglobin was <70 g/L (42, 43). Second, the surgeons at each institution had different surgical plans; other factors might also lead to blood transfusions, thus affecting the results. Third, the training and the validation sets were divided as a 7:3 ratio, and using other external validation sets might yield different results. Therefore, more datasets from other centers were needed for validation. Fourth, patients with missing critical data were excluded, causing selection bias. Fifth, like other retrospective studies, a selection bias might exist without considering unknown confounding factors. Lastly, although SHAP values were used to help interpret our machine learning model, a more interpretable model is still needed in clinical practice (44). As a future work, we planned to develop a Nomogram or machine learning-based automatic clinical scoring system based on our data, in order to provide clinicians a more usable and easy-to-understand tool (45).
",https://doi.org/10.3389%2Ffmed.2021.632210
Yasodhara et al. 2021,Identifying Modifiable Predictors of Long‐Term Survival in Liver Transplant Recipients With Diabetes Mellitus Using Machine Learning,"Diabetes mellitus (DM) significantly impacts long‐term survival after liver transplantation (LT). We identified survival factors for LT recipients who had DM to inform preventive care using machine‐learning analysis. We analyzed risk factors for mortality in patients from across the United States using the Scientific Registry of Transplant Recipients (SRTR). Patients had undergone LT from 1987 to 2019, with a follow‐up of 6.47 years (standard deviation [SD] 5.95). Findings were validated on a cohort from the University Health Network (UHN) from 1989 to 2014 (follow‐up 8.15 years [SD 5.67]). Analysis was conducted with Cox proportional hazards and gradient boosting survival. The training set included 84.67% SRTR data (n = 15,289 patients), and the test set included 15.33% SRTR patients (n = 2769) and data from UHN patients (n = 1290). We included 18,058 adults (12,108 [67.05%] men, average age 54.21 years [SD 9.98]) from the SRTR who had undergone LT and had complete data for investigated features. A total of 4634 patients had preexisting DM, and 3158 had post‐LT DM. The UHN data consisted of 1290 LT recipients (910 [70.5%] men, average age 54.0 years [SD 10.4]). Increased serum creatinine and hypertension significantly impacted mortality with preexisting DM 1.36 (95% confidence interval [CI], 1.21‐1.54) and 1.20 (95% CI, 1.06‐1.35) times, respectively. Sirolimus use increased mortality 1.36 times (95% CI, 1.18‐1.58) in nondiabetics and 1.33 times (95% CI, 1.09‐1.63) in patients with preexisting DM. A similar effect was found in post‐LT DM, although it was not statistically significant (1.38 times; 95% CI, 1.07‐1.77; P = 0.07). Survival predictors generally achieved a 0.60 to 0.70 area under the receiver operating characteristic for 5‐year mortality. LT recipients who have DM have a higher mortality risk than those without DM. Hypertension, decreased renal function, and sirolimus for maintenance immunosuppression compound this mortality risk. These predisposing factors must be intensively treated and modified to optimize long‐term survival after transplant.","DM affects a large proportion of LT recipients and has significant adverse impacts on long‐term survival.( 8 ) Consensus guidelines provide general recommendations for screening and treatment that are not targeted to this high‐risk group of recipients.( 14 ) Our study using the SRTR database provides a framework for developing such targeted guidelines in LT recipients. Pre‐DM significantly increased both general and cardiovascular mortality in LT recipients. Hypertension and elevated serum creatinine were significant factors that compounded the risk of mortality among LT patients with DM. This suggests that LT recipients with DM should be carefully screened for hypertension and chronic kidney disease (both of which are complications of the most common immunosuppressants) and strictly managed to optimize long‐term posttransplant survival.( 2 , 7 )

We also separated LT recipients into 3 categories (no DM, pre‐DM, and PTDM) and examined factors that affected posttransplant survival in each group. For recipients without DM, the use of corticosteroids and sirolimus resulted in an increase in general posttransplant mortality, whereas the use of the antimetabolite mycophenolate was associated with a decrease in general posttransplant mortality.

For patients with pre‐DM, elevated creatinine, hypertension, and use of steroids and sirolimus as immunosuppression affected posttransplant general survival. Use of sirolimus increases insulin resistance and has previously been associated with higher long‐term mortality.( 15 ) Elevated creatinine, age at time of transplant, and steroid use also affected posttransplant cardiovascular mortality. Regarding recipients with PTDM, the use of sirolimus for immunosuppression negatively impacted general survival.

ML has been used to identify patients most likely to benefit from LT along with factors affecting posttransplant complications and survival.( 16 ) Using ML, Bhat and colleagues identified several predictors of development of PTDM, including increasing age at time of transplant, male sex, and obesity.( 8 ) PTDM in turn has been found to be a risk factor for posttransplant cardiovascular events, graft loss, and development of infections.( 17 ) In this study, we did not find PTDM to significantly impact mortality, which may be attributed to the lower number of samples caused by high missingness.

Despite the fact that patients with DM have worse posttransplant outcomes, few studies have looked at factors affecting survival in LT recipients with DM.( 18 ) To our knowledge, our study is the largest to examine factors affecting survival in patients with DM post‐LT. Factors that reduced survival include age at the time of transplant, presence of hypertension, elevated creatinine, and use of steroids or sirolimus as an immunosuppressant.

Cardiovascular compromise reduces long‐term survival after LT.( 5 , 19 ) Not only does hypertension increase the risk of heart disease but also it is a major risk factor for renal failure, similar to DM. Again, having multiple risk factors leads to significantly higher rates of developing chronic kidney disease. Similar to heart disease, the presence of pretransplant kidney dysfunction negatively impacts posttransplant survival.( 20 ) This helps explain why the pretransplant creatinine level is an important factor affecting posttransplant outcomes in our study. Moreover, in LT recipients with pretransplant cardiovascular disease, elevated serum creatinine at 1 year posttransplant has been associated with higher cardiovascular events in the long term and consequently higher mortality.( 21 ) Another study showed that pre‐LT DM along with a ≥30% decrease in GFR within the first year after transplantation are predictors for advanced chronic kidney disease and long‐term mortality.( 22 ) Corticosteroids are a common immunosuppressant post‐LT. However, one of the major side effects with these medications is hyperglycemia.( 23 ) This often exacerbates and worsens PTDM glycemic control.( 24 ) A steroid‐free and mycophenolate mofetil–containing regimen has been shown to decrease risk of cardiovascular mortality, which is consistent with findings in our study.( 25 ) Sirolimus received a black box warning from the US Food and Drug Administration in 2002 stating it was associated with higher mortality based on clinical trial findings.( 15 ) Similarly, our study showed greater mortality in all LT recipients using sirolimus. Studies have demonstrated deleterious effects of hyperglycemia post‐LT. Wallia and colleagues showed that posttransplant hyperglycemia was associated with increased risks of graft rejection and infection.( 26 ) Overall, these complications reduced posttransplant survival.

By identifying the presence of pre‐DM as a risk factor for mortality post‐LT, efforts can be made during the pretransplant period to identify patients with DM and optimize their diabetic management. Focus should be put on glycemic management with both nonpharmacological and pharmacological strategies. Patients with cirrhosis are often on a low‐salt diet to minimize ascites and edema, and it is not feasible for most to engage in exercise. In addition, insulin is the drug of choice in patients with DM with cirrhosis because of the concerns regarding oral hypoglycemic agents such as metformin in the setting of impaired liver metabolism. The goal should be proper reduction of hemoglobin A1C levels to reduce the development of diabetic complications, which may certainly impact post‐LT outcomes. By also identifying individual mortality risk factors among patients with pretransplant DM, screening for these risk factors should be undertaken and addressed when present.

We found that hypertension and chronic kidney disease synergize with DM to increase cardiovascular mortality in LT recipients. Given that cardiovascular mortality is 2 to 3 times higher in LT recipients compared with the general population, this suggests that we must be particularly watchful and monitor LT recipients closely for hypertension and chronic kidney disease especially in patients with DM. Interventions would include minimizing calcineurin inhibitors as much as possible, managing optimally hypertension and chronic kidney disease, and referring patients with all 3 risk factors to a cardiologist for screening and follow‐up.",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8248095/
Thongprayoon et al. 2023,Differences between Very Highly Sensitized Kidney Transplant Recipients as Identified by Machine Learning Consensus Clustering,"Background and Objectives: The aim of our study was to categorize very highly sensitized kidney transplant recipients with pre-transplant panel reactive antibody (PRA) ≥ 98% using an unsupervised machine learning approach as clinical outcomes for this population are inferior, despite receiving increased allocation priority. Identifying subgroups with higher risks for inferior outcomes is essential to guide individualized management strategies for these vulnerable recipients. Materials and Methods: To achieve this, we analyzed the Organ Procurement and Transplantation Network (OPTN)/United Network for Organ Sharing (UNOS) database from 2010 to 2019 and performed consensus cluster analysis based on the recipient-, donor-, and transplant-related characteristics in 7458 kidney transplant patients with pre-transplant PRA ≥ 98%. The key characteristics of each cluster were identified by calculating the standardized mean difference. The post-transplant outcomes were compared between the assigned clusters. Results: We identified two distinct clusters and compared the post-transplant outcomes among the assigned clusters of very highly sensitized kidney transplant patients. Cluster 1 patients were younger (median age 45 years), male predominant, and more likely to have previously undergone a kidney transplant, but had less diabetic kidney disease. Cluster 2 recipients were older (median 54 years), female predominant, and more likely to be undergoing a first-time transplant. While patient survival was comparable between the two clusters, cluster 1 had lower death-censored graft survival and higher acute rejection compared to cluster 2. Conclusions: The unsupervised machine learning approach categorized very highly sensitized kidney transplant patients into two clinically distinct clusters with differing post-transplant outcomes. A better understanding of these clinically distinct subgroups may assist the transplant community in developing individualized care strategies and improving the outcomes for very highly sensitized kidney transplant patients.","Inferior clinical outcomes have been observed in very highly sensitized kidney transplant recipients (PRA ≥ 98%). Factors contributing to these inferior outcomes are multifactorial and include variables both directly and indirectly linked to immunologic risk [1,8,22,23]. Using an unsupervised ML consensus clustering tool, we were able to categorize very highly sensitized kidney transplant recipients (PRA ≥ 98%) in the OPTN/UNOS database into two distinct clusters that demonstrated high stability. This study identified that the two distinct subtypes had different clinical outcomes, specifically in relation to acute rejection and death-censored graft failure.
Mechanisms for sensitization are likely to be different between the two clusters as the cluster 1 recipients were more likely to be male and undergoing re-transplantation while the cluster 2 recipients were more likely to be female and undergoing a first-time transplantation. The first cluster, cluster 1, was more likely to be composed of male recipients who were undergoing re-transplantation, and their immune system may have recognized the new transplanted organ as foreign and mounted a response against it, potentially leading to rejection. In contrast, the second cluster, cluster 2, was more likely to be composed of female recipients undergoing their first transplant. These individuals may have been sensitized by prior pregnancies, which can lead to the development of antibodies against the transplanted organ as foreign and lead to rejection. Additionally, more recipients in cluster 1 had a PRA of 100%, suggesting that their immune system had a high level of pre-existing antibodies due to the sensitizing events associated with their prior transplants, which could have led to the development of a more robust immune response. In summary, the two clusters of organ transplant recipients may have had different mechanisms for sensitization based on their gender, previous transplant history, or pregnancies, which could affect their likelihood of developing rejection after transplant.
Furthermore, cluster 1 recipients were also more likely to have had exposure to prior immunosuppression, which is associated with an increased risk of infection and malignancy, in addition to the risk of donor-specific antibody reactivation and antibody-mediated graft rejection. These cumulatively may have translated to cluster 1′s lower patient and graft survival [1,24,25,26,27]. It has been acknowledged that the mode of sensitization can predict graft survival. Sensitization due to prior pregnancies or transfusions has been found to increase the risk of graft loss by 23%, while re-transplantation increases the risk by 58% [28].
Recipients in cluster 1 had a higher likelihood of receiving dialysis for over three years before transplantation compared to those in cluster 2. This prolonged waiting time and dialysis vintage were likely contributors to cluster 1’s inferior long-term outcomes [29,30,31]. Prior research has indicated that patients who underwent preemptive transplants exhibited superior graft and patient survival rates than those with dialysis vintage durations of <5 years, 5–9 years, and ≥10 years [29]. Cold ischemia time was also found to be associated with a higher risk of delayed graft function; however, this effect was modest and had less impact than the kidney donor profile index (KDPI). Therefore, prolonged cold ischemia time alone should not be considered as a primary reason to decline transplantation [30]. Unexpectedly, cluster 1 recipients had less diabetes (17% vs. 35%, respectively) and diabetic-related ESKD (3% vs. 29%, respectively) compared to cluster 2. Of note, ESKD was caused by unknown factors in around 70% of cluster 1 patients compared to only 16% in cluster 2. Although more patients in cluster 2 had diabetes, improvements in diabetes care and the optimization of immunosuppressant therapy may contribute to their superior graft outcomes.
There has been a long-standing debate in the transplant community about the appropriate level of priority that should be given to highly sensitized patients. The current algorithm allocates kidneys based on the level of PRA, which some have argued gives too much weight to sensitization status. In the present study, the majority of kidney transplants in both clusters 1 and 2 were from standard KDPI non-ECD donors, indicating that these patients had access to good quality allografts. However, the transplant outcomes observed in highly sensitized patients were not as good as might be expected. Recent discussions about the development of a new kidney allocation framework have focused on decreasing the emphasis on PRA and providing more equitable access to kidney transplantation for patients on the waiting list. These discussions have been fueled in part by the findings of studies like this one, which highlight the need for a more nuanced approach to kidney allocation. Specifically, the study suggests that while PRA remains an important factor in determining priority for transplantation, other factors such as dialysis vintage and waiting time should also be taken into account [32]. Additionally, the study underscores the need for further exploration of the access and priority concerns for highly sensitized patients. Certain very highly sensitized patients received preemptive transplants while others did not, raising questions about the effectiveness of the current allocation system in addressing the diverse sensitization statuses of patients. A comprehensive kidney allocation framework that considers multiple factors such as PRA, waiting time, dialysis vintage, and access will be critical to ensuring equitable and unbiased access to kidney transplantation for all patients.
In this study, there were several limitations that should be acknowledged. First, due to the national registry’s limitations, details regarding graft rejection, graft failure, and patient death are lacking. Moreover, there might be missing data and loss of follow-up in patients, which could have led to the underestimation of outcomes. However, to minimize bias, we utilized the multivariable imputation by the chained equation strategy for missing data. Additionally, the OPTN/UNOS registry tends to underreport rejection events, so the reported rejection events in this study may not fully represent the immunologic risk events in highly sensitized patients. Finally, our outcomes are not reflective of highly sensitized patients who remain on the waitlist.
To our knowledge, this is the first ML clustering approach that has been successfully applied to very highly sensitized kidney transplant recipients (PRA ≥ 98%). The ML clustering algorithms allowed for the identification of two distinct subgroups without human intervention. The patients in cluster 2 had better outcomes regarding death-censored graft survival and acute rejection, indicating that highly sensitized kidney transplant recipients, even with a PRA of ≥98%, are a heterogeneous population. Consequently, this categorization could enable targeted interventions to improve outcomes. Furthermore, the different cluster distributions among the 11 OPTN/UNOS regions may facilitate the identification of future management strategies that incorporate geographical location to enhance outcomes for highly sensitized kidney transplant recipients.
While this study’s unsupervised ML clustering approach provides extensive insight into the various phenotypes of very highly sensitized kidney transplant recipients in the United States and their corresponding post-transplant outcomes, it is important to note that ML clustering algorithms have limitations in terms of directly generating risk predictions for individual cases. As such, future research should investigate the efficacy of supervised ML prediction models (such as neural network and extreme gradient boosting) utilizing labeled outcomes for the purpose of predicting graft loss and mortality in very highly sensitized kidney transplant recipients. These models should be compared against traditional prediction models and standard risk-adjusted outcomes to ascertain their predictive performances.",https://www.mdpi.com/1648-9144/59/5/977
Jo et al. 2023,A convolutional neural network-based model that predicts acute graft-versus-host disease after allogeneic hematopoietic stem cell transplantation,"Background
Forecasting acute graft-versus-host disease (aGVHD) after allogeneic hematopoietic stem cell transplantation (HSCT) is highly challenging with conventional statistical techniques due to complex parameters and their interactions. The primary object of this study was to establish a convolutional neural network (CNN)-based prediction model for aGVHD.

Method
We analyzed adult patients who underwent allogeneic HSCT between 2008 and 2018, using the Japanese nationwide registry database. The CNN algorithm, equipped with a natural language processing technique and an interpretable explanation algorithm, was applied to develop and validate prediction models.

Results
Here, we evaluate 18,763 patients between 16 and 80 years of age (median, 50 years). In total, grade II–IV and grade III–IV aGVHD is observed among 42.0% and 15.6%. The CNN-based model eventually allows us to calculate a prediction score of aGVHD for an individual case, which is validated to distinguish the high-risk group of aGVHD in the test cohort: cumulative incidence of grade III–IV aGVHD at Day 100 after HSCT is 28.8% for patients assigned to a high-risk group by the CNN model, compared to 8.4% among low-risk patients (hazard ratio, 4.02; 95% confidence interval, 2.70–5.97; p < 0.01), suggesting high generalizability. Furthermore, our CNN-based model succeeds in visualizing the learning process. Moreover, contributions of pre-transplant parameters other than HLA information to the risk of aGVHD are determined.

Conclusions
Our results suggest that CNN-based prediction provides a faithful prediction model for aGVHD, and can serve as a valuable tool for decision-making in clinical practice.","This machine learning-guided retrospective cohort study investigating risk prediction of aGVHD revealed four major results/findings: (1) A CNN-based model, which can extract discriminating features from comprehensive patient characteristics, was developed to predict risk of aGVHD after HSCT. (2) The learning process employed by the CNN-based model successfully visualized the weight of each clinical factor. (3) Raw HLA data was utilized by the CNN-based model. (4) Influences of factors other than HLA disparity on the risk of aGVHD were clarified.

Whereas CNN made its early success in the area of image analysis, it has also been applied in various areas due to its excellent feature extraction capability31, 32. By applying the CNN algorithm, we developed a prediction model for grade II–IV and grade III–IV aGVHD. The generalizability of the model was determined using an internal test cohort. The model succeeded in discriminating between patient groups with a high and low risk of aGVHD. This risk stratification is important because aGVHD is one of the most serious complications, often leading to TRM33. Indeed, in this study, patients in the high-risk group had higher TRM and poorer OS than those in the low-risk group (Fig. 5B, C, E, and F). Our reliable prediction model optimizes transplantation procedures by choosing risk-adapted immunosuppression, thereby improving transplantation outcomes.

While the machine learning-based approach has the advantage of unbiased feature selection and prediction, one of the major challenges of machine learning is the difficulty of understanding how it functions. Transparency is essential in order to widely implement a machine-learning model in clinical practice; however, variables used by the model to make its judgment, vary among patients, depending on other clinical factors and interactions between variables. Therefore, these variables must be weighed on a case-by-case basis. For example, HLA disparity, which is a major contributing factor for aGVHD, has different effects among underlying diseases7. In this context, our CNN-based model succeeded in visualizing the learning process with t-SNE and in assessing the weights of variables in individual cases using LIME. Our results suggest that this CNN-based model employing techniques that render it transparent and comprehensible will help clinicians to select optimal donor sources and transplantation procedures with confidence.

While we and other groups have previously developed machine learning-based models to predict outcomes after HSCT7–9, but the arbitrariness of variable settings has not been solved, especially regarding HLA information. In this study, for the first time, we incorporated raw information about specific antigens and/or alleles of both donors and recipients into a machine learning-based prediction model. Previous studies with conventional linear proportional hazard models or machine-learning models treated HLA information as binary data (matched or mismatched). However, the degree of HLA disparity may not be equivalent depending on combinations of specific HLA antigens and/or alleles between donor and recipient. For example, the difference between HLA-A02:01 and HLA-A02:02 may not always be the same as that between HLA-A02:01 and HLA-A11:01. We successfully imported raw HLA information in the CNN-based model by utilizing word2vec, a natural-language processing method. While this study did not identify novel HLA combinations that consistently alter the risk of developing aGVHD irrespective of patient background, visualization efforts using LIME enabled us to assess the contributions of HLA antigens or alleles of donors and recipients to the risk of aGVHD in individual cases. Machine-learning models that combine biological HLA information, including epitopes and molecular structures, in a larger cohort may provide further detailed information about the contributions of specific combinations of HLA antigens or alleles to aGVHD risk6.

In this study, we clarified the impact of factors other than HLA disparity on the risk of aGVHD using a CNN-based model. HLA mismatching is the most important risk factor for acute GVHD, but effects of other clinical factors on the risk of aGVHD have differed from report to report due to differences in patient characteristics4, and the contributions of these factors other than HLA disparity have not been fully evaluated. This study revealed contributions of clinical factors other than HLA mismatches to the risk of aGVHD. In this study, we found that there was a group even among patients transplanted from HLA-matched donors who were at a higher risk of developing aGVHD than those transplanted from HLA-mismatched donors and another group among those transplanted from HLA-mismatched donors who had an extremely low risk of severe aGVHD. While this study showed that individual patients had a different weight for each factor in the risk of developing acute GVHD, a comparison of patients who had HLA-matched and the highest prediction scores for aGVHD II–IV (matched highest group) and those who had HLA-mismatched and the lowest prediction scores (mismatched lowest group) revealed that matched highest group patients tended to be older, more male, worse performance status, have more frequent complications of major organ, use more RIC, and use less MTX and MMF for GVHD prophylaxis than the entire cohort, and that mismatched lowest group patients tended to be younger, use less peripheral blood stem cells as graft sources, and use less ATG than the entire cohort (Supplemental Data 6). Thus, prophylactic measures to reduce the risk of aGVHD should be optimized according to comprehensive prediction models that incorporate various clinical factors, rather than depending solely on HLA matching.

The present study revealed the utility of CNN as a prognostic tool for aGVHD. However, there are some limitations to this study that must be addressed. While the CNN model was designed to avoid researcher bias in variable settings, some of the variables were categorized into subgroups based on clinically established criteria. For example, we stratified pre-transplant disease conditions using disease risk. Another limitation is that our outcome measure, the incidence of aGVHD, was also treated as a binary variable in the CNN-based model that we used in this study. Information on the onset time for cases of aGVHD was not included in the process of model development. In this study, the onset of aGVHD is limited to a small window (usually 30–100 days after HSCT); therefore, the effect of ignoring information regarding the time of onset is probably suboptimal. Because biological HLA information was not included in this study, the risk of aGVHD can potentially be affected by combinations of HLA alleles that are different in notation, but are biologically homologous. In this study, we included as many variables accessible and consistently evaluable in the existing registry for the establishment of prediction models but might ignore the potential effects of unavailable parameters on the risk of acute GVHD. And the inclusion of parameters early after transplantation in addition to pre-transplant factors can improve the stratification power of the model, as previously reported34. While technical improvements are required to collect information on a larger number of parameters, incorporating more parameters, including variables with uncertain significance at present, into the machine learning model is beneficial to maximize the potential of machine learning. In this study, missing values regarding several variables were handled by the model, missingness can potentially affect the prediction. Overfitting is the conventionally discussed limitation in machine learning35, and our algorithm is not completely free of this limitation, even though we took measures to avoid it. In addition, ethnicity affects the incidence and severity of GVHD36. While the main architecture of our model can be applied to various different cohorts, tuning the model is required to apply this model in different cohorts. Therefore, further validation of the CNN-based model using different cohorts, including other ethnic groups, by using our approach as a proof-of-concept is needed. We note that alternative machine learning algorithms, such as random forest regression and recurrent neural networks, have seen increased application to problems with clinical practices in recent years, and maybe equally suited to CNN-based models, and the optimal machine learning approach should be further studied.

In conclusion, we developed a CNN-based prediction model for aGVHD after allogeneic HSCT using a nationwide transplant database in Japan, which incorporates comprehensive HLA information, excluding arbitrariness, as well as ensuring transparency of the calculation process. This prediction model revealed that the risk of aGVHD is determined not only by HLA disparity but also by detailed HLA information, as well as various clinical factors other than HLA. This study suggests that our CNN-based prediction model can be used to establish various prognostic predictive models in the field of HSCT, which is applicable in clinical practice.",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10188562/
Shouval et al. 2015,Prediction of Allogeneic Hematopoietic Stem-Cell Transplantation Mortality 100 Days After Transplantation Using a Machine Learning Algorithm: A European Group for Blood and Marrow Transplantation Acute Leukemia Working Party Retrospective Data Mining Study,"Purpose: Allogeneic hematopoietic stem-cell transplantation (HSCT) is potentially curative for acute leukemia (AL), but carries considerable risk. Machine learning algorithms, which are part of the data mining (DM) approach, may serve for transplantation-related mortality risk prediction.

Patients and methods:
Patients and methods: This work is a retrospective DM study on a cohort of 28,236 adult HSCT recipients from the AL registry of the European Group for Blood and Marrow Transplantation. The primary objective was prediction of overall mortality (OM) at 100 days after HSCT. Secondary objectives were estimation of nonrelapse mortality, leukemia-free survival, and overall survival at 2 years. Donor, recipient, and procedural characteristics were analyzed. The alternating decision tree machine learning algorithm was applied for model development on 70% of the data set and validated on the remaining data.

Results: OM prevalence at day 100 was 13.9% (n=3,936). Of the 20 variables considered, 10 were selected by the model for OM prediction, and several interactions were discovered. By using a logistic transformation function, the crude score was transformed into individual probabilities for 100-day OM (range, 3% to 68%). The model's discrimination for the primary objective performed better than the European Group for Blood and Marrow Transplantation score (area under the receiver operating characteristics curve, 0.701 v 0.646; P<.001). Calibration was excellent. Scores assigned were also predictive of secondary objectives.

Conclusion: The alternating decision tree model provides a robust tool for risk evaluation of patients with AL before HSCT, and is available online (http://bioinfo.lnx.biu.ac.il/∼bondi/web1.html). It is presented as a continuous probabilistic score for the prediction of day 100 OM, extending prediction to 2 years. The DM method has proved useful for clinical prediction in HSCT.","Eligibility of patients with AL for allo-HSCT is based on a risk-benefit assessment of the relapse risk versus transplantation risk.25 By applying the ADT algorithm, we have developed a novel prediction model on the basis of 10 variables for day 100 OM. Scores correlated with objectives, enabling an individual continuous probabilistic evaluation of the primary objective (ie, OM at day 100) and a discretized risk assessment of secondary objectives at 2 years (OS, NRM, and LFS).

Insights can be derived from the tree-like structure of the model and variable weights (Fig 1 and Table 2). Disease stage and performance status were strong outcome determinants, corroborating previous studies.26,27 Earlier years (2000 to 2003) were associated with a worse outcome, reflecting advances in the field.1 An advantage of the ADT is its ability to detect interactions. For instance, the effect of the interval between diagnosis and transplantation, with a cutoff of 142 days, had impact only for certain disease stages (ie, CR1 and advanced). Thus, specific characteristics of unique subpopulations were captured, and the cutoff set by the EBMT score of 1 year for all disease stages was refined.3 A threshold of 20 transplantations or more per year in matched unrelated donors was associated with better outcomes, again stressing the importance of center experience and accreditation.28–30 Not surprisingly, reduced-intensity conditioning was a favorable prognostic factor when compared with MAC in older patients (age ≥ 37 years), corroborating interactions between age and conditioning. Interestingly, age was not an independent risk variable. It seems that transplantation practice and patient selection have downgraded age importance with respect to outcome.31

The ADT algorithm was able to detect variables associated with the primary outcome, assign weights, and ignore redundancies (eg, the recipient-donor CMV serostatus combination was selected, whereas individual CMV status, donor or recipient, was excluded). Body mass index and cytogenetics may play a role as prognostic factors,5,32 but were not selected, possibly because of many missing values. Transplantation from a female donor to a male recipient has also been associated with mortality in previous studies,3 but was not selected in the current study, because it mainly affects late mortality. Differences in variable selection compared with previous allo-HSCT prognostic studies probably reflect different measures of predictive importance assessment. Models augment, rather than contradict, one another. Their integration may lead to improved predictive accuracy.

The EBMT score is a well-recognized tool for adjusting transplantation analysis. The ADT model showed improved discrimination, although relatively small, in comparison to the EBMT score (AUC, 0.701 v 0.646; P < .001). Nevertheless, one must keep in mind that the EBMT score was designed for prediction of long-term survival; thus, comparison with our score is not trivial, because primary end points differ. In addition, the ADT score allows a continuous and personalized risk assessment for day 100 OM, as opposed to other contemporary scores, which focus on identification of prognostic groups.3–5,7 Moreover, when contemplating a transplantation, one must take into account specific patient history (eg, the interval from diagnosis to transplantation does not have the same impact in CR1 or CR2).33 Such interactions are not necessarily captured by standard statistical models.

Stratifying OM risk at 100 days by collapsing score intervals into different risk groups would lead to loss of important clinical information. By providing a continuous measure for patient risk, we transformed the prediction problem from a classification task to a regression task, and we enhanced physician and patient understanding regarding expected transplantation hazard. Two-year outcomes, which have previously been shown to predict long-term survival,34 were estimated according to the score (by deciles) established for day 100 OM. Thus, potential use was extended, and factors predicting day 100 OM may be surrogates for long-term survival.

The ADT is a classification algorithm designed for handling binary end points, but not censored or continuous end points. Therefore, we focused on a short-term outcome, in a population in which loss to follow-up was lower than 5% and center effect is unlikely, because transplantation volume was not linked to patient loss (Data Supplement). Patients lost had some differing characteristics (Data Supplement); however, given their relatively small number, they are not likely to affect model performance. An important aspect of the current study is the introduction of an alternative approach for prediction model development, rather than comparison with the conventional approach. A DM method has been applied in fields such as communication and finance, and one can think of potential uses in HSCT, because it allows prediction of the outcome of interest without strong assumptions regarding the distribution of the variables and the regression model used.11,12 It is reassuring that the Cox and ADT models achieved similar discrimination, stressing the validity of the DM method with short-term transplantation data. Nevertheless, alternative methods should be explored for modeling long-term outcomes.35,36

This study has several limitations. First, it is a retrospective analysis susceptible to data selection and measurement biases.37 However, the registry analyzed reflects real world data, conveying contemporary practice.10 Second, validation was done on an internal data set, and external validation is warranted. Nonetheless, the many patients in the analysis, the use of 10-fold cross validation for training in addition to a separate validation set, and the model's excellent calibration, all greatly enhance validity and robustness. Moreover, despite lack of prediction model development guidelines, we adhered to strict methodologic principals.23 Third, in contrast to the EBMT and HCT-CI scores, which are not disease specific, our score applies only to ALs, which are a leading indication for allo-HSCT transplantation1; thus, targeting this patient population is reasonable. Still, the diagnosis variable had low predictive influence, suggesting that the score may be applicable to other diseases. Fourth, given the ADT model complexity, calculation of patient score is nontrivial, as opposed to the EBMT score.3 Therefore, we provided an online interface (http://bioinfo.lnx.biu.ac.il/∼bondi/web1.html) to enable easy calculation. Finally, the primary objective focused on short-term survival. Nevertheless, our model showed competence in predicting NRM, OM, and LFS at 2 years. In addition, the high rate of day 100 OM (13.9%) highlights its importance as a valid objective.

In conclusion, we present a machine learning–based prediction model for mortality after allo-HSCT. The model was developed using a DM approach and internally validated on a large data set with excellent calibration. It can be readily used online and provides a personalized estimation of day 100 OM risk and a discretized estimation of long-term outcomes, and at the same time reveals variables' interactions. The model's potential applications include pretransplantation risk assessment and stratification, patient counseling during informed consent sessions, and tailoring transplantation regimens or referring to alternative treatments according to transplantation risk. Predictive accuracy is still not optimal. Integration with the HCT-CI score, detailed data on modifiable therapeutic factors, and data on somatic mutations (eg, Fms-like tyrosine kinase 3 and Nucleophosmin 1) may further enhance predictive power and aid treatment personalization. Having demonstrating that the DM approach can be applied to the EBMT registry data, future studies must aim to make more precise predictions for long-term outcomes using the recent methods developed to manage censored data.35,36",https://ascopubs.org/doi/10.1200/JCO.2014.59.1339?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub%20%200pubmed
Lasserre et al. 2012,Predicting the outcome of renal transplantation,"Objective
Renal transplantation has
Renal transplantation has dramatically improved the survival rate of hemodialysis patients. However, with a growing proportion of marginal organs and improved immunosuppression, it is necessary to verify that the established allocation system, mostly based on human leukocyte antigen matching, still meets today's needs. The authors turn to machine-learning techniques to predict, from donor–recipient data, the estimated glomerular filtration rate (eGFR) of the recipient 1 year after transplantation.

Design
The patient's eGFR was predicted using donor–recipient characteristics available at the time of transplantation. Donors' data were obtained from Eurotransplant's database, while recipients' details were retrieved from Charité Campus Virchow-Klinikum's database. A total of 707 renal transplantations from cadaveric donors were included.

Measurements
Two separate datasets were created, taking features with <10% missing values for one and <50% missing values for the other. Four established regressors were run on both datasets, with and without feature selection.

Results
The authors obtained a Pearson correlation coefficient between predicted and real eGFR (COR) of 0.48. The best model for the dataset was a Gaussian support vector machine with recursive feature elimination on the more inclusive dataset. All results are available at http://transplant.molgen.mpg.de/.

Limitations
For now, missing values in the data must be predicted and filled in. The performance is not as high as hoped, but the dataset seems to be the main cause.

Conclusions
Predicting the outcome is possible with the dataset at hand (COR=0.48). Valuable features include age and creatinine levels of the donor, as well as sex and weight of the recipient.","The analysis presented here simulates the process of decision-making that takes place when an organ is allocated, and is based exclusively on data provided by Eurotransplant at the time of organ allocation. As a result, it is very similar to what happens in reality and could be implemented in a clinical setting without much effort. A few differences with a physician's decision can be found. For example, some factors most probably influencing allograft function (such as recipient presensitization to alloantigen) were not taken into account. Moreover, while commonly used in kidney studies, the eGFR is biased by the influence of muscle mass on creatinine production. Even though the MDRD formula for eGFR reduces this bias, it cannot be said whether the recipient's variables (age, sex, height and weight) have a stronger impact on allograft function or on muscle mass. Clinicians have to consider a large number of parameters that may carry conflicting information, and an automatic prediction based on the most relevant factors may be of help when taking the decision. We hope to have made this help more valuable by keeping the experimental setting as close as possible to the real situations experienced at hospital. Our study identified the donor's age as the most important factor on allograft function. This may directly influence medical decision-making, if allocation programs were to increase the impact of this feature.

We performed a thorough analysis of the data at hand using four established regressors and different subsets of features, and were able to build a regressor that achieved a 0.48 correlation on our dataset. First, we want to emphasize the great care that was taken to ensure the machine learning was sound. Every experiment was carried out using 10-fold cross-validation, that is, each reported performance is an average and comes with error bars, so the results are much more reliable in this study than in most of the literature. Moreover, data that were used for selecting features or for tuning parameters were never used for testing. Indeed, features and parameters were chosen using cross-validation on the training set only, so each reported performance range should reflect the real performance.

We would also like to point out the benefits of running such an extensive analysis. When data are scarce, models might not behave as expected and it is not obvious which regressor should perform best, so it is more informative to show all the results rather than to report the best performance only. Our particular case shows that, even if we could identify a superior model, there is actually not much difference between the various set-ups, indicating that non-linearity is not critical for this dataset. For example, F-G-SVM-RFE is not significantly better than LR on the robust dataset with FFS (COR=0.48 vs COR=0.45; one-tailed Steiger test29: p=0.08) or than G-SVM on the robust dataset with RFE (COR=0.48 vs COR=0.45; one-tailed Steiger test: p=0.394). The models with a similar performance to F-G-SVM-recursive feature elimination are highlighted in appendix section 9.

Choosing the best outcome parameter is critical for the design of such a study. In renal transplantation, a variety of outcome parameters are regularly used, such as acute rejection, DGF, graft survival and renal functions (estimated GFR or, more rarely, measured GFR). Here the GFR estimated with the MDRD formula was chosen, because this formula is the base for the classification of chronic kidney diseases, and is associated with morbidity and mortality in ESRD. Additionally, the eGFR is a valid surrogate parameter for long-term graft survival.

The correlation obtained between predicted and real eGFR is 0.48 (R2=0.23), and the scatter plot in figure 3 looks very encouraging. The analysis only includes information available at the time of surgery, which means a small subset of all the possible parameters. Indeed, after transplantation, the patient is subject to a large number of influencing factors, such as occurrence of acute rejection, patient's adherence level to therapy, adverse effects of immunosuppression, infections, and so on. A perfect performance therefore can not be achieved. Additionally, the analysis only includes the grafts that were accepted for transplantation. Since Charité rejects roughly 10–20% of kidneys (due to organ quality, safety reasons, etc), our dataset is a biased selection.

It is tricky to compare our results to others'. The methods used in the literature (linear models and NNs) are also investigated here, but the critical differences between the various contributions lie in the data and in the outcome parameter which is usually binary as opposed to real-valued. Moreover, most published results are unclear, it is often hard to find critical details (such as which samples were included, whether the results are reported on training or test data, how many samples were in each set, etc), and every author uses a different measure of performance. However, when recast into a classifier (see figure 3 or appendix section 7 for details), our model had an area under the ROC curve of 0.72±0.08, which is fairly close to that of Lin et al16 (0.73), but with much fewer samples. It also achieved an accuracy of 0.68±0.07, which is better than that of Shadabi et al15 (0.62). For transplantation failure detection (eGFR<45 ml/min), which is the usual point of view in this kind of application, it achieved a sensitivity of 0.51±0.08 and a specificity of 0.8±0.08, which is better than that of Brier et al14 (0.3/0.7).

Part of the interest of this work was to extract important features. The variables that came out as important are known to influence the eGFR after transplantation. While donor's age and creatinine are directly related to the donor's renal function before transplantation, the influence of the recipient's weight is less evident. Some data suggest an adverse impact of donor/recipient weight mismatch.30 Our analysis reveals that the age of the donor is by far the strongest factor (included in the data) for allograft function. This is consistent with earlier published data.28 The clinical relevance of this variable suggests paying special attention to it when adapting allocation strategies. The minor impact of the HLA mismatches, which used to be regarded as one of the most important factors, probably reflects the higher efficiency of modern immunosuppressive agents in preventing graft rejection.

The dataset is the source of two limitations, which leaves room for improvement and hope for the future. First, the information content seems quite low. Indeed, the age of the donor contains much of the predictive power, which is odd in itself. Consequently, the flexibility of the model becomes optional rather than necessary, and G-SVMs are only neck and neck with LR. Furthermore, some error bars are quite wide, showing that performance is sensitive to a particular subset of the data. High variance is usually a sign of insufficient amounts of data. This can be fixed, as time will provide more samples.

Another limitation of this analysis lies in the imputation method. For now, missing values are predicted with linear models using complete variables. In our case, imputation should not be too harmful, though, as the robust dataset contains at least 90% real values. However, a much better and more elegant way would be to design a generative probabilistic model that would suppress the need for imputation. There are too many features and too few samples to design such a model reliably just yet, but as datasets grow larger, and dependencies between variables are better understood, generative graphical models should lead the way.",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3277611/
Tong et al. 2020,Predicting Heart Rejection Using Histopathological Whole-Slide Imaging and Deep Neural Network with Dropout,"Cardiac allograft rejection is one major limitation for long-term survival for patients with heart transplants. The endomyocardial biopsy is one gold standard to screen heart rejection for patients that have heart transplantation. However, manual identification of heart rejection is expensive and time-consuming. With the development of imaging processing techniques and machine learning tools, automatic prediction of heart rejection using whole-slide images is one promising approach to improve the care of patients with heart transplants. In this paper, we first develop a histopathological whole-slide image processing pipeline to extract features automatically. Then, we construct deep neural networks with and without regularization and dropout to classify the patients into nonrejection and rejection respectively. Our results show that neural networks with regularization and dropout can significantly reduce overfitting and achieve more stable accuracies.","Based on the observation of extensive experiments, we conclude that neural networks with regularization and dropout can significantly reduce overfitting and achieve more stable accuracy compared to neural networks without regularization and dropout.

One limitation of
One limitation of this study is the small sample size. With only 43 whole-slide images available, training a comprehensive model with desired training, validations, and testing sets is difficult. Also, because of the small sample size, we are unable to perform multi-class classification. As a next step, we plan to include more patients under heart transplant and collect their whole slide images to enlarge the cohort of this study.

In this study, we demonstrated the feasibility of applying deep neural network for the prediction of heart rejection. However, optimization of the network configuration is yet to be further explored to improve the accuracy and reduce overfitting. To construct a deep neural network, we need to determine the number of layers, the number of perceptrons in each layer, activation function, and the cost function for optimization. And when we further apply regularization and dropout to the neural network, we need to decide the regularizations we want to add to the cost function and the probability of dropout in each layer. With all these factors taken into consideration, it’s especially difficult to find an optimal network for different situations. One future direction of this study is to establish a guideline for determining values of large number of hyperparameters.",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7324296/
Medved et al. 2017,Predicting the Outcome for Patients in a Heart Transplantation Queue using Deep Learning,"Heart transplantations have made it possible to
extend the median survival time to 12 years for patients with
end-stage heart diseases. This operation is unfortunately limited
by the availability of donor organs and patients have to wait on
average about 200 days in a waiting list before being operated.
This waiting time varies considerably across the patients. In
this paper, we studied the outcome for patients entering a
transplantation waiting list using deep learning techniques. We
implemented a model in the form of two-layer neural networks
and we predicted the outcome as still waiting, transplanted
or dead in the waiting list, at three different time points: 180
days, 365 days, and 730 days. As data source, we used the
United Network for Organ Sharing (UNOS) registry, where
we extracted adult patients (>17 years) from January 2000
to December 2011. We trained our model using the Keras
framework, and we report F1 macro scores of respectively
0.674, 0.680, and 0.680 compared to a baseline of 0.271. We
also applied a backward elimination procedure, using our
neural network, to extract the 10 most significant parameters
predicting the patient status for the three different time points.","The distribution of patient outcomes within the cohorts
is quite imbalanced, where transplanted is the outcome for
57-77% of the patients, during the chosen time periods.
We tried a simple baseline, where we classified all the
patient outcomes as the most frequent, see Table III for the
results. It produced quite good micro averaged values, mostly
because these metrics are biased towards the largest class, but
comparatively bad macro values.
The largest misclassification error in Figure 2 corresponds
to queueing as transplanted. This is probably because it is
hard to differentiate between the patients that were transplanted at a certain time point versus those that are still
waiting in the queue, based on the available features.
We carried out a backward elimination using our neural
network and the ten most contributing features is shown
in Table IV. This results in a decrease of only about 2%
(absolute difference) from the F1 macro score with all the
features, see Table V. This means that most of the predictive
power from the ANN comes from a few features. Neural
networks do a kind of feature selection naturally as part of the
model, weighing up more predictive features and weighing
down the less predictive. Because of this, feature search
for neural networks is usually not needed. But considering
it is hard to interpret the matrices produced by the ANN
model directly, we carried out a backward elimination to
approximate the features importance.
Fig. 2. Confusion matrix for 365 days time period.
The features shared by all of the three sets are: urgency
status 2, weight, height and body mass index (BMI). BMI can
be considered a feature transformation of weight and height
as BMI = weight × height2
, but it provided extra predictive
information over the constituent variables. A sufficiently
complex neural network could probably approximate this
transformation and therefore BMI would probably not be
needed.
Table VI shows some discrepancy between the number
of transplanted patients depending on having blood group
O. This can probably be explained by the fact that only
patients that are blood-group compatible with the donor are
transplanted. Even though type O is quite common, patients
of this group can only receive from donors from the same
blood group and can give to all other types.
A. Future Work
We did not have time to fully optimize the hyperparameters of the neural network and there are some variables
that are available that we did not include, both which could
produce better results.
We also plan to build a more advanced model based on
networks similar to those we described in this paper to be
able to estimate the probability the patient would die or
would be transplanted depending on the time s/he spent in
the waiting list",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8036766&casa_token=l0ufEk2M2GYAAAAA:i7OJSxtwpkZJDrLg5Yhvq0EbF8aFQ15iTrjG8I3TnFdIj6NzHvu0TF7hBahEGhtBllklsSk&tag=1
Yoon et al. 2018,Personalized survival predictions via Trees of Predictors: An application to cardiac transplantation,"Background
Risk prediction is crucial in many areas of medical practice, such as cardiac transplantation, but existing clinical risk-scoring methods have suboptimal performance. We develop a novel risk prediction algorithm and test its performance on the database of all patients who were registered for cardiac transplantation in the United States during 1985-2015.

Methods and findings
We develop a new, interpretable, methodology (ToPs: Trees of Predictors) built on the principle that specific predictive (survival) models should be used for specific clusters within the patient population. ToPs discovers these specific clusters and the specific predictive model that performs best for each cluster. In comparison with existing clinical risk scoring methods and state-of-the-art machine learning methods, our method provides significant improvements in survival predictions, both post- and pre-cardiac transplantation. For instance: in terms of 3-month survival post-transplantation, our method achieves AUC of 0.660; the best clinical risk scoring method (RSS) achieves 0.587. In terms of 3-year survival/mortality predictions post-transplantation (in comparison to RSS), holding specificity at 80.0%, our algorithm correctly predicts survival for 2,442 (14.0%) more patients (of 17,441 who actually survived); holding sensitivity at 80.0%, our algorithm correctly predicts mortality for 694 (13.0%) more patients (of 5,339 who did not survive). ToPs achieves similar improvements for other time horizons and for predictions pre-transplantation. ToPs discovers the most relevant features (covariates), uses available features to best advantage, and can adapt to changes in clinical practice.

Conclusions
We show that, in comparison with existing clinical risk-scoring methods and other machine learning methods, ToPs significantly improves survival predictions both post- and pre-cardiac transplantation. ToPs provides a more accurate, personalized approach to survival prediction that can benefit patients, clinicians, and policymakers in making clinical decisions and setting clinical policy. Because survival prediction is widely used in clinical decision-making across diseases and clinical specialties, the implications of our methods are far-reaching.","In this study, we develop a methodology for personalized prediction of survival for patients with advanced heart failure while on the wait-list and after heart transplantation. Our methods and associated findings are important because they outperform the clinical risk scores currently in use and also because they automate the discovery and application of cluster-specific predictive models and tune predictions to the specific patient for which the prediction is to be made. Moreover, our predictive model can be easily and automatically re-trained as clinical practice changes and new data becomes available. The clinical and public health implications of our findings are broad and include improved personalization of clinical assessments, optimization of decision making to allocate limited life-saving resources and potential for healthcare cost reduction across a range of clinical problems.

Key findings
We emphasize three key findings of our study:

Our method significantly outperforms existing clinical risk scores, familiar regression models, and state-of-the-art machine learning benchmarks in terms of accurate prediction of survival on the wait-list and post-transplantation.
This improvement in performance has clinical significance: our method correctly predicts both survival and mortality for a larger number of patients.
There is substantial heterogeneity—both across clusters of patients and across different time horizons. Our method captures this heterogeneity far better than other methods.
The sources of gains for ToPs/R
As the results show, ToPs/R achieves large performance improvements over current clinical risk-scoring models. It does so by explicitly addressing the weaknesses of these clinical models:

Tops/R addresses the heterogeneity of the population(s) by identifying sub-populations (clusters) and the specific predictive models that are best suited to prediction in each sub-population. Tops/R makes predictions that are personalized to the features of the patient (or patient and donor).
Tops/R addresses the interactions between features by using non-linear predictive models for those sub-populations (clusters) in which the interactions between features are important.
Tops/R addresses the heterogeneity across time horizons by constructing different predictions for different time horizons.
Using different predictive models on different clusters means that we allow different features to have different importance and to interact differently for different clusters of patients. And because we make the choice of predictive models endogenously and optimally, and update as clinical practice changes and new data becomes available we are letting the data tell us which choices to make.

As we have already noted, ToPs/R finds the most relevant features and uses those features more effectively.

Non-proportional and heterogeneous hazards models
The existing clinical risk scores do not produce individual survival curves. Clinicians using these scores infer survival curves by clustering patients with similar scores and constructing Kaplan-Meier curves from the actual survival times for these clusters. Our method produces individual survival curves by interpolating the predictions for 3 months, 1 year, 3 years and 10 years. Our approach can be viewed as providing a non-parametric survival model. In terms of hazard functions, our method could be interpreted as forming clusters, assigning a model to each cluster, and then aggregating those models to construct a non-proportional hazard function. In contrast to familiar approaches [29, 30] our approach learns which clusters to create, which models to assign and how to aggregate these models.

Cox Regression does produce individual survival curves. However, because Cox Regression assumes that relative hazard rates are constant over time, the survival curves produced by Cox Regression for two different patients cannot cross: if the survival probability for patient 1 is greater than that of patient 2 at a time horizon of 3 months it will also be greater at every time horizon. However, actual survival curves may cross [31, 32]. Our method allows for this—and this is a virtue because it reflects the fact that the features and interactions that are most important for survival at 3 months are different from the features and interactions that are most important for survival at longer horizons. (See Fig 8).

Clinical support
Our work provides support for clinical decision making in real time. By using our user-friendly website http://medianetlab.ee.ucla.edu/ToPs_TransplantSurvival and entering relevant features, the clinician can obtain immediate predictions for a specific patient, including survival on the wait-list, the impact of an LVAD (if relevant), and benefit of transplantation (if relevant). All of this can be done at the desk of the clinician or the bedside of the patient, in no more time than is currently required to access the clinical risk scores, and with much greater accuracy (and confidence).

Risk scoring and usability
Our analysis shows that our method provides significantly greater predictive power for survival while on the wait-list and post-transplantation. However, more research in actual practice is required. This success of our method in the setting of cardiac transplantation suggests it may have wide applicability and usability for risk prognosis and diagnosis for other medical conditions and diseases. The methodology developed here can also be applied in other settings, in particular to transplantation of other organs, such as kidneys; we leave this for future work. Such wider applications may benefit from further refinements of our method.

Limitations
The results of this study carry limitations associated with the quality of the source data and the amount of missing data. Moreover, is not possible to integrate the effect of changes in treatment protocols until sufficient time has elapsed and sufficient new data becomes available. However, once sufficient data has become available, our method can integrate the new data to provide improved predictions. (See Tables ​Tables44 and ​and5,5, where we use data from the first 5 years of the LVAD era to make more accurate predictions for the following 5 years).

Our method produces risk scores and survival prediction both for patients both pre- and post-transplantation. We have provided extensive performance comparisons with the post-transplantation predictions of clinical and machine learning methods and with the pre-transplantation predictions of machine learning risk methods. As we have noted, it is unfortunately it is not possible to provide meaningful pre-transplantation comparisons with existing clinical methods such as HFSS, SHFM, and MAGGIC [11, 14, 15] because UNOS does not collect many of the features on which these clinical risk scores rely, such as ejection fraction, etc.",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5874060/
Zhou et al. 2021,Prediction of 1-year mortality after heart transplantation using machine learning approaches: A single-center study from China,"Background
Heart transplantation (HTx) remains the gold-standard treatment for end-stage heart failure. The aim of this study was to establish a risk-prediction model for assessing prognosis of HTx using machine-learning approach.

Methods
Consecutive recipients of orthotopic HTx at our institute between January 1st, 2015 and December 31st, 2018 were included in this study. The primary outcome was 1-year mortality. Least absolute shrinkage and selection operator method was used to select variables and seven different machine-learning approaches were employed to develop the risk-prediction model. Bootstrap method was used for model validation. Shapley Additive exPlanations (SHAP) method was used for model interpretation.

Results
381 recipients were included with average age of 43.783 years old. Albumin, recipient age and left atrium diameter ranked top three most important variables that affected the 1-year mortality of HTx. Other important variables included red blood cell, hemoglobin, lymphocyte%, smoking history, use of lyophilized rhBNP, use of Levosimendan, hypertension, cardiac surgery history, malignancy and endotracheal intubation history. Random Forest (RF) model achieved the best area under curves (AUC) of 0.801 and gradient boosting machine (GBM) showed the best sensitivity of 0.271. SHAP method was introduced to display the RF model's predicting processes of “survival” or “death” in individual level.

Conclusions
We established the risk-prediction model for postoperative prognosis of HTx patients by using machine learning method and demonstrated that the RF model performed the highest discrimination with the largest AUC when validated. This prediction model could help to recognize high-risk HTx recipients, provide personalized therapy plan and reduce organ wastage.","With the rapid development of HTx in China, the imbalance between the organ demand and donation is gradually emerging. A feasible prognosis prediction model would help clinicians distinguish high-risk patients and optimize organ allocation strategies, making patients benefit most from HTx. Herein, for the first time, the present study assessed the predicting performance of different machine learning algorithms for overall 1-year mortality after HTx in China. The major novel findings are as follows: (I) RF model performed optimal predictive power with the best AUC of 0.801 and GBM model owned slightly lower AUC of 0.786. However, sensitivity was rather low for all the models with GBM achieving the best sensitivity of 0.271; (II) Top-five most importance variables related to the short-term prognosis of HTx included ALB, age, LA, RBC and Hb level of recipient; (III) Shapley-value shed light into model explanation among variables and displayed the prediction process in individual level. To our knowledge, our study presented the first 1-year survival prediction model of HTx for Chinese population using machine learning approach, and introduced Shapley-value to the field of HTx for the first time to interpretate and present the process of prediction.

Of all the establishing approaches, machine learning has been an up-and-coming method as they showed more advantages in finding inherent correlations of complex and high-dimension data. Machine learning has been widely used in the field of clinical diagnosis and prognosis prediction. Compared with traditional regression analysis, machine learning methods, especially ensemble-based RF and GBM are more resist to overfitting [9,10]. Only a few studies focusing on predicting prognosis of HTx using machine-learning approaches have been published in the literature yet. Medved et al. [11] applied machine learning to compare the predictive performance of the existing International Heart Transplantation Survival Algorithm (IHTSA) and Index for Mortality Prediction After Cardiac Transplantation (IMPACT) for 1-year survival after HTx, and found that the IHTSA model (AUC = 0.654) showed superior power than the IMPACT model (AUC = 0.608). Miller and colleagues [12] used RF, ANN and classification and regression trees (CARF) to predict 1-, 3- and 5-year mortality after pediatric HTx and found RF performed best with AUC of 0.72 for 1-year mortality. However, sensitivity was poor across these models. Based on the International Society of Heart and Lung Transplant (ISHLT) registry data, Agasthi et al. [13] developed a risk prediction model with the help of a GBM model. The model performed well in predicting 5-year mortality and graft failure, with AUC reaching 0.717 and 0.716, respectively. Significant variables with strong correlation with mortality were identified including recipient and donor age and BMI, bilirubin, renal function, history of mechanical ventilation, dialysis, diabetes and so on [13,14].

In the present study, we compared seven commonly used machine learning approaches in predicting 1-year mortality after HTx. In general, as an unbalanced task, most machine learning algorithms tend to perform like a naïve guess. For example, SVM model predict all cases in test data negative and did not make any positive prediction. In this situation, SVM showed no prediction ability and hence should not be applied for HTx patients. RF model outperformed other models in the study with accuracy of 0.828, AUC of 0.801, sensitivity of 0.268 and specificity of 0.927. In this case, the RF model reached decent performance in the external validation while the AUC of other reported models [12,15] were mostly between 0.6 and 0.7. On the other hand, however, as short time death is one of the most primary negative outcomes after HTx which we should prevent in any cases, a model that can make more accurate positive predictions would be preferable. The sensitivity of RF model was 0.268, indicating that only 26.8% of positive patients in the test data set would be found by RF model, which is barely satisfactory.

As the development of electronic medical record system, the application of machine-learning methods for prognosis-predicting and decision-making is getting extended. However, many models like XGBoost are the black box models which we cannot know the exact contribution of each feature to the decision for a specific case. Interpretability of model can be defined as the extent to which a human can understand the cause of the machine learning model's prediction [16]. The higher the interpretability of the model, the easier it will be for clinicians to understand the model's behavior and trust the model's conclusion, so as to make appropriate clinical decisions in the best interests of the patient [17]. With regard to this, we introduced the Shapley-value to provide explanations for the RF model. The SHAP method is a Game Theory-based method, within which the individual features act as players in a prediction task and the Shapley-value helps us fairly distribute the prediction performance among the features [5]. SHAP method possesses three main properties [[17], [18], [19]]: (1) Local Accuracy, (2) Missingness, (3) Consistency. Moreover, SHAP method are better aligned with human intuition and could support explanation of multiple model type including tree-based models, neural network and ensemble models.

By using SHAP method, we could shine light into the black box of machine learning models. According to the feature importance based on Shapley-values, higher value of ALB and complete blood counts (CBC) reduced the risk of early mortality after HTx, whereas older recipient age and larger LA diameter, use of Levosimendan and lyophilized rhBNP, hypertension, cardiac surgery history and endotracheal intubation increased the risk of death. Younger age and higher ALB and CBC value reflected better preoperative general conditions and nutritional status, while hypertension, surgery and intubation showed poor physical condition and increased the complexity of surgical process. Enlarged left atrium and using higher dose of Levosimendan and lyophilized rhBNP before operation implied more severe cardiac dysfunction, and thus related to higher risk of death. It must be stated that smoking history was unexpectedly figured out as a protective variable for mortality, which was inconsistent with common sense and other researches [20,21]. In fact, our previous study [22] also found smoking history as a protective variable using Cox proportional hazard model. We think this surprising result may be due to the characteristics of our sample and need further verified by large-sample, multi-center studies.

Collectively, the present study proposed the first short-term prognosis prediction model for HTx based on Chinese population using machine learning approach. We demonstrated that the RF model performed the highest discrimination with the largest AUC when validated. Albumin, diameter of left atrium and recipient age ranked the top three most significant features that exert influence to the 1-year survival of HTx. Under the combined influence of these variables, an individual was predicted as “survival” or “death”. As shown in the Fig. 5, with the help of Shapley-value, a doctor could easily and intuitively understand how the machine learning model makes the decision, and consequently trust and apply the model for clinical decision-making. This prediction model may help to recognize high-risk HTx recipients, optimize preoperative management in clinical practice and reduce organ wastage. It should be noted that the precision of positive predictions has yet to be improved. Further study is warranted in multicenter with a large sample to establish a HTx prognosis prediction model with more sufficient validity and reliability.

4.1. Limitation
This study was not devoid of limitations. Compared with traditional prediction model, machine learning method is particularly good at efficiently processing of “big data” sources, exploiting the complex, nonlinear relationships in a comprehensive and unbiased fashion [23]. Therefore, it is conceivable that the model performance largely depends on sample size of the original data. The sample size in the current study was relatively small from a machine learning perspective, especially the number of positive events (1-year mortality in this case), and might be partially responsible for the poor sensitivity of the prediction model. Furthermore, the retrospective clinical dataset was restricted by accuracy of data entry and variables collected, and potential recall bias and data incompleteness for a significant proportion of the patients would inevitably exist. This might blunt phenotyping of complex patients and attenuate the predictive ability of machine learning techniques.",https://www.sciencedirect.com/science/article/pii/S0167527321011694
Agasthi et al. 2020,Machine learning helps predict long-term mortality and graft failure in patients undergoing heart transplant,"Objective
We aimed to develop a risk prediction model using a machine learning to predict survival and graft failure (GF) 5 years after orthotopic heart transplant (OHT).

Methods
Using the International
Using the International Society of Heart and Lung Transplant (ISHLT) registry data, we analyzed 15,236 patients who underwent OHT from January 2005 to December 2009. 342 variables were extracted and used to develop a risk prediction model utilizing a gradient-boosted machine (GBM) model to predict the risk of GF and mortality 5 years after hospital discharge. After excluding variables missing at least 50% of the observations and variables with near zero variance, 87 variables were included in the GBM model. Ten fold cross-validation repeated 5 times was used to estimate the model’s external performance and optimize the hyperparameters simultaneously. Area under the receiver operator characteristic curve (AUC) for the GBM model was calculated for survival and GF 5 years post-OHT.

Results
The median duration of follow-up was 5 years. The mortality and GF 5 years post-OHT were 27.3% (n = 4161) and 28.1% (n = 4276), respectively. The AUC to predict 5-year mortality and GF is 0.717 (95% CI 0.696–0.737) and 0.716 (95% CI 0.696–0.736), respectively. Length of stay, recipient and donor age, recipient and donor body mass index, and ischemic time had the highest relative influence in predicting 5-year mortality and graft failure.

Conclusion
The GBM model has a good accuracy to predict 5-year mortality and graft failure post-OHT.
","In this paper, we present a powerful analysis tool for predictive evaluation of patients prior to OHT using a complex machine learning algorithm. This analysis included more extensive, updated recipient, and donor information as compared with previous models [6, 7, 15,16,17,18]. In addition, this model was used to not only predict long-term mortality after OHT, but also used to predict the likelihood of graft failure after 5 years, another clinically significant outcome in this patient population which has not been as extensively studied in previous models [15]. Definition of graft failure in this study is the same as used for reporting to the ISHLT database; that is: any cause of decompensated failure of the LV or RV or both due to any of the following: primary failure in the immediate postoperative period, hyperacute, acute or chronic rejection, graft infection, recurrent disease, chronic allograft vasculopathy (CAV) or any other cause [5]. Use of this new model can accurately predict graft failure and 5-year mortality prior to transplant based on hundreds of pre- and post-transplant variables. This study further solidifies the notion that non-linear ANN models can be used to predict mortality and outcomes with high accuracy [8].

In terms of predictive strength and clinical importance, the AUC is the best tool for describing a model’s performance [7, 19, 20]. Our model, with an AUC of 0.717 and 0.716 for 5-year mortality and graft failure, respectively, predicts outcomes with great accuracy and improves on all previous models of estimating post-transplant success. In addition, this model included more variables and updated recipient and donor information as previously mentioned, giving it greater predictive value overall. Previous models, with the exception of the recent IHTSA model, have attempted to predict short-term outcomes, namely 1-year mortality. Our model has extended on these previous algorithms with high-accuracy outcome prediction for mortality and graft failure 5 years after OHT.

The most important variable for predicting mortality and graft failure after 5 years in our model was hospital length of stay, which differs from previous studies in which donor age was reported as being the most significant factor [6, 7, 21]. Increasing length of hospital stay increased the risk of 5-year mortality and graft failure about three times more than any other variable measured in the study. Donor age, previously reported to be the most important variable in predicting post-transplant mortality, was also shown to be a significant variable in predicting mortality and graft failure in our model; however, this particular variable was found to have less relative influence than several others discussed below namely: recipient age and ischemic time. Due to the fact that hospital length of stay is difficult to modify and cannot be predicted prior to transplant, other variables of importance were more significant for selection criterion to maximize transplant success prior to allograft according to our model. However, any variable known to affect the length of a recipient’s stay should be optimized to increase the likelihood of graft and patient survival at 5 years.

Second to length of stay, in terms of predictive effect, was recipient age prior to transplant. This finding is unsurprising given that the older a patient is before transplant the less likely they are to survive after 5 years and the more comorbidities the patient is likely to have. In previous studies, it was shown that there was a significant difference in mortality outcomes in patients of different ages, such as in the IHTSA study which noted a cutoff of age 38 having the greatest difference [7]. The contributing factors to worsened survival and graft function at 5 years after transplant for increasing age are likely multimodal and complex, but also intuitive. Another finding related to age was the increased accuracy of the model for patients with an age > 60 years. Given that a large number of patients needing OHT fall into this age group this is encouraging data that supports use of predictive models. Speculating on why predictability suffers for other age groups below 60 years of age is difficult, but may be due to the effect of different variables or unmeasured variables in younger patients or with congenital heart disease.

Use of anti-rejection immunosuppressive regimen prior to discharge was the next most influential variable in terms of predicting graft failure after 5 years. However, this was shown to have less of an effect on 5-year mortality prediction than graft failure. The reason for this is unclear, it may be that a more potent regimen of anti-rejection therapy increases the likelihood of graft success but due to toxicity also increases overall mortality, more analysis of this specific finding may be warranted.

Prolonged ischemic time was reported to be significant in our model, which has previously been noted in other models [6, 22,23,24]. In the present study, prolonged ischemic time was found to have the third and fourth highest weight in predicting 5-year mortality and graft failure, respectively. Prolonged ischemic time has been reported numerous times to worsen short-term graft function and can occasionally lead to short-term graft failure and morbidity [2, 5, 6, 22,23,24].

The importance of body size was reiterated in our model. Donor BMI and recipient BMI increased the risk of 5-year mortality significantly. Likewise, the risk of graft failure at 5 years was increased by these same variables. This is consistent with previously published studies [6, 7, 25]. No effect from gender differences was seen in our model as opposed to previous models [7, 26].

Lastly, two more recipient variables were found to influence graft failure and mortality prediction at 5 years; pre-transplant creatinine and bilirubin. This is also not surprising, as creatinine and bilirubin are common surrogates for overall kidney and liver function, respectively, it is conceivable that recipients with baseline impaired end organ function may do worse in long-term outcome post-transplant and thus increase rates of mortality and graft failure. The effects of creatinine, BUN to creatinine ratio, and bilirubin have been reported previously in the literature [6, 15, 17].

In terms of generalizability, this model’s use of the ISHLT database offers excellent accuracy for a broad range of patient populations. There was no subset of regional data for a specific cohort, but the results may be applied to patients that are demographically similar to the derivation cohort in particular. Previous models that have used this same data set have split validation results according to temporal and regional variation and found similar results across these variations [6, 7].

Similar to the IHTSA algorithm, use of a large number of variables (in this case, 87 precludes easy hand calculation by the clinician and thus computer-based prediction is necessary. This model could potentially be utilized in conjunction with existing donor and recipient registries, such as the United Network for Organ Sharing (UNOS) for optimizing donor and recipient matching for improved 5-year outcomes. In theory, adding a donor to the UNOS registry would automatically enter their many variables and find potential matches that are used to calculate a 5-year mortality and graft failure score using the algorithm. The algorithm could also provide useful information for alterable recipient and donor characteristics that may improve transplant outcomes, such as optimizing variables that affect hospital length of stay as mentioned above.

Limitations
There are several limitations evident in this study. Use of a data registry for retrospective analysis has opportunity for error; missing data, lack of standardization across regions, alterations in standards of care across time eras for example, though this last example is mitigated by the use of data from only between 2005 and 2009. These limitations have been well described [6, 7]. The registry does not collect data of duration of time of recipients on the waiting list or deaths on the waiting list. This data have become less important over time given the improvement in heart failure treatment and left-ventricular assist devices to prolong life prior to OHT. No data were collected on racial differences and its effect on outcomes. One limit of the present study is that there was a significant difference in the level of total bilirubin between the DC and the VC groups with the VC having a significantly higher average level of total bilirubin. Thus, in the VC, the expected outcomes would be worse given that higher bilirubin was shown to significantly influence the risk of mortality and graft failure at 5 years. Despite this, the purpose of this study is to show the predictive value of the machine learning algorithm and although the bilirubin levels in the validation cohort were higher, the algorithm takes this into account when attempting to predict outcomes. One of the benefits of using a machine learning system to predict outcomes is the ability to model non-linear relationships and is not dependent on linear associations to make accurate predictions.

Conclusion
We present an updated and more extensive approach of analyzing and stratifying success of OHT by use of a machine learning algorithm. This model analyzed 87 variables in a non-linear fashion to accurately predict 5-year mortality and graft failure in an individual instance of OHT and provided the top 10 most influential variables for predicting 5-year mortality and graft failure. This model would likely function as a predictive algorithm to estimate risk of 5-year mortality and graft failure in a given donor–recipient match and could also be used to risk stratify several matches on a given donor registry such as UNOS. Use of this model or a model like this could aid the transplant services in confidently selecting matches for transplant that will likely succeed and may increase the number of transplants that are both performed and succeed overall. Identification of patients at a higher risk of mortality and graft failure at the time of discharge can help physicians modify existing post-transplant protocols to emphasize close outpatient clinic follow-up, intensive cardiac rehabilitation, aggressive titration of immunosuppressants, frequent endomyocardial biopsies to evaluate for rejection, emphasize importance of long-term social support, and patient education on medication compliance in an attempt in minimize risk of mortality and graft failure. Given the high relative importance of length of hospital stay in predicting mortality and graft failure post-OHT, we plan to explore and identify predictors of longer length of stay post-OHT in subsequent studies.",https://link.springer.com/article/10.1007/s11748-020-01375-6#Sec12
Dolatsara et al. 2020,A two-stage machine learning framework to predict heart transplantation survival probabilities over time with a monotonic probability constraint,"The overarching goal of this paper is to develop a modeling framework that can be used to obtain personalized, data-driven and monotonically constrained probability curves. This research is motivated by the important problem of improving the predictions for organ transplantation outcomes, which can inform updates made to organ allocation protocols, post-transplantation care pathways, and clinical resource utilization. In pursuit of our overarching goal and motivating problem, we propose a novel two-stage machine learning-based framework for obtaining monotonic probabilities over time. The first stage uses the standard approach of using independent machine learning models to predict transplantation outcomes for each time-period of interest. In the second stage, we calibrate the survival probabilities over time using isotonic regression. To show the utility of our framework, we applied it on a national registry of U.S. heart transplants from 1987 to 2016. The first stage produces an area under the receiver operating curve (AUC) between 0.60 and 0.71 for years 1–10. While the 1-year prediction AUC result is comparable to the reported results in the literature, our 10-year AUC of 0.70 is higher than the current state-of-the-art results. More importantly, we show that the application of isotonic regression to calibrate the survival probabilities for each patient over the 10-year period guarantees monotonicity, while capitalizing on the data-driven and individualized nature of machine learning models. To promote future research, our code and analysis are publicly available on GitHub. Furthermore, we created a web app titled “H-TOP: Heart Transplantation Outcome Predictor” to encourage practical applications.","6.1. Contextualizing the prediction results based on the transplantation literature
Prior to discussing our prediction results, it is important to compare the consistency between our models' important variables and those found in the literature. The majority of the literature, where ML methods were used for predicting transplantation outcomes, examined one [e.g., see 7,14,[44], [45], [46]] or at most three time periods [8,11]. Thus, from our analysis, we can gain additional insights into the contribution of a variable to predicting graft rejection, short-term, medium-term, and long-term survival. This is not possible for studies that have focused on one time-period. Moreover, the study of Dag et al. [8] did not include 1-month acute transplant rejection period and the study of Yoon et al. [11] has only investigated 1-, 3-, and 5- year survival outcomes. To illustrate whether our important predictors have been reported in the previous literature, let us consider Table 6 where we provide a summary of whether the variables selected for all 11 time-periods through our LASSO implementation have been selected in [7,8,14]. We have selected these three references as a representative sample of the literature since (a) Dag et al. [8] examined three time periods, (b) Dag et al. [7] focused on long-term survival outcomes, and (c) Medved et al. [14] used state-of-the-art deep learning methods for short-term survival predictions. From Table 6, there are three observations to be made. First, the donor's age and the recipient's medical condition at transplant were the two variables where our model agreed with the three papers. Second, our model was the only methodology, where the body mass index for the recipient was found important. Third, all other variables were also selected by at least one paper. Note that the differences between the models can be attributed to (a) differences in data cleaning procedures, (b) utilization of different variable selection methodologies, (c) differences in sample size of dataset used for training (e.g., other papers might not have used the entire UNOS dataset and/or did not include results up to 2016), and/or (d) correlation among one or more independent variables in the UNOS dataset. As we show below, our prediction results are consistent with the literature, and thus, the differences shown in Table 6 may not be significant from a prediction perspective.

If we focus on the prediction results in Stage I, our model is (at least) comparable to the prediction outcomes reported in the literature that utilize the UNOS dataset. For example, for the 1-year survival outcomes, our utilized model results in specificity, sensitivity, AUC and G − Mean values of 0.590, 0.522, 0.581, and 0.555, respectively (as shown in Table 4). Our AUC value, for the full, non-censored data in Fig. 4, of 0.614 is similar to the AUC values reported in Dag et al. [8] (0.624), Yoon et al. [11] (0.641 for their best model), Medved et al. [14] (0.61 for their IMPACT model which was developed using the UNOS dataset), Miller et al. [45] (who reported AUC values of 0.613, 0.59, and 0.663 for three different transplantation eras), and Villela et al. [46] (whose auto-ML model had an AUC of 0.66). However, the results in Dag et al. [8] can be considered much worse in practice since their average sensitivity was only 0.128 for their validation data sets, which means that their model was not able to predict deaths for the first year. None of the other cited works reported sensitivity or G − Mean results and thus, it is not clear if their models suffer from such a performance discrepancy as well. To further demonstrate the performance of our model, let us consider the 10-year end-point of our prediction period, our model results in specificity, sensitivity, AUC and G − Mean values of 0.533, 0.752, 0.702, and 0.633, respectively (as shown in Table 4). Our AUC value is better than the 0.631 reported by Yoon et al. [11]. Perhaps more importantly, none of the cited papers presented a detailed description of their data cleaning procedure, which makes reproducing their work difficult in practice. Thus, while our prediction results are comparable to the published literature, we believe they can be more useful in practice since we make our code and analysis publicly available through our R Markdown document.

In the second stage of our application, we constructed a monotonically decreasing survival probability curve for each patient using isotonic regression. Overall, the application of isotonic regression did not affect the overall shape of the curve as evident from Fig. 5, Fig. 7. However, the results are medically compelling due to the non-increasing survival probabilities. Furthermore, the individualized nature of the predictions brings us a step closer to personalized medicine [15].

6.2. Contributions to heart transplantation research and practice
In Section 6.1, we have shown that the stage I predictive performance of the proposed framework is in line or better than the results reported in the literature. The proposed framework has several merits as it (a) provides personalized results to a given “match case”, (b) is guaranteed to be monotonic, (c) is explainable due to the use of logistic regression, (d) is easy to implement through the provided code, and (e) is flexible since it can easily incorporate any data preparation, variable selection and ML modeling procedure in Stage I of the framework. Furthermore, as stated in Section 2, by examining the UNOS data we are able to
•
develop a flexible framework, which can result in accurate and precise heart transplantation outcome predictions;

•
provide UNOS with a benchmark to compare the expected outcomes from a match, which can inform the policies and algorithms governing phase II of the transplantation process; and

•
inform the protocols prescribed by medical professionals in phases IV and V of the transplantation process based on the estimated survival risk.


The first aim is achieved based on the obtained results from the application of our framework. To accomplish the second and third aims, we have created a web application (app) titled “Heart Transplantation Outcome Predictor (H-TOP)” that allows UNOS and transplant teams to utilize the proposed framework on prospective “match cases”. Our approach can be used on prospective cases since we (i) do not include any variables that UNOS no longer collects, (ii) do not use transplant year as a predictor, and (iii) present a comprehensive approach that can be used for data preparation, which is illustrated by our detailed data cleaning, encoding and imputation procedures.

We have created and deployed the app using the R “shiny” package. The app can be accessed at http://dataviz.miamioh.edu/Heart-Transplant/monotonic/. To facilitate using the app, it contains an instructional video showing how one can use the app to make predictions. The app allows users to utilize one of two scenarios for inputting data (a) manual entry, where dropdown menus are provided for categorical variables and text inputs are provided for numeric variables; or (b) a CSV upload, where practitioners should load a CSV file based on a provided template. Once the data is inputted, the app (a) performs basic checks on the quality of the imported data, (b) implements the trained logistic regression model for the 11 time points, (c) calibrates the obtained survival probability curves using isotonic regression, and (d) provides a table and a plot of the calibrated survival probabilities. An overview of the app's workflow is presented in Fig. 8. In our estimation, the app is user-friendly and it requires no machine learning background for usage. 
While we have applied our framework to the problem of predicting heart transplantation outcomes, our proposed two-stage framework presents a generic and flexible methodology that can be applied to any multi-period prediction application where ML methods are used and monotonic outcomes are required. In Section 1, we have presented several decision-making applications that can benefit from our proposed framework. In the context of ML methodologies, our framework represents an extension to the use of “hybrid” methodologies. Existing “hybrid” methods typically incorporate two or more approaches for the purposes of improving prediction accuracy [e.g., see 48]. However, our proposed framework introduces the idea of using a hybrid approach to constrain the predictions from the initial ML modeling stage, which can influence the development of other “hybrid” approaches where different criteria for calibration are to be enforced.

6.4. Limitations and future research opportunities
The limitations of our study provides opportunities for future research. First, the goal of this study was to present a methodological machine learning based framework that uses isotonic regression to calibrate and guarantee the monotonicity of outcome probabilities over time. While we examined a large number of modeling approaches, we did not attempt to optimize the prediction performance (e.g., through a detailed investigation of parameter tuning or through examining more complex data imputation schemes). We examined these scenarios to show that even with the “best” modeling approach, non-monotone probability curves can be obtained. Future research could optimize the findings of our research to broader decision making domains. Second, we examined a limited number of few machine learning models and their parameterizations in Stage I. Thus, we cannot guarantee that logistic regression will be superior when compared to ensemble or hybrid models that were not considered in our analysis. Future research could investigate the use of methodologies excluded from this study. The selection of LR could have also differed if other data cleaning procedures, time-samples of the UNOS data, and criteria for balancing predictive accuracy and interpretation (which will differ from one application to another) were used. Third, we did not consider how to optimize the calibration of the survival curve obtained from Stage I. We have only considered the use of isotonic regression for obtaining monotonic survival probability curves. Future work can examine the use of other approaches to achieve monotonicity, while having a smoother function (e.g., the use of an exponential curve).

Specific to heart transplantation application, there are some additional issues that need to be emphasized. The secondary and retrospective nature of our analysis from a registry database means that we cannot account for “the quality of the source data, the number of missing data, and the lack of standardization associated with multicenter studies (such as different immunosuppressive regimens and different matching criteria)” [14,p. 6]. In addition, we have no control over whether the variables included in our model will continue to be collected in the future. Exclusion of these variables from the UNOS data collection protocol will require future researcher to retrain our models. By sharing our detailed code for data cleaning, we explicitly show how we handled missing data, observations where we identified data quality issues, and how we removed all variables that had an ending data per the time of our data acquisition. While our efforts cannot guarantee suitability for future changes in UNOS's data collection protocol, it allows researchers to easily build on our analysis if needed. The results depicted in Fig. 5, Fig. 7, where our approach consistently underestimates the survival probabilities (when compared to observed rates) for the first four years. There are two important considerations that need to be emphasized (a) it is unclear whether the previous methods in the literature would have similar performance characteristics since this type of analysis has not been done before (only metrics for the dichotomous classification are typically reported); and (b) the utility of our approach in practice is not diminished from this limitation. Specifically, from a practitioner's perspective knowing with a high degree of certainty that the prediction probabilities at 5+ years from transplant is accurate, is sufficient to obtain a lower bound on the survival probabilities for the earlier time-periods. That being said, future research should examine how to reduce the bias in the predictions for ≤4 years.",https://www.sciencedirect.com/science/article/pii/S0167923620301184?via%3Dihub
Oztekin et al. 2011,Development of a structural equation modeling-based decision tree methodology for the analysis of lung transplantations,"Lung transplantation has
Lung transplantation has a vital role among all organ transplant procedures since it is the only accepted treatment for the end-stage pulmonary failure. There have been several research attempts to model the performance of lung transplants. Yet, these early studies either lack model predictive capability by relying on strong statistical assumptions or provide adequate predictive capability but suffer from less interpretability to the medical professionals. The proposed method described in this paper is focused on overcoming these limitations by providing a structural equation modeling-based decision tree construction procedure for lung transplant performance evaluation. Specifically, partial least squares-based path modeling algorithm is used for the structural equation modeling part. The proposed method is validated through a US nation-wide dataset obtained from United Network for Organ Sharing (UNOS). The results are promising in terms of both prediction and interpretation capabilities, and are superior to the existing techniques. Hence, we assert that a decision support system, which is based on the proposed method, can bridge the knowledge-gap between the large amount of available data and betterment of the lung transplantation procedures.","Medical experts are trained to reason “medically” whereas data miners place more importance on model's performance, e.g. prediction accuracy. Since research designs vary in both areas, such differences grow even more later on [52]. In addition to this conflict of interests, some machine learning methods (e.g. neural networks) are powerful in terms of predictive ability, yet they are black boxes. Namely, they give no (or very limited) explanation of the “reasoning” used behind the scene to achieve high predictive accuracy. Therefore, their acceptance by the medical experts is limited [52]. Our proposed method balances this trade-off and overcomes aforementioned issues that have been faced in collaboration between medical experts and data miners. Our integrated method with structural equation modeling and decision trees is proven to be fairly capable in terms of predictive accuracy with an R2 value of 0.68 as well as interpretability with a much lower computational time requirement compared to Bayesian neural networks-based USM technique. Proposed method not only covers nonlinear relations among various variables but also brings more explanation into the scene to make the lung transplant procedure more understandable and transparent in terms of variables used for modeling and prediction. It provides concise rules which can be visualized in the final decision tree.

A main future research stream of this study might be to create a decision support system equipped with a user-friendly frontend and a near-transparent backend application which would help medical professionals to deal with voluminous data more effectively and efficiently (e.g. providing reliable results in a very short time period). Having entered hundreds of predictive variables into the system, a medical professional can then visualize the summarized information through our proposed method and make decisions for the upcoming transplants. In other words, having a potential donor organ on hand a medical expert could make a rapid decision as to which potential recipient patient to allocate the donor organ. As explained in this paper, this could be achieved by utilizing the most critical variables which are related to recipient's and donor's profiles and their match level information as opposed to using couple of hundreds of variables.",https://www.sciencedirect.com/science/article/pii/S016792361000237X
Dag et al. 2016,A probabilistic data-driven framework for scoring the preoperative recipient-donor heart transplant survival,"Recent research has shown that data mining models can accurately predict the outcome of a heart transplant based on predictors that include patient and donor's health/demographics. These models have not been adopted in practice, however, since they did not: a) consider the interactions between the explanatory variables; b) provide a patient's specific risk of survival (reported results have been primarily deterministic); and c) offer an automated decision tool that can provide some data-driven insights to practitioners. In this study, we attempt to overcome these three limitations through the use of Bayesian Belief Networks (BBN). The proposed BBN framework is comprised of four phases. In the first two phases, the data is preprocessed, and a candidate set of predictors is generated based on employing several variable selection methods. The third phase involves the addition of medically relevant variables to the list. In phase four, the BBN model is applied. The results show that the proposed BBN method provides similar predictive performance to the best approaches in the literature. More importantly, our method provides novel information on the interactions among the predictors and the conditional probability of survival for a given set of relevant donor–recipient characteristics. We offer U.S. practitioners a decision support tool that presents an individualized survival score based on our BBN model (and the UNOS dataset).","The main objectives of this paper were to develop a mathematical model to identify patient-specific survival risk scores and the interactions between the explanatory variables. To achieve these goals, we have proposed a BBN framework that is based on variables selected from data-mining models, statistical models, and the literature. Our approach is used to investigate a large, feature-rich dataset obtained from UNOS, containing all recorded information on heart transplant operations that were performed in the U.S. between October 1, 1987 and December 31, 2012. In our analysis, we have addressed the following questions:
a)
What predictive factors contribute to the outcome of a heart transplant for 9-year survival?

b)
Can the interactions between these predictors be quantified and visualized?

c)
How can data-driven methods be used to construct a probabilistic patient-specific failure risk score based on the values of the relevant preoperative predictors?

The innovation in our framework is based on our ability to address the latter two questions, which were not previously examined within the heart transplantation research community. In addition, we provide a decision support tool that can be used by practitioners to obtain a 9-year survival probability for a patient given attributes extracted from the dead donor.

The results obtained through using the proposed framework indicates that employing a comprehensive variable selection procedure leads to an excellent predictive model as illustrated by our examination of 6 metrics. More importantly, the information obtained using a probabilistic (graphical) approach in the final step has provided the relations among the important predictors as well as the patient-specific failure probabilities. These significant outcomes can be insightful to medical practitioners and policy-makers since they may provide with the ability to conduct prospective studies.

A number of limitations in our analysis need to be mentioned. First, there are several variables in the UNOS dataset whose collection has started after October, 1987. For instance, the UNOS team started to collect the information about the variable called “Alchohol_heavy_donor” on June 30th, 2004. In our data cleaning procedure, most of these variables had to be deleted since these variables have excessive number of missing records regarding to the dates in which they were not considered. A potential remedy was to solve such a problem would be using only the last 10 years of the dataset; however, this would have prevented us from having a large enough dataset to study the factors predicting the long-term survival. A second limitation is that we have only compared two data mining approaches in our variable selection procedure approaches (ANN and DT) for analyses. The results could have been different if other approaches are investigated. Since it is computationally infeasible to apply every possible variable selection model, we chose to use these common approaches. Similarly, the use of a different information fusion method may have resulted in different findings and combinations of important variables. With that being said, we believe that our results are satisfactory when compared to commonly reported values in the data mining literature.

In summary, this paper proposed a novel framework to predict the long-term survival outcomes after heart transplantation. We have shown that extracting the predictive factors from multiple source helps to increase the performance of the prediction model used in this study. Our approach can be easily extended to other organs, especially since UNOS provides information on other organ transplants. Finally, it should be noted that the analysis presented in this paper may inform new prospective studies if the findings are interpreted in detail by medical practitioners.",https://www.sciencedirect.com/science/article/pii/S0167923616300185
Hoot & Aronsky 2005,Using Bayesian Networks to Predict Survival of Liver Transplant Patients,"The relative scarcity of grafts available for liver transplantation highlights the need to identify patients likely to have good outcomes after treatment. We used transplant information from the United Network for Organ Sharing database to construct a Bayesian network model to predict 90-day graft survival. The final model incorporated a set of 29 pre-transplant variables, and it achieved performance, as measured by area under the receiver operating characteristic curve, of 0.674 by cross-validation and 0.681 on an independent validation set. The results showed a positive predictive value of 91%, while the negative predictive value was lower at 30%. With additional refinement and validation, our model may be useful as an adjunct to clinical experience in identifying patients most likely to have good outcomes following liver transplantation.","A Bayesian network model may be constructed using domain expertise to predict outcomes following liver transplantation. Our network for predicting 90-day graft survival performed modestly well, according to the AUC. Performance was found to be highly consistent between the training and independent validation sets. Model performance was also examined using a fixed threshold that yielded a sensitivity level of 95%, and the resulting specificity level was low. The model showed a very good positive predictive value (91%), whereas the negative predictive value was relatively low. We interpreted this to mean that the model performs very well at the task of identifying good survivors. However, the model does not perform well at recognizing poor survival candidates for liver transplantation.

Of noteworthy mention is the fact that the UNOS instituted a major allocation policy change in February, 2002, by switching from a ranking of four sickness categories – Status 1, 2A, 2B, and 3 – to using a continuous scale called the Model for End Stage Liver Disease (MELD). This policy change closely coincided with the division between our main dataset from 2000–2001 and our independent validation set from 2002. This observation may explain why differences were seen in certain descriptive statistics as noted above. Regardless, since performance was very consistent between both study groups despite the policy change, this suggests that our model may generalize well.

The performance of our model measured by the AUC appears to compare well with previous Cox regression models in the literature. [4–6] To conclude whether there is a significant performance difference between our model and others would require well-controlled validation of different models using the same independent dataset. Nevertheless, our present research sheds new light on the field of modeling liver transplant outcomes in a few ways. First, to our knowledge Bayesian networks have not previously been applied to the problem of modeling liver transplant outcomes. The use of this technique may better lend itself to high model complexity than Cox regression does, as evidenced by the 29 predictors included in our Bayesian network model. Another benefit to this modeling technique, as compared with Cox regression, is that Bayesian analysis is tolerant to missing values such that no imputation is necessary. Moreover, by examining the positive and negative predictive value of the model, we found that the difficult part of the problem lies not in identifying good survivors, but in identifying poor survivors. Lastly, our model incorporates some interactions between important predictors, and while it is possible to explicitly specify variable interactions in a Cox regression model, none of the previously mentioned studies have included them.

The greatest practical value in a liver transplant survival model lies in its ability to make predictions at or before the time of transplantation. Parmanto and Doyle showed that it is possible to use neural networks to develop a highly accurate liver transplant survival model if some early post-transplant information is included in the analysis. [16] Moreover, the performance increases with each additional day of post-transplant information that is made available to the model. This suggests that some factors from the surgery or the early post-operative course have an important effect on survival. Markmann et al. identified three intraoperative factors that predict poor outcomes following transplantation. [17] However, the utility of a post-transplant model is mitigated by its inability to make predictions at the time of decision-making.

Performance in predicting liver transplant outcomes may also be limited by the quality of the data available for analysis. As mentioned previously, the UNOS database has been noted to contain some errors or inconsistencies. These errors are perhaps due to variation in how different transplant centers record and maintain data. We attempted to compensate for this with the careful data verification scheme described above. Despite our efforts, the data quality may remain a limitation.

Moreover, there is a selection bias inherent in the UNOS data, because each patient is selected for transplantation not randomly, but according to a specific allocation policy. Those candidates deemed by clinicians to be too sick for transplantation are removed from the waiting list. Thus, the patients most likely to do poorly following transplantation may never receive a graft, and thus are not represented in the UNOS transplant database. The lack of this information may explain why our model can easily recognize good outcomes following transplantation, but it has difficulty predicting poor outcomes.

Our future research will focus on constructing a Bayesian network that incorporates both pre-transplant information as well as some early post-transplant information. The model can be trained with joint probability distribution governing all nodes in the network, even if inferences are made by entering findings only into pre-transplant nodes. The presence of this additional information in the network may help improve predictive power.

Go to:",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1560677/
Ashfaq et al. 2023,Survival analysis for pediatric heart transplant patients using a novel machine learning algorithm: A UNOS analysis,"Background
Impact of pretransplantation
Impact of pretransplantation risk factors on mortality in the first year after heart transplantation remains largely unknown. Using machine learning algorithms, we selected clinically relevant identifiers that could predict 1-year mortality after pediatric heart transplantation.

Methods
Data were obtained from the United Network for Organ Sharing Database for years 2010-2020 for patients 0-17 years receiving their first heart transplant (N = 4150). Features were selected using subject experts and literature review. Scikit-Learn, Scikit-Survival, and Tensorflow were used. A train:test split of 70:30 was used. N-repeated k-fold validation was performed (N = 5, k = 5). Seven models were tested, Hyperparameter tuning performed using Bayesian optimization and the concordance index (C-index) was used for model assessment.

Results
A C-index above 0.6 for test data was considered acceptable for survival analysis models. C-indices obtained were 0.60 (Cox proportional hazards), 0.61 (Cox with elastic net), 0.64 (gradient boosting), 0.64 (support vector machine), 0.68 (random forest), 0.66 (component gradient boosting), and 0.54 (survival trees). Machine learning models show an improvement over the traditional Cox proportional hazards model, with random forest performing the best on the test set. Analysis of the feature importance for the gradient boosted model found that the top 5 features were the most recent serum total bilirubin, the travel distance from the transplant center, the patient body mass index, the deceased donor terminal Serum glutamic pyruvic transaminase/Alanine transaminase (SGPT/ALT), and the donor PCO2.

Conclusions
Combination of machine learning and expert-based methodology of selecting predictors of survival for pediatric heart transplantation provides a reasonable prediction of 1- and 3-year survival outcomes. SHapley Additive exPlanations can be an effective tool for modeling and visualizing nonlinear interactions.","Contemporary heart transplantation risk scores demonstrate variable accuracy in predicting post-transplantation outcomes.25 In our study, we combined ML algorithm and feature selection by clinical experts to achieve high predictability for survival. We found that all our performance models were able to predict survival reasonably well at both 1- and 3-year marks and describe the application of SHAP values to interpret predictions.

For example, the Scientific Registry of Transplant Recipients utilizes conventional hierarchical and Bayesian prediction modeling to assist transplant programs in patient selection. While this has helped to improve pre- and post-transplant patient and graft survival, the disadvantages are that Bayesian modeling can be computationally intensive and relies on prior subjective knowledge.26 There has been a growing interest in using machine learning to predict transplant outcomes, but the performance has been variable and rarely utilized in the pediatric patient population.7, 27, 28 Naruka et al29 performed a systematic review of 13 ML studies and suggested that Artificial Intelligence (AI) and ML are more accurate in predicting graft failure and mortality than traditional scoring systems and conventional regression analysis. In their study, major predictors of graft failure and mortality identified in the ML models were length of hospital stay, immunosuppressive regimen, recipient's age, congenital heart disease, and organ ischemia time. While the majority of the papers in their study discussed the use of ML models in accurately predicting mortality and survival post-transplantation, a few discussed models which predicted the risk of rejection pre-transplant and predicting waitlist mortality. This underscores the potential impact of ML and AI in heart transplantation management.

All MI ML models had an AUC over 0.7 for the first 2 years, with only the SME model failing to reach this threshold. Out of the 4 models, the MI + SME model achieved this metric as far out as 4 years. All four models performed well within the first year and 3 of the models performed well within the first 2 years. As expected, the time-dependent AUC decreases over time, with a large decline at the 10-year mark. This is likely due to the gradual reduction in the number of samples available in the model. As time progresses, fewer samples become available for training as patients die or are lost to follow up. A decrease for all models is observed at the 5-year mark, which may indicate a reduction in the number of samples available to the model or events caused by factors not adequately captured by the UNOS database that far out. Additionally, it is important to note that the models used only information available pretransplant. Because important post-transplant predictors are not included, these results may also be subject to a “performance ceiling.” Earlier studies applying ML algorithms to 1 year heart transplantation outcomes in combined adult and pediatric populations have reported AUCs of 0.59 to 0.66 for the best-performing models, while studies examining 5-year outcomes have reported AUCs of 0.6 to 0.67.6, 27, 30 Most of these studies used UNOS data as well but had a broader time range, some going as far back as 19853, 7, 27, 31 which could introduce bias by not sufficiently accounting for changes in donor listing, waitlisting criteria, evolution in mechanical circulatory support technology, or changes in post-transplant management. Furthermore, since these were combined adult and pediatric data, it is challenging to translate those findings specifically for the pediatric cohort.

We have demonstrated in this study that application of the SHAP value framework has the potential to generate novel insights of significant clinical relevance. As demonstrated in Figures 2 and 3, local feature importance can be used to determine feature importance for subset of populations or even for an individual patient prediction. When comparing patients in the presence of a ventilator or ECMO, the contribution of each feature is different, and changes with respect to the feature being studied. For example, if a patient is on ECMO, features that become important are mechanical ventilation, serum bilirubin, inotropic support, and functional status among others. However, if a patient is not on ECMO, then the order of importance of these factors changes and ethnicity becomes a major factor in predicting survival. Insights from SHAP plots may help supplement clinical intuition for survival prediction, supplement the development of more nuanced risk systems, and assist in identifying inflection points in predicting risk for pediatric patients undergoing heart transplantation.

Our study has several limitations. First, even with the use of ML models, the usual limitations of registry analyses apply. For example, factors such as selection bias and variation in data collection methods will limit the validity of casual inference. Second, statistical significance is not readily interpretable. We also note that SHAP values illustrate relationships specific to a given model and dataset and cannot be used to infer causality or underlying biological processes. Finally, we note that our analyses are based on a single dataset obtained from the UNOS Registry. Even though it is the largest available dataset, validation in other datasets, ideally prospective, will be needed for generalizability of our findings.

Future directions include efforts to validate the model prospectively for clinical deployment. This consists of 2 phases. The first is a single center prospective study and provides insight into what model fine-tuning may be required to be sufficiently performant for local deployment. The second would be prospective testing on the UNOS dataset. Testing against future data collected through UNOS would measure the ability of the model to be used on similar cohorts.

Our study demonstrates that a combination of ML and expert-based methodology of selecting predictors of survival for pediatric heart transplantation is superior to the traditional approaches and provides a reasonable prediction of 1- and 3-year survival outcomes. Application of SHAP can be an effective tool for modeling and visualizing nonlinear interactions and provide meaningful insight into risk stratification.",https://www.sciencedirect.com/science/article/pii/S1053249823018995
Lisboa et al. 2022,Enhanced survival prediction using explainable artificial intelligence in heart transplantation,"The most limiting factor in heart transplantation is the lack of donor organs. With enhanced prediction of outcome, it may be possible to increase the life-years from the organs that become available. Applications of machine learning to tabular data, typical of clinical decision support, pose the practical question of interpretation, which has technical and potential ethical implications. In particular, there is an issue of principle about the predictability of complex data and whether this is inherent in the data or strongly dependent on the choice of machine learning model, leading to the so-called accuracy-interpretability trade-off. We model 1-year mortality in heart transplantation data with a self-explaining neural network, which is benchmarked against a deep learning model on the same development data, in an external validation study with two data sets: (1) UNOS transplants in 2017–2018 (n = 4750) for which the self-explaining and deep learning models are comparable in their AUROC 0.628 [0.602,0.654] cf. 0.635 [0.609,0.662] and (2) Scandinavian transplants during 1997–2018 (n = 2293), showing good calibration with AUROCs of 0.626 [0.588,0.665] and 0.634 [0.570, 0.698], respectively, with and without missing data (n = 982). This shows that for tabular data, predictive models can be transparent and capture important nonlinearities, retaining full predictive performance.","The development of a new risk calculator for decision support to be used in the clinical care of patients is complex. It is not only about achieving better performance compared to previous decision support tools but also about increasing the understanding and importance of the risk factors involved. To achieve this, the new decision support must be interpretable, which can be difficult when it is based on machine learning. The results from this study show that an interpretable machine learning model is competitive in performance compared to previously developed deep learning models tested on the same data. Although the two IHTSA models have the best overall score in terms of AUCROC, the interpretable PRN-Lasso model was within their confidence intervals, unlike the classical interpretable model, IMPACT. Furthermore, the PRN-Lasso model was better calibrated on the external validation data compared to IHTSA and IMPACT.

Considering the confidence intervals, all the machine learning models in Table 3, PRN-Lasso, EBM, IHTSA and IHTSA recalibrated, are comparable in performance. This supports the view that the limiting factor in overall predictive power is noise; therefore, any model capable of fitting the structure in the data should result in similar performance to the optimum achievable. A particular strength of interpretable machine learning models is the clear link between the input variables and the model prediction.

Why is it important to be able to risk-stratify a patient before transplantation? One reason may be to identify the best combination of risk factors for recipients/donors to optimize the outcome. This also includes avoiding combinations that result in a poor result to make the best use of the available organs15. It is also important to understand how the input factors on which the decision support is based arrive at the result. A potential disadvantage of using risk scoring models is that if the model focuses on clinical criteria and is difficult to interpret, any ethically negative effects that are built into the model might not be identified16. Furthermore, it can be difficult for a model to identify risk factors from small subgroups. Such negative effects are important to identify early in the development phase17. After the model is implemented, it can be difficult to change it. This emphasises the importance of a model being interpretable as well as the importance of evaluation before clinical implementation.

There are currently approximately 15 different algorithms that predict survival after heart transplantation. They all have a relatively poor ability to discriminate i.e. identify an individual patient with increased risk when used on external data18. The most cited risk stratification model is the IMPACT score—an interpretable algorithm published in 2011 by Weiss et al.13. The scoring model is developed using logistic regression analysis where only recipient characteristics are included. Compared with the IHTSA model developed with artificial neural networks (ANNs), the ability to discriminate the IMPACT score is inferior. On the other hand, IMPACT is easier to interpret than IHTSA. The PRN-Lasso model, which is based on both recipient and donor variables, has the same discriminatory ability as IHTSA and is interpretable as IMPACT. Tree of prediction (TOP), an interpretable model developed using regression trees, showed similar discrimination results in an internal validation, such as PRN-Lasso19. However, no external validation is available for this model.

To assess how well decision support can work in clinical practice, it should be appropriately validated. Simple cross-validation is usually not sufficient, and a separate test cohort should be used. This was recently demonstrated in a study evaluating the predictive power of popular ML and statistical algorithms. The authors use different validation techniques to assess the accuracy of the prediction of 1-year mortality. The results showed that temporal validation similar to that used in this study is important due to the temporal changes in the sample of patients and donors20.

In medical fields where the patient population is not very large, clinical databases are often required, which gather information from many different centres. Thus, external validation alone is not sufficient, but local validation is equally important. This is due to differences in local processes and protocols that may change the measurement of individual covariates, as well as factors that change over time. The external validation on the Scandia data set shows how well our model trained on UNOS data applies. This indicates that there is a high level of consistency between the measurement of the predictive variables and for the overall mortality trend over time.

In the PRN-Lasso and IHTSA models, the year of transplantation was one of the most significant variables. It is well known that 1-year survival has improved continuously over the last two decades, as have the temporal changes in the sample of patients and donors20,21. Interactions between different risk factors may also vary over time, as Hsich et al. recently demonstrated using random forest2. If the model does not take this into account, performance will be impaired, as we can see for the IMPACT model in this study. IMPACT was significantly less well calibrated in the external validation cohort than the PRN-Lasso model. The fact that the calibration of the IHTSA models also does not work so well is because the variable is divided into time eras instead of years and the last era in the development cohort was 2010. The PRN-Lasso model treats transplant years as a continuous variable and can therefore extrapolate risk forward in time. The transplant year acts as a recalibration to take into account the gradual decrease in mortality from year to year.

The five most significant variables in the PRN model are also among the top 10 most important variables in the IHTSA model in addition to ischemia12. The IMPACT model lacks donor-related variables, which are two of the top five13. Both donor age and ischemia are known predictors of 1-year survival. Their efficacy persisted despite strong regularization at the LASSO selection in this study, especially donor age, which has proven to be the most important variable in other prediction models. In the early 1970s, Griepp et al. argued that an ideal heart donor should be younger than 30 years, as confirmed in the present study22. A donor age above 30 years carries an increased risk of mortality. Unlike a noninterpretable model, the PRN-Lasso model clearly shows that the risk increase for donor age is not linear but is protective at young ages. Our model also shows that the risk increase caused by ischemia comes at 3 h and not at 4 h, which is an old clinical rule21. The fact that ischemia is not ranked as highly in the IHSTA model is probably because this model was not primarily optimised for 1-year survival but for long-term survival time.

When quantifying the univariate and bivariate effects modelled, the input factors whose weights add together to make the final prediction may be checked against clinical expertise. The importance of recipient age for survival varies in the literature. The biological difficulty for the algorithm to model is that the risk of total mortality increases with age, as for other diseases. The risk of transplant-related complications increases with age2,12. For example, the ISHLT report shows that mortality risk for 1 year increased after the age of 5523. In addition, the risk of rejection is reduced. The immune system is at its most active at younger ages. The findings from this study show an increased mortality rate for older recipients but a protective effect for those between 35 and 55 years of age (Fig. 2a). In contrast to the IHTSA model, we see that the risk again increases for the youngest recipients. However, it is interesting to note that the predicted risk is reduced for younger recipients if the patient is diagnosed with ICM. While this bivariate effect is consistent with crude filtering of the data as noted in the results section, it may be difficult to explain biologically. ICM is traditionally a diagnosis associated with higher mortality, especially compared to NICM. An explanation might be that the aetiology of ICM is often different for younger patients compared to older ones. For an older patient, ICM is usually associated with general atherosclerotic disease, whereas for a younger patient, the cause is different24. It may be in the form of a coronary artery anomaly, meaning that the problem is completely bypassed when the heart is replaced.

The results of this study have limitations associated with the retrospective analysis of a registry database, the quality of source data, and the lack of standardization associated with multi-centre studies (such as various immunosuppressive regimens and various matching criteria), as has been described previously25. The existence of missing values is another problem that can affect the result. We used a multiple imputation technique to be able to use the entire material and avoid selection bias if one would instead choose to remove patients or variables with missing values. However, this means that the importance of variables with many missing values is more difficult to quantify.

In this study, we present an interpretable sparse algorithm with the same classification performance as today's most well-known deep learning models. In particular, the PRN model is considered to be self-explanatory because the impact of the input variables on the output is transparent. By dividing a complex multivariate algorithm into elements with lower dimensionality, the elements of this additive model are easy to read and can be interpreted by clinicians. Although interpretability is better, the low level of discrimination still persists i.e. low predictive power at the individual level. However, this is a known problem when modelling tabular data where the data are largely preprocessed. Classification performance measured by the AUROC is maintained for the interpretable mode compared with others on the same data. However, the clear weights associated with individual effects enable a detailed discussion to be had about the clinical plausibility of the inputs to the model, which is where the discussion of risk models for high-stakes applications needs to go. This level of transparency provides both a rigorous explanation for predictions made with respect to individual decisions and a diagnostic route for potential failure modes present in the model. Both are essential elements in deciding when the model is and is not safe to use.",https://www.nature.com/articles/s41598-022-23817-2#Sec6
Divard et al. 2022,Comparison of artificial intelligence and human-based prediction and stratification of the risk of long-term kidney allograft failure,"Background
Clinical decisions are mainly driven by the ability of physicians to apply risk stratification to patients. However, this task is difficult as it requires complex integration of numerous parameters and is impacted by patient heterogeneity. We sought to evaluate the ability of transplant physicians to predict the risk of long-term allograft failure and compare them to a validated artificial intelligence (AI) prediction algorithm.

Methods
We randomly selected 400 kidney transplant recipients from a qualified dataset of 4000 patients. For each patient, 44 features routinely collected during the first-year post-transplant were compiled in an electronic health record (EHR). We enrolled 9 transplant physicians at various career stages. At 1-year post-transplant, they blindly predicted the long-term graft survival with probabilities for each patient. Their predictions were compared with those of a validated prediction system (iBox). We assessed the determinants of each physician’s prediction using a random forest survival model.

Results
Among the 400 patients included, 84 graft failures occurred at 7 years post-evaluation. The iBox system demonstrates the best predictive performance with a discrimination of 0.79 and a median calibration error of 5.79%, while physicians tend to overestimate the risk of graft failure. Physicians’ risk predictions show wide heterogeneity with a moderate intraclass correlation of 0.58. The determinants of physicians’ prediction are disparate, with poor agreement regardless of their clinical experience.

Conclusions
This study shows the overall limited performance and consistency of physicians to predict the risk of long-term graft failure, demonstrated by the superior performances of the iBox. This study supports the use of a companion tool to help physicians in their prognostic judgement and decision-making in clinical care.","In this study, we investigated the prediction performances of an AI system and 9 transplant physicians with distinct clinical experiences, in assessing the risk of long-term allograft failure after kidney transplant. We showed that the iBox had better prediction performances than physicians, regardless of their experience. We also showed that physicians had limited performance, reproducibility, and consistency to predict the risk of long-term allograft failure.

Interestingly, few physicians had a discrimination close to the iBox but they all tended to overestimate the risk of allograft failure while the iBox showed a good discrimination and a strong, stable calibration. The predicted risk corresponds to the actual outcome for a large combination of predictor values. In addition, the predicted probabilities of long-term allograft failure were highly heterogeneous between physician estimates, while the iBox is stable.

We further supported this argument by ranking the feature importance for both the iBox and physicians. The physicians demonstrated a high heterogeneity in the choice of features that best predict the risk of long-term allograft failure. This result was not influenced by clinical experience, underscoring the possibility that this heterogeneity may be present in all physicians regardless of their experience. Overall, we demonstrated that physicians estimated that some key features independently associated with allograft failure described in the literature and included in the iBox were not, in their professional opinion, the most relevant driving forces6,30,31.

Therefore, as one given patient has one given risk of losing the allograft according to a spectrum of parameters, this disparity demonstrates that even if one physician may sometimes accurately predict the risk of a patient, other physicians are unlikely to have the same accuracy. This can lead to heterogeneity of practices for the same patient between physicians with potential invasive examinations or unnecessary treatments without benefit to the patient. Better predicting kidney allograft survival can help physicians improve risk stratification with reinforced surveillance for patients at high risk.

Overall, these findings illustrate that the iBox can inform physicians’ prognostic judgment and therefore decision-making and monitoring. As such, the iBox is a promising companion tool in daily transplant practice.

Kidney transplantation is a health care field representative of the quest for precision medicine over the past two decades32. Transplant physicians are overwhelmed with increasing data that are subject to many changes in definition and evaluation. For instance, the international Banff classification of allograft pathology has been updated every two years since 1991, making the interpretation of histological lesions increasingly complex33,34. Furthermore, to detect anti-HLA donor-specific antibodies, the Luminex single antigen bead assay technique is used worldwide, but remains difficult to interpret for physicians due to the use of different cut-offs and interpretations between laboratories25,35. Together, these ongoing changes represent a diverse knowledge that requires, for a physician, a long experience in transplant care and research to be correctly understood and integrated. Therefore in this context, the iBox, which suffers from less bias associated with memory and computation capability than humans, is likely to be of valuable assistance in transplant care.

More generally, this study reinforces the effort already made by researchers to compare machines to humans in the diagnosis or prognosis based on clinical data. This effort has often been focused on how machines could outperform physicians for image classification and disease diagnosis, but also more recently in patient prognostication of short-term outcomes36,37,38,39. However, to the best of our knowledge, this study is one of the first to compare long-term outcome predictions from a validated integrative prognostication system to physician predictions using EHR.

Despite its superior prediction performances, the iBox system will not replace physicians. The value of the iBox is its integration of a large spectrum of parameters from miscellaneous sources highly associated with the risk of long-term allograft failure in kidney transplantation9. However, it does not integrate the complexity of the physician-patient relationship, which involves many subtleties that contribute to decision-making. Further, additional specific data such as complications related to immunosuppressive treatment and events like cancer and infections have an important influence on clinical decisions. They are however not considered by the iBox system, although it may indirectly integrate the consequences of these events. Therefore, even though physicians predict with lower accuracy the risk of long-term allograft failure, they also have a large overview of the patient that cannot be currently reached by the machine. Thus, instead of opposing these two perspectives, the iBox should be considered as a companion tool that helps the physician in the evaluation of the patient, and thereby may serve as a support decision-making tool.
",https://www.nature.com/articles/s43856-022-00201-9#Sec20
Nilsson et al. 2015,The International Heart Transplant Survival Algorithm (IHTSA): A New Model to Improve Organ Sharing and Survival,"Background
Heart transplantation is
Heart transplantation is life saving for patients with end-stage heart disease. However, a number of factors influence how well recipients and donor organs tolerate this procedure. The main objective of this study was to develop and validate a flexible risk model for prediction of survival after heart transplantation using the largest transplant registry in the world.

Methods and Findings
We developed a flexible, non-linear artificial neural networks model (IHTSA) and classification and regression tree to comprehensively evaluate the impact of recipient-donor variables on survival over time. We analyzed 56,625 heart-transplanted adult patients, corresponding to 294,719 patient-years. We compared the discrimination power with three existing scoring models, donor risk index (DRI), risk-stratification score (RSS) and index for mortality prediction after cardiac transplantation (IMPACT). The accuracy of the model was excellent (C-index 0.600 [95% CI: 0.595–0.604]) with predicted versus actual 1-year, 5-year and 10-year survival rates of 83.7% versus 82.6%, 71.4% – 70.8%, and 54.8% – 54.3% in the derivation cohort; 83.7% versus 82.8%, 71.5% – 71.1%, and 54.9% – 53.8% in the internal validation cohort; and 84.5% versus 84.4%, 72.9% – 75.6%, and 57.5% – 57.5% in the external validation cohort. The IHTSA model showed superior or similar discrimination in all of the cohorts. The receiver operating characteristic area under the curve to predict one-year mortality was for the IHTSA: 0.650 (95% CI: 0.640–0.655), DRI 0.56 (95% CI: 0.56–0.57), RSS 0.61 (95% CI: 0.60–0.61), and IMPACT 0.61 (0.61–0.62), respectively. The decision-tree showed that recipients matched to a donor younger than 38 years had additional expected median survival time of 2.8 years. Furthermore, the number of suitable donors could be increased by up to 22%.

Conclusions
We show that the IHTSA model can be used to predict both short-term and long-term mortality with high accuracy globally. The model also estimates the expected benefit to the individual patient.","In this paper, we present the first international survival prediction model for heart-transplanted patients. By using such a model, we can make an improved outcome prediction based on the characteristics of both the recipient and donor. The findings from this study show that a flexible non-linear ANN model can be used to predict both short- and long-term mortality with higher accuracy. Furthermore, we demonstrate that a survival prediction based organ sharing system may allocate more organs compared with a criterion-based system.

The registry used to develop the model is unique in many ways. The registry has been collected data since the 1980’s, it provide data from more than 300 participating institutions and include data from almost 70% of all heart transplantations performed in the world. The most essential variable in our model was donor age, which is in consistence with previous findings [21]. However, we further show that donor age influences the estimated survivals by a factor of two compared with the recipient age and that recipients matched to a donor younger than 38 years had an additional expected median survival time of three years. Previous single center reports, have suggested that it is more beneficial in terms of patient survival to receive an allograft from a donor >40 years old than to remain on the waiting list [4,22]. Our finding raises the question again; may it be more beneficial in terms of patient survival to remain on the waiting list than to receive an allograft from an older donor, especially for non-urgent status-II patients?

Prolonged duration of ischemia has an adverse effect on survival [5,23,24]. Interestingly, our analyses showed that the duration of ischemia affects the discrimination of survival less than expected. The reason might be that our model was developed to predict overall survival and previous studies have been focused on short-term survival. Our finding that the HR was most prominent in the early postoperative era but declined over time supports this.

The influences of body size and gender match are more controversial than duration of ischemia. The ""classic rule” from the early 1990s, supported by previous studies, requires that the donor/recipient weight ratio is not less than 0.8 [4,25]. In the present study, the minor importance of body size match is more consistent with recently published studies [26]. However, we show that a female donor matched to a male recipient has a detrimental effect on survival due to the donor gender and not the body size. A gender effect on graft survival has been reported in renal and liver transplantation but also observed for cardiac allografts, one might speculate that the differences in immunogenicity according to donor gender may be a possible explanation [27].

As illustrated in the sub-group analysis with matched cohorts, we could identify both a non-linear relationship and interactions between the recipient and donor risk factors. For example the HR for a 30 years old recipient were constant the first 5 years post transplant in contest to a 60 years old recipient where it increased with 6%. Furthermore, the HR for different donor age groups was not consistent. The strength of the ANN model is its flexibility to model non-linear relationships and all possible interactions in a virtual fashion, and furthermore it is not dependent on the assumption of a proportional hazard [21,28]. In the present study, the hazard ratio varied over time in several of the variables. A methodology based on standard linear models such as Cox proportion hazard regression requires a priori knowledge of the variable relationships and that the hazards are proportional, and thus not optimal to use in the present material.

To gain acceptance for a model, it is important to quantify the model’s generalizability. By using validation cohorts from an external database and a different time era, the generalizability for the model will be evaluated in an accurate way. The IHTSA model was validated on an internal validation cohort, a temporal validation cohort, and on a regional dataset. Validation in a similar way has only been performed in one of the risk-stratification models compared in this study, the IMPACT model [29]. However, that study did not show whether there was a difference in discrimination between the different cohorts. Our ITHSA model presents a similar degree of discrimination in all of the validation cohorts.

Even if comparison of risk models remains controversial, the C-index is probably the best statistical tool for describing performance [30,31]. A C-index of 0.65 may appear to be low but it should be kept in mind that the IHTSA model predicts long term survival, up to 18 years post transplant and it is higher than previously reported. Despite the fact that the DRI, IMPACT, and RSS were designed to predict one-year mortality, the IHTSA shows superior or similar discrimination in all of the cohorts. Furthermore, the new model generated a unique survival prediction for almost all of the patients. Given that the model in our study was optimized for good prediction at all time points (and not at any specific time point), this demonstrates high flexibility of the IHTSA model.

A model including 43 variables (32 recipient and 11 donor variables) where advanced machine learning algorithms are used makes it impractical or impossible for computation by hand. By developing a model that can simulate the outcome for a patient undergoing heart transplantation, we will be able to avoid poor case match and identify best possible match in the clinical setting. Our web-based calculator (http://www.ihtsa.med.lu.se) allows the user to perform convenient interactive calculations of estimated survival and also of the predicted effects on survival of adding (or changing) different patient characteristics or use of devices together with a donor’s specific profile. The estimate of median life-years in the web-application is provided to illustrate the potential change in long-term mortality with the addition of recipient and/or donor risk factors; however, the potential error for such long-term estimates may be large for low-risk patients.

During 2011, approximately 17 000 donors were reported [32]. Unfortunately, not more than one third of all donors could be utilized for heart transplantation. One explanation for this may be an uncertainty in the risk of early and late graft dysfunction, which means that some suitable donors are not accepted. Although there are many donor predictors of allograft discard in the current era, these characteristics seem to have little effect on recipient outcomes when the hearts are transplanted [33]. A more liberal use of cardiac allografts with relative contraindications may be warranted. However, predicting the impact of a proposed new organ allocation policy is difficult since there are many factors that can change in unpredictable ways when rules change [34]. In the present study, a simulation model evaluated the influence on such different allocation policy. The results show that the survival predicting model (IHTSA) increase the number of organ that could be used without compromise the survival time compared with a traditional criterion based model. Furthermore, the number of possible matches increases by the size of the waiting list, which support the use of centralized organ allocation system such as United Network for Organ Sharing (UNOS).

The waiting lists used worldwide typically consist of a limited number of patients. Since that is not about to change in the near future, the IHTSA model can be used as a virtual recipient-donor matching tool that models survival for potential recipients on a waiting list when a donor’s heart is available. The program then produces a ranking list; from low-risk to high-risk recipients, that can be presented to the transplant physicians [35].

The results of this study carry limitations associated with the retrospective analysis of a registry database, the quality of the source data, the number of missing data, and the lack of standardization associated with multicenter studies (such as different immunosuppressive regimens and different matching criteria), as has been described previously [36]. Some data elements, including center volume and geographic location, were not released by the ISHLT registry. Duration on the waiting list, deaths pre-listing and deaths on the waiting list would have been desirable to include in the model. Unfortunately, the registry does not collect these data. However, with the improvement of heart failure treatment and mechanical assist devices, the waiting-list mortality has decreased and the post transplant outcome has become more significant [9,37–40]. The predicted survival in this study did not account for the race mismatch and may have underestimated the mortality rates in these cases. A small percentage of the derivation cohort included patients from the NTTD which may have had a minor effect on the validation results.",https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0118644#sec027
Mirzazadeh et al. 2021,Improving Heart Transplant Rejection Classification Training using Progressive Generative Adversarial Networks,"Cardiac allograft rejection is a life-threatening complication that can occur in patients following heart transplantation. Endomyocardial biopsies, the current gold-standard for
monitoring rejection, require manual identification of samples
by experts; however, this can be subjective, costly and timeconsuming. Computer-aided diagnosis models can potentially
provide an automated analysis that offers accurate and consistent
detection of biopsy samples. Unfortunately, the lack of a large
dataset of rejection pathology signs, due to time consuming
clinical annotations, limits the classification performance of such
conventional AI-based models. In this paper, we developed a
generative adversarial network (GAN) that creates synthetic
tissue tiles from heart transplant whole slide images (WSIs)
to serve as data for training rejection classifier models. To
generate synthetic rejection histology regions, we used inspirational image generation (IIG) with a single rejection reference
image. Additionally, to demonstrate objective improvement in
image classification using synthetic rejection regions, we use a
pretrained VGG-19 classifier to differentiate between rejection
versus nonrejection tiles. We greatly improved classification
performance, achieving an increase in the Matthews correlation
coefficient (MCC) from 0.411 to 0.790 when the training set was
augmented with our synthetic rejection tiles. Our model was able
to create visibly realistic rejection tissue tiles which was used
to augment the rejection tile database and enhanced automated
rejection detection.","The synthetic rejection tiles created by the GAN were able
to improve the training of a classifier differentiating between
rejection and nonrejection tissue tiles. These tiles resulted in
an MCC score improvement from 0.411 to 0.790. Since the
annotated regions contain many cells, not all of which show
rejection signs, our classifier is biased towards high sensitivity
for rejection detection. For future work we would like to
either change the methodology so that the rejection tiles are
created with additional rejection annotation examples to limit
this effect.
On the heat map, we noticed that there were regions that
the classifier had labeled as potential rejection. Since some of
the rejection tiles that were fed into the classifier for training
might have contained nonrejection signal, we would expect
to see more tiles from the WSIs that might be labeled as
rejection even if they were not given a manual annotation.
This is acceptable in our scenario as the main goal would
be to signal potential regions of interest for the clinician to
examine. With the emphasis of sensitivity over specificity, we
would rather falsely flag potential regions of rejection than not
identify true rejection regions.",https://ieeexplore.ieee.org/document/9508532
Bao et al. 2023,Rare Heart Transplant Rejection Classification Using Diffusion-Based Synthetic Image Augmentation,"Heart Transplant Rejection (HTR) is a rare condition that requires early detection to prevent lasting damage to the transplanted heart. Unfortunately, the current HTR grading through biopsy image classification lacks consistency among pathologists. In addition, it is a time-consuming task. In this work, we have developed an automated diagnosis pipeline to streamline the heart transplant histopathology image quantification and classification, in order to provide objectivity for clinical decision support for pathologists. Traditionally, developing an automated image classification requires a substantial amount of labeled data. However, HTR is a rare condition and the dataset is usually unbalanced. For example, the dataset from DNA Based Transplant Rejection (DTRT) comprises 1,509 rejection tile images and 190 times more non-rejection tile images. To address the small data sample challenge in training the classifiers, we developed a novel strategy that used diffusion model to generate synthetic images of rejection. We conducted comprehensive HTR grade classification comparing results using dataset with synthetic rejection tiles versus the dataset without any synthetic rejection tiles. The introduction of synthetic augmentation resulted in an improvement from 0.781 to 0.981 for sensitivity, and an improvement from 0.984 to over 0.998 in AUROC. This study illustrated that synthetic data augmentation is a feasible strategy in developing AI solutions for rare diseases. In the future, we will expand in this direction to benefit more rare disease clinical decision support development.","We present a novel approach to identifying rare pediatric heart rejections using WSIs. Due to the rarity of the condition, we have an extremely imbalanced dataset consisting of 1,614 rejection tiles and 299,273 non-rejection tiles. Despite the unusual dataset, we achieved a robust classification and proved that synthetic image augmentation improves the classification performance from 0.781 sensitivity to 0.981 sensitivity and 0.984 AUROC to over 0.998 AUROC. Our findings further suggest that augmenting with diffusion-generated image yields superior sensitivity compared to PGAN. In the era of big models and big data, the scarcity of data shall not be a hindrance for rare diseases. Synthetic image augmentation using the state-of-the-art diffusion model is shown to be a promising solution. We genuinely hope that our work could inspire future applications of diffusion models for other rare diseases.",https://ieeexplore.ieee.org/document/10313377
Dooley et al. 2018,Prediction of heart transplant rejection using histopathological whole-slide imaging,"Endomyocardial biopsies are the current gold standard for monitoring heart transplant patients for signs of cardiac allograft rejection. Manually analyzing the acquired tissue samples can be costly, time-consuming, and subjective. Computer-aided diagnosis, using digitized whole-slide images, has been used to classify the presence and grading of diseases such as brain tumors and breast cancer, and we expect it can be used for prediction of cardiac allograft rejection. In this paper, we first create a pipeline to normalize and extract pixel-level and object-level features from histopathological whole-slide images of endomyocardial biopsies. Then, we develop a two-stage classification algorithm, where we first cluster individual tiles and then use the frequency of tiles in each cluster for classification of each whole-slide image. Our results show that the addition of an unsupervised clustering step leads to higher classification accuracy, as well as the importance of object-level features based on the pathophysiology of rejection. Future expansion of this study includes the development of a multi-class classification pipeline for subtypes and grades of cardiac allograft rejection.","A major limitation of this study was the small sample size. Dividing each observational group into training and testing sets further reduced the number of observations that could train each classification model. Future revisions to this study should include adding more whole-slide images. This could help improve accuracy as well as allow for multi-class classification into types of rejection, ACR and AMR, as well as the grading schemes for rejection (0R, 1R, 2R, and 3R). We can also use hyperparameter tuning for the classification methods in future studies.

More features can also be extracted at both the pixel-and object-level. Features that have been extracted in other studies using computer-aided diagnosis for histopathological features include Gabor and wavelet filters and topology features [4]. As most of the top ten contributing factors for PCA came from object-level features, looking into extracting more object-level features may improve overall predictive accuracy.

Also, more features that specifically describe the different types and grades of rejection, such as percent of monocyte inflammation and the locations of foci of infiltrates, can be extracted. This will be especially useful for multi-class classification, for distinguishing between ACR and AMR and different grades of rejection. This can include extracting a few features to describe a specific pathologic finding, such as the breakdown of the myocardium stratification or circular nuclei patterns around capillaries. These pathologic findings can then be combined, so that if more than one instance of specific features pathologists look for are present, the WSI cannot be classified as normal. A mask indicating regions of interests can also be developed to indicate areas that need to be further examined.",https://ieeexplore.ieee.org/document/8333416
Giuste et al. 2023,Explainable synthetic image generation to improve risk assessment of rare pediatric heart transplant rejection,"Expert microscopic analysis of cells obtained from frequent heart biopsies is vital for early detection of pediatric heart transplant rejection to prevent heart failure. Detection of this rare condition is prone to low levels of expert agreement due to the difficulty of identifying subtle rejection signs within biopsy samples. The rarity of pediatric heart transplant rejection also means that very few gold-standard images are available for developing machine learning models. To solve this urgent clinical challenge, we developed a deep learning model to automatically quantify rejection risk within digital images of biopsied tissue using an explainable synthetic data augmentation approach. We developed this explainable AI framework to illustrate how our progressive and inspirational generative adversarial network models distinguish between normal tissue images and those containing cellular rejection signs. To quantify biopsy-level rejection risk, we first detect local rejection features using a binary image classifier trained with expert-annotated and synthetic examples. We converted these local predictions into a biopsy-wide rejection score via an interpretable histogram-based approach. Our model significantly improves upon prior works with the same dataset with an area under the receiver operating curve (AUROC) of 98.84% for the local rejection detection task and 95.56% for the biopsy-rejection prediction task. A biopsy-level sensitivity of 83.33% makes our approach suitable for early screening of biopsies to prioritize expert analysis. Our framework provides a solution to rare medical imaging challenges currently limited by small datasets.","Using a data-driven AI approach, we overcame the challenges of detecting cell-level pathology within large images for a rare disease in three steps: synthetic data generation, tile-level rejection detection, and biopsy-level rejection assessment. Our innovative implementation of PGAN and IGAN to generate synthetic examples of microscopic rejection signs using only eight expert annotated regions succeeded in obtaining state-of-the-art biopsy-level rejection detection of 95.55% AUROC, with a sensitivity of 83.33% and specificity of 66.67%. This success was achieved using a multi-site dataset of biopsy images to support the generalizability of our results.

Explainable AI techniques were used throughout our approach to support the validity and utility of our models while also providing a way for experts to trust automated predictions. The combination of a biopsy-level risk score, rejection burden histogram, and heatmap of local rejection risk greatly facilitates quick analysis of these biopsies to improve the care of pediatric heart transplant patients.

Future work will focus on generating additional expert-annotated examples of cellular rejection signs to further improve and validate our model’s performance. Users will be equipped with biopsy-level risk scores, rejection heatmaps, and biopsy rejection histograms via a web-based application that will perform automated analysis of de-identified biopsy images. This research has the potential to support clinicians in their fight against pediatric heart transplant rejection by providing objective and explainable biopsy risk scores in real-world clinical practice.",https://www.sciencedirect.com/science/article/pii/S1532046423000242?via%3Dihub#sec4
Lipkova et al. 2022,Deep learning-enabled assessment of cardiac allograft rejection from endomyocardial biopsies,"Endomyocardial biopsy (EMB) screening represents the standard of care for detecting allograft rejections after heart transplant. Manual interpretation of EMBs is affected by substantial interobserver and intraobserver variability, which often leads to inappropriate treatment with immunosuppressive drugs, unnecessary follow-up biopsies and poor transplant outcomes. Here we present a deep learning-based artificial intelligence (AI) system for automated assessment of gigapixel whole-slide images obtained from EMBs, which simultaneously addresses detection, subtyping and grading of allograft rejection. To assess model performance, we curated a large dataset from the United States, as well as independent test cohorts from Turkey and Switzerland, which includes large-scale variability across populations, sample preparations and slide scanning instrumentation. The model detects allograft rejection with an area under the receiver operating characteristic curve (AUC) of 0.962; assesses the cellular and antibody-mediated rejection type with AUCs of 0.958 and 0.874, respectively; detects Quilty B lesions, benign mimics of rejection, with an AUC of 0.939; and differentiates between low-grade and high-grade rejections with an AUC of 0.833. In a human reader study, the AI system showed non-inferior performance to conventional assessment and reduced interobserver variability and assessment time. This robust evaluation of cardiac allograft rejection paves the way for clinical trials to establish the efficacy of AI-assisted EMB assessment and its potential for improving heart transplant outcomes.","Here we present CRANE, a weakly supervised deep learning model for automated screening of EMBs in H&E-stained WSIs. Using multitask learning, the model can simultaneously identify ACRs, AMRs, Quilty B lesions and their concurrent appearance, while an additional network is used to determine the rejection grade. The model has been trained using only patient diagnoses readily available in the clinical records, allowing seamless model deployment for large datasets from multiple centers. The usability of CRANE has been demonstrated on two independent international test cohorts, which reflect diverse geographic populations, scanners and biopsy protocols.

Although the datasets used in all three cohorts reflected the screening population of each participating institution, the proportion of AMR cases was considerably lower than that of ACR cases in all cohorts, which is one limitation of the study. This can be attributed to the rarer occurrence of AMRs and to the later recognition of this rejection type by the medical community. Although CRANE’s performance for the rejection grading is comparable to that of human experts, this task remains challenging and will benefit from future model improvements.

Our model demonstrates the promise of integrating AI into the diagnostic workflow. However, optimal use of weakly supervised models in clinical practice remains to be determined. The specific advantages of CRANE suggest its potential to act as an assistive diagnostic tool with aims to increase the efficiency of EMB assessment and decrease interobserver variability by highlighting predictive regions; such assistive tools that highlight areas of interest for human analysis are currently in use for cytology36. Improved robustness and accuracy of rejection assessment could reduce the number of unnecessary follow-up biopsies, a highly valuable outcome given the cost and risks associated with EMBs. CRANE can be further deployed to automatically screen for critical and time-sensitive cases that may benefit from priority inspections.

Although our study focuses on morphology-based biopsy assessment based on current standards of the International Society for Heart and Lung Transplantation (ISHLT), future works could benefit from the integration of clinical end points such as echocardiography or cardiac hemodynamic measurements to improve patient stratification. Additional incorporation of emerging molecular biomarkers such as donor-specific antibodies, intragraft mRNA transcripts31,37,38,39, cell-free DNA40, exosomes41 and gene expression profiling42 could further enhance our understanding of the pathophysiology of cardiac rejection and involved immune interactions. This study lays the foundation and prompts the need for future prospective trials to fully evaluate the extent to which AI-based assessments can improve heart transplant outcomes.",https://www.nature.com/articles/s41591-022-01709-2#Sec8
Kim et al. 2019,A Fully Automated System Using A Convolutional Neural Network to Predict Renal Allograft Rejection: Extra-validation with Giga-pixel Immunostained Slides,"Pathologic diagnoses mainly depend on visual scoring by pathologists, a process that can be time-consuming, laborious, and susceptible to inter- and/or intra-observer variations. This study proposes a novel method to enhance pathologic scoring of renal allograft rejection. A fully automated system using a convolutional neural network (CNN) was developed to identify regions of interest (ROIs) and to detect C4d positive and negative peritubular capillaries (PTCs) in giga-pixel immunostained slides. The performance of faster R-CNN was evaluated using optimal parameters of the novel method to enlarge the size of labeled masks. Fifty and forty pixels of the enlarged size images showed the best performance in detecting C4d positive and negative PTCs, respectively. Additionally, the feasibility of deep-learning-assisted labeling as independent dataset to enhance detection in this model was evaluated. Based on these two CNN methods, a fully automated system for renal allograft rejection was developed. This system was highly reliable, efficient, and effective, making it applicable to real clinical workflow.","To develop clinically applicable system, deep-learning-assisted labeling with more efficiency, enhancing the detection model with pathologists’ insights, combining CNN based classification and detection for a fully automated system were developed in this paper. Training the CNN detection models with enlarged masks surrounding PTC region is inspired by the actual pathologists’ insights, which enhanced the detection performance highly (Fig. 7 and Table 2). Deep learning models generally need massive data for training. To overcome the problem of small dataset, we tried to determine a feasibility of deep-learning-assisted labeling that is made from independent dataset with low-labor, which could alleviate massive manual labor. In our experiments, using deep-learning-assisted labeling for training, the performance of the detection model was enhanced (Fig. 8(c,d) and Table 3) compared with only dataset by using manual labeling only, because deep-learning-assisted labeling help draw PTC masks more robustly with less variations such as inter- and/or intra-observer, illumination, and degree of staining. In addition, massive dataset would improve the performance of the CNN detection models. In the stress test, we showed that 380 slides were sufficient to train CNN detection model for finding positive and negative PTC.

Deep learning on pathologic images could depend on various scanning conditions such as not only illumination, but also the degree of staining, different equipment, and so on. To overcome these problems, pre-trained network which has been already trained with bunch of tremendous number of images having a variety of complex variation, histogram normalization that is one of stain normalization method, and doubled dataset by deep-learning-assisted labeling were used to train our deep learning model to have robustness of them.

The most important aspects of application of this system to pathology are full automation for objective diagnosis and alleviation of manual labor. This study proposed a fully automated two-step CNN system for the diagnosis of allograft rejection. The first step consists of the use of a CNN classification model to identify feasible ROIs in all tissue regions and the second step consists of the use of a CNN detection model to identify and count C4d positive and negative PTCs, a marker of allograft rejection in kidney transplant recipients. These findings suggest that this system may be applicable to most tasks in digital pathology.

Classification of all tissue regions as feasible or non-feasible ROIs using the CNN classification model is practical, as pathologists cannot determine all feasible ROIs in a tissue sample and have difficulty identifying negative PTCs. By contrast, the CNN classification model can precisely evaluate the entire specimen, and the CNN detection model can accurately count the numbers of C4d positive and negative PTCs in all feasible ROIs. Determining both C4d positive and C4d negative PTCs may alter clinical diagnoses.

In addition, two kinds of performance comparisons were conducted. Firstly, the performances of models trained with different size of margin including PTC region were compared. Enlarged mask with a certain size improved detection CNN model, which method was mimicked by a real clinical experience. This novelty including surrounding regions could be used widely for similar tasks. Secondly, the performances of models trained with data by manual labeling and data by deep-learning-assisted labeling were compared. The generation of data by deep-learning-assisted labeling and confirmation by expert pathologists may help improve the performance of these models. Pathologic labeling is very difficult, even for expert pathologists, whereas the deep-learning-assisted method generated relatively robust labels.

Several obstacles should be overcome before clinical application. The sample size (380 slides) is about 1.2 times the average renal allograft biopsy per year in this center, which is one of the largest medical center in South Korea. Though it is also relatively larger than other studies related to pathologic assessment using convolutional neural network9,11,12, we will try to evaluate the performance of this more with wild dataset from larger data. Also, all cases were recruited from a single center using only one slide scanner, which could lead to less variations such as background illumination or degree of staining. To evaluate the robustness of this method, further studies with multi-center could be needed. In addition, comparisons of the performance and outcomes of this method with those of pathologists are needed to determine the clinical effectiveness of this system.",https://www.nature.com/articles/s41598-019-41479-5#Sec8
Tong et al. 2017,Predicting Heart Rejection Using Histopathological Whole-Slide Imaging and Deep Neural Network with Dropout,"Cardiac allograft rejection is one major limitation for long-term survival for patients with heart transplants. The endomyocardial biopsy is one gold standard to screen heart rejection for patients that have heart transplantation. However, manual identification of heart rejection is expensive and time-consuming. With the development of imaging processing techniques and machine learning tools, automatic prediction of heart rejection using whole-slide images is one promising approach to improve the care of patients with heart transplants. In this paper, we first develop a histopathological whole-slide image processing pipeline to extract features automatically. Then, we construct deep neural networks with and without regularization and dropout to classify the patients into nonrejection and rejection respectively. Our results show that neural networks with regularization and dropout can significantly reduce overfitting and achieve more stable accuracies.","Based on the observation of extensive experiments, we conclude that neural networks with regularization and dropout can significantly reduce overfitting and achieve more stable accuracy compared to neural networks without regularization and dropout.

One limitation of this study is the small sample size. With only 43 whole-slide images available, training a comprehensive model with desired training, validations, and testing sets is difficult. Also, because of the small sample size, we are unable to perform multi-class classification. As a next step, we plan to include more patients under heart transplant and collect their whole slide images to enlarge the cohort of this study.

In this study, we demonstrated the feasibility of applying deep neural network for the prediction of heart rejection. However, optimization of the network configuration is yet to be further explored to improve the accuracy and reduce overfitting. To construct a deep neural network, we need to determine the number of layers, the number of perceptrons in each layer, activation function, and the cost function for optimization. And when we further apply regularization and dropout to the neural network, we need to decide the regularizations we want to add to the cost function and the probability of dropout in each layer. With all these factors taken into consideration, it’s especially difficult to find an optimal network for different situations. One future direction of this study is to establish a guideline for determining values of large number of hyperparameters.",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7324296/
Topuz et al. 2018,Predicting graft survival among kidney transplant recipients: A Bayesian decision support model,"Predicting the graft survival for kidney transplantation is a high stakes undertaking considering the shortage of available organs and the utilization of healthcare resources. The strength of any predictive model depends on the selection of proper predictors. However, despite improvements in acute rejection management and short-term graft survival, the accurate prediction of kidney transplant outcomes remains suboptimal. Among other approaches, machine-learning techniques have the potential to offer solutions to this prediction problem in kidney transplantation. This study offers a novel methodological solution to this prediction problem by: (a) analyzing the retrospective database including > 31,000 U.S. patients; (b) introducing a comprehensive feature selection framework that accounts for medical literature, data analytics methods and elastic net (EN) regression (c) using sensitivity analyses and information fusion to evaluate and combine features from several machine learning approaches (i.e., support vector machines (SVM), artificial neural networks (ANN), and Bootstrap Forest (BF)); (d) constructing several different scenarios by merging different sets of features that are optioned through these fused data mining models and statistical models in addition to expert knowledge; and (e) using best performing sets in Bayesian belief network (BBN) algorithm to identify non-linear relationships and the interactions between explanatory factors and risk levels for kidney graft survival. The results showed that the predictor set obtained through fused data mining model and literature review outperformed the all other alternative predictors sets with the scores of 0.602, 0.684, 0.495 for F-Measure, Average Accuracy, and G-Mean, respectively. Overall, our findings provide novel insights about risk prediction that could potentially help in improving the outcome of kidney transplants. This methodology can also be applied to other similar transplant data sets.","Different relationships and interactions generated by the BBN model may provide medical practitioners with significant insight into factors that could affect the outcome of kidney transplantations. For example, interactions between the body mass index (BMI_CALC), the weight of the recipient (WGT_KG_CALC), and height of the recipient (HGT_CM_CALC) are intuitive, since BMI_CALC is calculated using the other two metrics.

In the BBN model, there are several very interactive predictors that must be discussed in detail. First, the primary diagnosis at the time of listing (DGN_TCR) has a direct relationship with eight predictors: donor blood type (ABO_DON), recipient's diabetes at registration (DIAB), number of previous pancreas transplants (NPPAN), recipient treated for rejection within one year (TRTREJ1Y_KI), human leukocyte antigen HLA mismatch level (HLAMIS), kidney recipient primary diagnosis at transplantation (DIAG_KI), and the number of previous kidney transplantations (NPKIDs).

Second, the kidney recipient's primary diagnosis at transplantation (DIAG_KI) has a direct relationship with six predictors: calculated recipient weight (kg) (EGT_KG_CALC), number of pre-transplant transfusions (PRE_TX_TXFUS), recipient's total serum albumin at registration (TOT_SERUM_ALBUM), recipient serum creatinine at time of transplant (CREAT_TRR), recipient's ethnicity category (ETHCAT), and the primary diagnosis at the time of listing (DGN_TCR).

Third, the recipient's functional status at registration (FUNC_STAT_TCR), defined as the ability to carry out daily activities, has a direct relationship to six variables: the primary diagnosis at time of listing (DGN_TCR), recipient's highest educational level at registration (EDUCATION), if the recipient works for income at the time of registration (WORK_INCOME_TRR), total days on waiting list, including inactive time (DAYSWAIT_CHRON), deceased donor Epstein-Barr virus by IGG test result (EBV_IGG_CAD_CON), and recipient's functional status at transplant (FUNC_STAT_TRR).

Fourth, clinicians can draw useful, practical information from the constructed BBN by using “what-if analysis.” For example, if the total days on the waiting list, including inactive time (DAYSWAIT_CHRON_KI), increases from < 170 days to > 1300 days, then the probability of graft failure for the high-risk category within RSKLV variable increases from 41% to 48%. Furthermore, in the BBN model, one also can observe that the functional status at registration (FUNC_STAT_TCR) contributes to the effect of the total days on a waiting list (DAYSWAIT_CHRON_KI) has on graft survival. As such, if the FUNC_STAT_TCR of the patient is one of the following three categories: (1) “moribund, fatal processes progress,” (2) “very sick, hospitalization necessary: active treatment necessary,” (3) “severely disabled: hospitalization is indicated, death not imminent,” then having shorter waiting days do not have much effect on the risk; on the contrary, it exhibits the RSKLVL as “high” as 80%. However, if the FUNC_STAT_TCR of the patient is “fully active, normal,” then the total days on the waiting list (DAYSWAIT_CHRON_KI) becomes very important. For instance, when the FUNC_STAT_TCR of the patient is “Fully active, normal,” as the total days on the waiting list (DAYSWAIT_CHRON_KI) increases from < 170 days to > 1300 days, the probability of graft failure for the high-risk category within RSKLV variable increases from 30.60% to 53.50%. Our finding of functional status is an important condition that may be useful for clinicians since medical literature also suggests that shorter wait list time [76], [77] and better functional [5], [9] status leads to improved graft survival. In summary, the BBN models could help the health care providers in examining the interactions among the different variables to develop clinically relevant protocols that could improve clinical outcomes in the transplant population.",https://www.sciencedirect.com/science/article/pii/S0167923617302233
Luck et al. 2017,Deep Learning for Patient-Specific Kidney Graft Survival Analysis,"An accurate model of patient-specific kidney graft survival distributions can help to improve shared-decision making in the treatment and care of patients. In this paper, we propose a deep learning method that directly models the survival function instead of estimating the hazard function to predict survival times for graft patients based on the principle of multi-task learning. By learning to jointly predict the time of the event, and its rank in the cox partial log likelihood framework, our deep learning approach outperforms, in terms of survival time prediction quality and concordance index, other common methods for survival analysis, including the Cox Proportional Hazards model and a network trained on the cox partial log-likelihood.","In conclusion, our method outperforms previous state-of-the-art methods in terms of the commonly
used C-index metric. Moreover, it gives important clues about the survival prediction for different
time thresholds. This shows the advantages of directly modeling the survival function.",https://arxiv.org/abs/1705.10245
Shahmoradi et al. 2016,Comparing Three Data Mining Methods to Predict Kidney Transplant Survival,"Introduction:
One of the most important complications of post-transplant is rejection. Analyzing survival is one of the areas of medical prognosis and data mining, as an effective approach, has the capacity of analyzing and estimating outcomes in advance through discovering appropriate models among data. The present study aims at comparing the effectiveness of C5.0 algorithms, neural network and C&RTree to predict kidney transplant survival before transplant.

Method:
To detect factors effective in predicting transplant survival, information needs analysis was performed via a researcher-made questionnaire. A checklist was prepared and data of 513 kidney disease patient files were extracted from Sina Urology Research Center. Following CRISP methodology for data mining, IBM SPSS Modeler 14.2, C5.0, C&RTree algorithms and neural network were used.

Results:
Body Mass Index (BMI), cause of renal dysfunction and duration of dialysis were evaluated in all three models as the most effective factors in transplant survival. C5.0 algorithm with the highest validity (96.77%) was the first in estimating kidney transplant survival in patients followed by C&RTree (83.7%) and neural network (79.5%) models.

Conclusion:
Among the three models, C5.0 algorithm was the top model with high validity that confirms its strength in predicting survival. The most effective kidney transplant survival factors were detected in this study; therefore, duration of transplant survival (year) can be determined considering the regulations set for a new sample with specific characteristics.","As kidney transplantation is increasing every day in the world and fear of transplantation rejection, high costs and growing number of ESRD patients on the other hand, designing a model for predicting transplantation survival would be a great help leading to increasing survival rates and consequently, decreasing transplantation waiting times and costs.

In the present study, factors affecting kidney transplantation survival are determined through information needs analysis conducted on nephrologists/urologists and a researcher-made questionnaire. After screening the data and omitting incomplete records, modelling was performed using neural network and the two decision making trees of C5.0 and C&RTree and their accuracy, specificity and sensitivity were evaluated and compared.

In the study of Ashrafi et al conducted on 316 kidney transplant patients, demographic information of recipients and donors, type and location of transplant, recipient’s BMI and diabetic status were extracted from patient files and death or transferring patients to dialysis were considered as the end point (38). In order to analyse 10-year survival of transplanted kidney and determining the effective factors, Hassanzadeh et al added cold ischemic time, relation to recipient (relative, non-relative), side of donated kidney, predialysis duration, creatinine level at discharge and duration of hospitalization to the said factors (7). The retrospective study of Hashiani examined the survival rate of kidney transplantation by studying variables like age and sex of donors and recipients (39). The strong point of the present study compared to the previous ones is the method of determining data effective in predicting survival which was conducted scientifically and through questionnaires distributed among nephrologists and urologists. Evaluating a series of factors affecting kidney dysfunction was a determining factor in predicting survival which were not, except one (diabetic status of recipient) taken into account in the study of Ashrafi et al.

In another study conducted by Saleh Nasab et al (39), a checklist was prepared (like the present study), information was extracted from files of kidney patients and modelling was performed based on the information. Although, the said study also aimed at extracting effective models in predicting survival using data mining, some factors like immunosuppressive regimen, cold ischemic time, creatinine level at discharge and duration of hospitalization were ignored due to the difference in the study’s perspective because the aim of the present study is predicting transplantation survival prior to surgery; however, factors similar to this study in the previous ones were post-transplant variables that cannot be considered in this present study.

In the study of Ashrafi, statistical methods of Kaplan–Meier, Cox regression and goodness of fit were compared in the artificial neural network and the neural network model was introduced as the highest one among others with 72% precision (38).

In a study titled “predicting chronic allograft kidney disease using decision making tree” by Lou Faro et al, C4.8 algorithm and laboratory factors if transplant patients were used; the validity of the model was <83% (40). In the same year, Greco et al predicted transplantation survival or rejection using a binary tree at 4 levels’ sensitivity and specificity of the tree were estimated at 88.2% and 73.8%, respectively. In the present study, C5.0 algorithm, the optimized version of C4.8 algorithm was used and the validity of the survival rate of the model in each transplantation case was estimated at 96.7%. On the other hand, the target field, in this study, should be classified; the difference between the recent study and that of Lou Faro and Greco is in the output of the tree. The output in the latter studies was merely failure or success of transplantation but the output of our study was not binary and could express duration of transplantation survival in 6 different conditions (41).

In this study, the three data mining algorithms were compared to estimate transplantation survival in kidney patients; the highest accuracy belonged to C5.0 model (96.77%) followed by C&RTree (83.7%) and neural network model (79.5%).

Evaluating the significance of transplantation survival predicting factors in the present study, cause of kidney dysfunction, BMI and pre-transplant dialysis were determined as the most effective factors that are compatible with the findings of previous studies. By comparing preceding researches in the area of data mining and kidney transplantation survival, it is clear that the C5.0 model offered in our study has the highest accuracy; moreover, another strong point of the study is implementing all phases of knowledge discovery according to CRISP standard that was not mentioned in other studies. On the other hand, in order to facilitate the application of the model, the researchers designed and run a mobile application under android and iOS platforms. Using the said apps, the user can see the predicted survival rate of transplanted kidney in one of the rows of Table 2 after filling out input fields according to Table 1.

Since the findings of our study are obtained from the data of one single hospital, it is suggested that data of different research centres are used and compared for further evaluation of the subject.",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5256037/
Zhu et al. 2019,Improved Prediction on Heart Transplant Rejection Using Convolutional Autoencoder and Multiple Instance Learning on Whole-Slide Imaging,"Heart transplant rejection is one major threat for the survival of patients with a heart transplant. Endomyocardial biopsies are effective in showing signs of heart transplant rejection even before patients have any symptoms. Manually examining the tissue samples is costly, time-consuming and error-prone. With recent advances in deep learning (DL) based image processing methods, automatic training and prediction on heart transplant rejection using whole-slide images expect to be promising. This paper develops an advanced pipeline for quality control, feature extraction, clustering and classification. We first implement a stacked convolutional autoencoder to extract feature maps for each tile; we then incorporate multiple instance learning (MIL) with dimensionality reduction and unsupervised clustering prior to classification. Our results show that utilizing unsupervised clustering after feature extraction can achieve higher classification results while preserving the capability for multi-class classification.","In [6], 461 hand crafted features led to the highest classification accuracy of 70%. Utilizing convolutional autoencoder for feature extraction gave rise to an accuracy of 72.2%. The improvement is not significant, largely due to the small data size we have. Stacked CAE, nevertheless, has shown its effectiveness in feature extraction, achieving higher accuracy with carefully designed, hand-crafted object-level and pixel-level features extraction. Consequently, this feature extraction process is more scalable to much larger data size, and more robust on different WSI datasets.

Meanwhile, the proposed
Meanwhile, the proposed clustering method achieve higher accuracy and AUC score than baseline, which was inspired by the state-of-art WELDON and CHOWDER model. Besides, the proposed clustering method preserved the capability of multi-class classification.

Improvement can be made through adopting a pre-trained deep convolutional neural network for feature extraction. Furthermore, K-means clustering can be replaced with a deep clustering algorithm. In this way, we can perform end-to-end training across the entire pipeline.

The limitations of this study are the assignment of rejection grade to the whole slide and the sample size. We anticipate the performance of the method to significantly improve with larger sample size and further refinement of the classification. We anticipate to validate the study using a prospectively acquired robust data set and to test the performance at finer granularity rather than the binary classification used here.",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7310716/
Li et al. 2010,Bayes Net Classifiers for Prediction of Renal Graft Status and Survival Period,"This paper presents the development of a Bayesian
belief network classifier for prediction of graft status and survival
period in renal transplantation using the patient profile information
prior to the transplantation. The objective was to explore feasibility
of developing a decision making tool for identifying the most suitable
recipient among the candidate pool members. The dataset was
compiled from the University of Toledo Medical Center Hospital
patients as reported to the United Network Organ Sharing, and had
1228 patient records for the period covering 1987 through 2009. The
Bayes net classifiers were developed using the Weka machine
learning software workbench. Two separate classifiers were induced
from the data set, one to predict the status of the graft as either failed
or living, and a second classifier to predict the graft survival period.
The classifier for graft status prediction performed very well with a
prediction accuracy of 97.8% and true positive values of 0.967 and
0.988 for the living and failed classes, respectively. The second
classifier to predict the graft survival period yielded a prediction
accuracy of 68.2% and a true positive rate of 0.85 for the class
representing those instances with kidneys failing during the first year
following transplantation. Simulation results indicated that it is
feasible to develop a successful Bayesian belief network classifier for
prediction of graft status, but not the graft survival period, using the
information in UNOS database.","Software tools are needed to help with the complex decision
making process associated with the identification of a good
transplant candidate for an available kidney. Predictors for
renal transplantation graft status and graft survival period
using Bayes net classifiers were developed using the
University of Toledo Medical Center (UTMC) patient data as
reported to UNOS. The Bayes net classifier for the graft
status demonstrated very high prediction accuracy and true
positive values for all classes suggesting that it can be readily
employed in a clinical setting. The second Bayes net classifier
for the prediction of graft survival period failed to demonstrate
an acceptable level of performance. ",https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=061de24506764e7d55fd6397540300fc14e8001e
Lofaro et al. 2010,Prediction of Chronic Allograft Nephropathy Using Classification Trees,"Introduction
For its intrinsic
For its intrinsic potential to mine causal relations, machine learning techniques are useful to identify new risk indicators. In this work, we have shown two classification trees to predict chronic allograft nephropathy (CAN), through an evaluation of routine blood and urine tests.

Methods
We retrospectively analyzed 80 renal transplant patients with 60-month follow-up (mean = 55.20 ± 12.74) including 52 males and 28 females of overall average age of 41.65 ± 12.52 years. The primary endpoint was biopsy-proven CAN within 5 years from transplantation (n = 16). Exclusion criteria were multiorgan transplantations, patients aged less than 18 years, graft failure, or patient death in the first 6 months posttransplantation. Classification trees based on the C 4.8 algorithm were used to predict CAN development starting from patient features at transplantation and biochemical test at 6-month follow-up. Model performance was showed as sensitivity (S), false-positive rate (FPR), and area under the receiver operating characteristic curve (AUC).

Results
The two class of patients (no CAN versus CAN) showed significant differences in serum creatinine, estimated Glomerular Filtration Rate with Modification of Diet in Renal Disease study formula (MDRD), serum hemoglobin, hematocrit, blood urea nitrogen, and 24-hour urine protein excretion. Among the 23 evaluated variables, the first model selected six predictors of CAN, showing S = 62.5%, TFP = 7.2%, and AUC = 0.847 (confidence interval [CI] 0.749–0.945). The second model selected four variables, showing S = 81.3%, TFP = 25%, and AUC = 0.824 (CI 0.713–0.934).

Conclusions
Identification models have predicted the onset of multifactorial, complex pathology, like CAN. The use of classification trees represent a valid alternative to traditional statistical models, especially for the evaluation of interactions of risk factors.
","Decision trees are tools for classification rule representation though a hierarchical and sequential architecture. Univocal subject inclusion in a specified class derives from comparisons of patient data and tree rules. We analyzed two different classes: the first, named “CAN,” included patients at risk of developing CAN within 5 years after transplantation. The second, identified as “no CAN,” consisted of patients not at risk. Models able to predict the onset of multifactorial and composite diseases like CAN, from quantitative data evaluation, give great benefit to clinical practice, especially in the era of evidence-based medicine. Although it is small sample study, this work shows the advantages derived from use of quantitative methods for laboratory parameters analysis. Moreover, it provides not only quick predictive model mining, but also allows an immediate evaluation of interactions of risk factors.

Few authors have used these algorithms in the field of kidney transplantation. Furthermore, these works have been limited to the evaluation of well-known classical predictive factors (cold ischemia time, acute rejection, etc).

In conclusion, despite these only preliminary results, we have demonstrated that analysis of a routine data set of clinical variables helps in the early detection of chronic graft dysfunction. The use of data mining models is a valid alternative to traditional statistical methods.",https://www.sciencedirect.com/science/article/pii/S0041134510003507?casa_token=Q7oEXLTIuhYAAAAA:pSuM1ik_HW7NVo56Q0MFyUbjZuP_Bc65XirNwLlFdJgSK5-wZlwZIx8m6o4OEMqvHUvzKaGf
Decruyenaere et al. 2015,Prediction of delayed graft function after kidney transplantation: comparison between logistic regression and machine learning methods,"Background
Predictive models for delayed graft function (DGF) after kidney transplantation are usually developed using logistic regression. We want to evaluate the value of machine learning methods in the prediction of DGF.

Methods
497 kidney transplantations
497 kidney transplantations from deceased donors at the Ghent University Hospital between 2005 and 2011 are included. A feature elimination procedure is applied to determine the optimal number of features, resulting in 20 selected parameters (24 parameters after conversion to indicator parameters) out of 55 retrospectively collected parameters. Subsequently, 9 distinct types of predictive models are fitted using the reduced data set: logistic regression (LR), linear discriminant analysis (LDA), quadratic discriminant analysis (QDA), support vector machines (SVMs; using linear, radial basis function and polynomial kernels), decision tree (DT), random forest (RF), and stochastic gradient boosting (SGB). Performance of the models is assessed by computing sensitivity, positive predictive values and area under the receiver operating characteristic curve (AUROC) after 10-fold stratified cross-validation. AUROCs of the models are pairwise compared using Wilcoxon signed-rank test.

Results
The observed incidence of DGF is 12.5 %. DT is not able to discriminate between recipients with and without DGF (AUROC of 52.5 %) and is inferior to the other methods. SGB, RF and polynomial SVM are mainly able to identify recipients without DGF (AUROC of 77.2, 73.9 and 79.8 %, respectively) and only outperform DT. LDA, QDA, radial SVM and LR also have the ability to identify recipients with DGF, resulting in higher discriminative capacity (AUROC of 82.2, 79.6, 83.3 and 81.7 %, respectively), which outperforms DT and RF. Linear SVM has the highest discriminative capacity (AUROC of 84.3 %), outperforming each method, except for radial SVM, polynomial SVM and LDA. However, it is the only method superior to LR.

Conclusions
The discriminative capacities of LDA, linear SVM, radial SVM and LR are the only ones above 80 %. None of the pairwise AUROC comparisons between these models is statistically significant, except linear SVM outperforming LR. Additionally, the sensitivity of linear SVM to identify recipients with DGF is amongst the three highest of all models. Due to both reasons, the authors believe that linear SVM is most appropriate to predict DGF.","The risk prediction of DGF may be important in preventing its deleterious short-term and long-term consequences. To date, four predictive models are developed as a clinical tool to quantify the risk for DGF [14–17]. All models are developed using LR. We compared in this study several machine learning methods, including LR, in terms of their predictive accuracy for DGF. There are no studies that have used DT, SGB, RF, LDA, QDA or SVM in the prediction of DGF.

In our study, DT is not able to discriminate between recipients with and without DGF, and is inferior to the other methods. SGB, RF and polynomial SVM are mainly able to identify recipients without DGF and only outperform DT. Despite lower sensitivity in varying degrees to identify recipients without DGF, LDA, QDA, radial SVM and LR also have the ability to identify recipients with DGF, resulting in higher discriminative capacity, which outperforms DT and RF. Linear SVM has the highest discriminative capacity (AUROC of 84.3 %), outperforming each method, except for radial SVM, polynomial SVM and LDA. However, it is the only method superior to LR.

The AUROC focuses solely on the predictive accuracy of a model. As such, it cannot tell us whether the model is worth using in clinical practice, because it does not incorporate information on consequences. The method with maximal accuracy is not necessarily the best to choose. This choice should depend on the disadvantages or costs of not identifying a recipient with DGF as opposed to incorrectly predicting DGF in a recipient who will not develop it [28]. The advantages of an early hypothetic treatment should be weighed against possible iatrogenic damage and unnecessary additional costs. If we assume that the damage of an unnecessary treatment of DGF (a false-positive result) is limited, a more sensitive method should be used. If an unnecessary treatment is harmful, a more specific method should be used. Of course the trade-off between sensitivity and specificity should be kept in mind: a very sensitive method is useless when it is not specific enough and vice versa [29].

Currently, the management of DGF consists of a careful follow-up. Besides sonographic evaluation and precise biochemical monitoring, a biopsy is often performed, which is costly and invasive, possibly damaging the graft. Because of the complex and multifactorial characteristics of DGF, a standard therapy or drug does not yet exist [30]. Although a biopsy might be harmful, this is outweighed by the potential benefit of an early management, because DGF has deleterious short-term and long-term consequences. To date, a more sensitive method is therefore preferred. In our study, linear SVM, radial SVM and LR have the highest sensitivity in identifying recipients with DGF (83.8, 88.8 and 85.5 %, respectively).

To sum up, the discriminative capacities of LDA, linear SVM, radial SVM and LR are the only ones above 80 % (82.2, 84.3, 83.3 and 81.7 %, respectively). None of the pairwise AUROC comparisons between these models is statistically significant, except linear SVM outperforming LR. Additionally, a method with higher sensitivity is preferred over a method with higher specificity in the prediction of DGF. The sensitivity of linear SVM to identify recipients with DGF (83.8 %) is amongst the three highest of all methods used. Only radial SVM and LR have a slightly higher sensitivity (88.8 and 85.5 %, respectively). Due to both reasons, the authors believe that linear SVM is most appropriate to predict DGF.

72.0 % of the recipients who will not develop DGF are identified. These recipients can undergo the kidney transplantation without the need for a more precise monitoring. Only 3.1 % will still develop DGF. 83.8 % of the recipients who will develop DGF are identified. These recipients will have to be precisely monitored after kidney transplantation, making an early identification of graft dysfunction possible. 69.4 % of all positively identified recipients will eventually not develop DGF.

Our study does have limitations. Firstly, our sample size of approximately 500 transplantations is lower than in the existing models. It is known that machine learning techniques generally benefit from a large amount of data, increasing their performance [19]. However, we benefited from the detailed and high-quality peritransplant data that could be collected, which is largely unavailable in registries. Secondly, the incidence of DGF in our cohort is lower than in the existing models. This imbalance is addressed by assigning more weight to the ‘DGF’ class during the learning phase of the predictive models. Thirdly, single-center models limit generalizability. However, we used cross-validation to attenuate the generalization error. Finally, our analysis included most, but not all, of the identified risk factors for DGF.
",https://link.springer.com/article/10.1186/s12911-015-0206-y#Sec11
Santori et al. 2007,Application of an Artificial Neural Network Model to Predict Delayed Decrease of Serum Creatinine in Pediatric Patients After Kidney Transplantation,"Artificial neural network, a computer-based technology that uses nonlinear statistics to recognize the relationship between input variables and an output variable, has been previously applied to outcome prediction in adult kidney recipients. In this study, we evaluated the effectiveness of a neural network model to predict a delayed decrease of serum creatinine in pediatric kidney recipients. The neural network was constructed with a training set of pediatric kidney recipients (n = 107) by using 20 input variables and assuming for the output variable, the time after 3 days to reach a serum creatinine level 50% below that before kidney transplantation. In the final model, the following input variables showing higher predictive values were retained: serum creatinine on day 1 post transplant, urine volume in the first 24 hours, diagnostic category, pretransplant dialysis mode, patient sex, donor sex, body weight on day 1 posttransplant, and patient age. The model was validated in a second set of patients (n = 41) by blinding the network for the output variable. The overall accuracies of the neural network for the training set, the validation set, and the whole patient cohort were 89.1%, 76.92%, and 87.14%, respectively. A comparative logistic regression analysis revealed only serum creatinine on day 1 posttransplant to be an independent predictor for the output variable (overall accuracy: 79.05%). The neural network showed sensitivity and specificity for the whole patient cohort to be 0.875 and 0.87, respectively, whereas using logistic regression sensitivity and specificity yields 0.37 and 0.94, respectively. This study proposes a neural network model that seemed to predict a delayed decrease in serum creatinine among pediatric kidney recipients. The availability of the source code may allow development of stand-alone neural networks to validate our model in prospective studies.","Artificial neural network, a computer-based technology that uses nonlinear statistics to recognize the relationship between input variables and an output variable, has been previously applied to outcome prediction in adult kidney recipients. In this study, we evaluated the effectiveness of a neural network model to predict a delayed decrease of serum creatinine in pediatric kidney recipients. The neural network was constructed with a training set of pediatric kidney recipients (n = 107) by using 20 input variables and assuming for the output variable, the time after 3 days to reach a serum creatinine level 50% below that before kidney transplantation. In the final model, the following input variables showing higher predictive values were retained: serum creatinine on day 1 post transplant, urine volume in the first 24 hours, diagnostic category, pretransplant dialysis mode, patient sex, donor sex, body weight on day 1 posttransplant, and patient age. The model was validated in a second set of patients (n = 41) by blinding the network for the output variable. The overall accuracies of the neural network for the training set, the validation set, and the whole patient cohort were 89.1%, 76.92%, and 87.14%, respectively. A comparative logistic regression analysis revealed only serum creatinine on day 1 posttransplant to be an independent predictor for the output variable (overall accuracy: 79.05%). The neural network showed sensitivity and specificity for the whole patient cohort to be 0.875 and 0.87, respectively, whereas using logistic regression sensitivity and specificity yields 0.37 and 0.94, respectively. This study proposes a neural network model that seemed to predict a delayed decrease in serum creatinine among pediatric kidney recipients. The availability of the source code may allow development of stand-alone neural networks to validate our model in prospective studies.",https://www.sciencedirect.com/science/article/pii/S0041134507006057?via%3Dihub
Tangpanithandee et al. 2022,Clinical Phenotypes of Dual Kidney Transplant Recipients in the United States as Identified through Machine Learning Consensus Clustering,"Background and Objectives: Our study aimed to cluster dual kidney transplant recipients using an unsupervised machine learning approach to characterize donors and recipients better and to compare the survival outcomes across these various clusters. Materials and Methods: We performed consensus cluster analysis based on recipient-, donor-, and transplant-related characteristics in 2821 dual kidney transplant recipients from 2010 to 2019 in the OPTN/UNOS database. We determined the important characteristics of each assigned cluster and compared the post-transplant outcomes between clusters. Results: Two clinically distinct clusters were identified by consensus cluster analysis. Cluster 1 patients was characterized by younger patients (mean recipient age 49 ± 13 years) who received dual kidney transplant from pediatric (mean donor age 3 ± 8 years) non-expanded criteria deceased donor (100% non-ECD). In contrast, Cluster 2 patients were characterized by older patients (mean recipient age 63 ± 9 years) who received dual kidney transplant from adult (mean donor age 59 ± 11 years) donor with high kidney donor profile index (KDPI) score (59% had KDPI ≥ 85). Cluster 1 had higher patient survival (98.0% vs. 94.6% at 1 year, and 92.1% vs. 76.3% at 5 years), and lower acute rejection (4.2% vs. 6.1% within 1 year), when compared to cluster 2. Death-censored graft survival was comparable between two groups (93.5% vs. 94.9% at 1 year, and 89.2% vs. 84.8% at 5 years). Conclusions: In summary, DKT in the United States remains uncommon. Two clusters, based on specific recipient and donor characteristics, were identified through an unsupervised machine learning approach. Despite varying differences in donor and recipient age between the two clusters, death-censored graft survival was excellent and comparable. Broader utilization of DKT from high KDPI kidneys and pediatric en bloc kidneys should be encouraged to better address the ongoing organ shortage.","DKT, the transplantation of two kidneys from the same donor into a single recipient, has been utilized as an alternative approach to expand the available donor pool [7,11]. Many reports show that the graft survival rate was higher in the DKT recipients than in those single kidney transplant (SKT) with ECD or high KDPI kidney [11,12,15,19]. According to our study, DKT remains uncommon in the United States, accounting for only 1.8% of the overall kidney transplants. Recent OPTN allocation changes took effect in 2019 and were further affected by implementation of the 250 nautical mile fixed circle allocation [50]. Despite policy intent, there was a decrease in the number of dual kidney transplants performed albeit initial monitoring occurred during the COVID pandemic.
In this study, we use an unsupervised machine learning consensus clustering approach to categorize DKT into two different clusters based on recipient and donor characteristics in the OPTN/UNOS database. Cluster 1 patients, which accounted for nearly 70% of all DKT, were younger patients who received DKT from pediatric non-ECD donors. In contrast, cluster 2 patients were characterized by older patients who received DKT from adult donors with higher KDPI scores.
Cluster 2 patients received higher KDPI kidneys and had higher incidence of delayed graft function, and thus higher acute rejection was observed in cluster 2 recipients when compared to cluster 1 patients. While it is perhaps not unexpected that patient survival was better in cluster 1 given that patients in cluster 1 were significantly younger and less likely to be diabetic as compared to those in cluster 2, death-censored graft survival was; however, comparable between the two clusters. Early graft losses due to technical complications likely account for decreased graft survival in pediatric DKT recipients along with increased probability of seeing recurrent primary disease within the allograft [51]. By comparison, recipient characteristics in combination with lower expected longevity in higher KDPI kidneys likely accounts for similarities in patient survival and death-censored graft loss for cluster 2. The findings of this study illustrate excellent death-censored graft survival in both clusters. This reflects appropriate donor-recipient pairing and kidney utilization in the transplant community. These data align with what has been observed within the OPTN post-policy implementation where a higher proportion of dual kidney recipients were aged 65+ year whereas the proportion of pediatric en bloc kidney transplants for recipients aged 18–34 and 35–49 years notably increased [50].
There are some limitations in this study. Due to the registry nature of this cohort, there is lack of detail specific to exact causes for graft loss and death as well as specific detail related to donor-recipient pairing and center-specific criteria for DKT utilization. Additionally, lack of difference in death-censored graft loss between the 2 clusters can be explained by differences in recipient and donor characteristics, which are not necessarily novel. Although these clusters clinically different, and the application of machine learning is novel, there are limitations in how these data will enhance current clinical decision-making. Future studies applying supervised machine learning with prediction models based upon this initial data will be of greater utility in assessing predictors of survival for DKT.
To the best of our knowledge, this is the first machine learning approach specifically targeted at DKT. Two DKT clusters were identified using machine learning clustering methods without human intervention. The outcomes of our clustering approach, based on machine learning, support existing studies demonstrating the importance of donor-recipient pairing in DKT outcomes which also highlights opportunities to improve the kidney allocation system in the United States. There are likely existing opportunities to further expand DKT utilization within the transplant community, particularly for high KDPI kidneys. The application of ML consensus clustering approach in this study provides a novel understanding of unique phenotypes of DKT recipients in order to advance allocation systems to expand the donor pool. Given excellent outcomes among both clusters, DKT from high KDPI kidneys and pediatric en bloc kidneys should be encouraged to better address the ongoing organ shortage.",https://www.mdpi.com/1648-9144/58/12/1831
Thongprayoon et al. 2023,Characteristics of Kidney Transplant Recipients with Prolonged Pre-Transplant Dialysis Duration as Identified by Machine Learning Consensus Clustering: Pathway to Personalized Care,"Longer pre-transplant dialysis duration is known to be associated with worse post-transplant outcomes. Our study aimed to cluster kidney transplant recipients with prolonged dialysis duration before transplant using an unsupervised machine learning approach to better assess heterogeneity within this cohort. We performed consensus cluster analysis based on recipient-, donor-, and transplant-related characteristics in 5092 kidney transplant recipients who had been on dialysis ≥ 10 years prior to transplant in the OPTN/UNOS database from 2010 to 2019. We characterized each assigned cluster and compared the posttransplant outcomes. Overall, the majority of patients with ≥10 years of dialysis duration were black (52%) or Hispanic (25%), with only a small number (17.6%) being moderately sensitized. Within this cohort, three clinically distinct clusters were identified. Cluster 1 patients were younger, non-diabetic and non-sensitized, had a lower body mass index (BMI) and received a kidney transplant from younger donors. Cluster 2 recipients were older, unsensitized and had a higher BMI; they received kidney transplant from older donors. Cluster 3 recipients were more likely to be female with a higher PRA. Compared to cluster 1, cluster 2 had lower 5-year death-censored graft (HR 1.40; 95% CI 1.16–1.71) and patient survival (HR 2.98; 95% CI 2.43–3.68). Clusters 1 and 3 had comparable death-censored graft and patient survival. Unsupervised machine learning was used to characterize kidney transplant recipients with prolonged pre-transplant dialysis into three clinically distinct clusters with variable but good post-transplant outcomes. Despite a dialysis duration ≥ 10 years, excellent outcomes were observed in most recipients, including those with moderate sensitization. A disproportionate number of minority recipients were observed within this cohort, suggesting multifactorial delays in accessing kidney transplantation.","Prolonged dialysis duration preceding kidney transplantation has been linked to inferior transplant outcomes [9,10,11]. However, outcomes for kidney transplant recipients with a dialysis duration exceeding 10 years were, overall, excellent. Surprisingly, this cohort comprised a disproportionately high percentage of black (52%) and Hispanic (25%) individuals. Despite the older recipient age in cluster 2 and the moderate sensitization in cluster 3, the majority of recipients in the study received a standard kidney, likely due to their high allocation priority, a result of prolonged dialysis duration.
In our study, we utilized an unsupervised machine learning technique to classify patients with extended dialysis durations before transplantation. These patients were drawn from the OPTN/UNOS database and categorized into three distinct clusters. Cluster 1 was comprised of younger, non-diabetic recipients who obtained kidneys from comparatively youthful donors. Individuals in cluster 2 were relatively older with higher BMIs; however, they were typically non-diabetic and received kidneys with a standard KDPI, and procured from slightly older donors. The majority of recipients in cluster 3 were female individuals with an elevated PRA. Despite the fact that these discrete recipient clusters exhibited varying clinical outcomes, including graft longevity and patient survival, the overall results across all clusters were positive.
The proportion of black and Hispanic recipients having greater than 10 years of dialysis time highlights health care disparities for minorities and the need for improved access to kidney transplantation. This observed percentage of minorities is significantly higher as compared to the prevalent dialysis population [34]. Although the current OPTN dataset does not account for the time an individual remains inactive on the waiting list, it is likely that delays in referral to transplant are responsible for the prolonged dialysis duration across all three clusters.
While delays in referral to transplantation likely play a role for many recipients in all three clusters, recipients in cluster 3 might have experienced additional delays due to moderate sensitization and a scarcity of compatible match offers. This is despite the current allocation system assigning some extra priority to those with moderate sensitization. Arguably, most recipients in cluster 3 were not highly sensitized and would not be expected to have such prolonged waiting times based on PRA alone. It is possible that additional barriers related to an overall lower number of black and Hispanic donors and lack of compatible matches for ethnic minorities may have further compounded this delay [35,36,37,38,39]. These observations underscore yet another barrier that minorities encounter in the transplantation process.
This study has several limitations. Due to the nature of the national registry cohort, it is not possible to identify exact causes of graft loss and patient death. Comparative studies from centers able to provide more granular data would help to confirm these findings. In addition, data specific to patients listed but inactive on the waitlist remains unavailable. Similarly, data on qualifying time is unavailable for comparison. Consequently, it is assumed that delays in referral to transplantation were the primary factor contributing to these extended dialysis times. It is, however, possible that other factors played a role. Another potential limitation of our study is the exclusion of multi-organ transplant patients, including kidney–pancreas recipients, which might limit the broader applicability of our findings. Notably, cluster 2 recipients exhibited higher rates of diabetes, which might suggest that the consideration of pancreas transplantation could indeed have been a contributing factor. Future investigations addressing the outcomes and challenges specific to kidney–pancreas recipients could offer additional insights. In addition, there was a small amount of missing data which could have impacted our clustering results. To reduce the likelihood of bias, we used the multivariable imputation by chained equation (MICE) approach to replace the missing data, which generates plausible values for missing data while preserving the original variability and dataset distribution. While techniques like SMOTE have their merits, our priority was to handle missing data while maintaining the statistical integrity of the original dataset.
To the best of our knowledge, this is the first machine learning approach specifically targeting kidney transplant recipients with extended dialysis durations preceding transplantation. The outcomes of this machine learning clustering allow for a better understanding of characteristics in recipients with prolonged dialysis vintage. Future studies are necessary to individualize pre- and post-transplant care for recipients with prolonged dialysis duration before transplant in order to improve their outcomes.
Leveraging the novel insights offered by this machine learning approach, future studies will assume a crucial role in enhancing the optimization of the kidney transplantation process for patients who have undergone prolonged dialysis before transplantation. These studies should delve deeper into exploring and quantifying the distinct barriers that minorities and sensitized patients encounter during the kidney transplantation process. This is especially important, as these populations have been identified as experiencing extended wait times and reduced match rates. This will require an intersectional approach that blends demographic analysis with a comprehensive grasp of medical, social and systemic barriers. In addition, strategies should be developed to increase the pool of compatible donors for minority and sensitized patients. In terms of future implications, the identification of these unique patient clusters could help reshape the protocols around kidney transplantation. By tailoring the transplantation process based on the distinct requirements of various patient clusters, transplant providers might be able to reduce patients’ dialysis duration, enhance graft survival, and lower mortality rates. This strategy, in conjunction with policy modifications designed to tackle the obstacles encountered by minority and sensitized patients, has the potential to eventually foster a more equitable access to transplantation and yield improved overall outcomes.",https://www.mdpi.com/2075-4426/13/8/1273
Júnior et al. 2020,Logistic Regression Model in a Machine Learning Application to Predict Elderly Kidney Transplant Recipients with Worse Renal Function One Year after Kidney Transplant: Elderly KTbot,"Background. Renal replacement therapy (RRT) is a public health problem worldwide. Kidney transplantation (KT) is the best treatment for elderly patients’ longevity and quality of life. Objectives. The primary endpoint was to compare elderly versus younger KT recipients by analyzing the risk covariables involved in worsening renal function, proteinuria, graft loss, and death one year after KT. The secondary endpoint was to create a robot based on logistic regression capable of predicting the likelihood that elderly recipients will develop worse renal function one year after KT. Method. Unicentric retrospective analysis of a cohort was performed with individuals aged ≥60 and <60 years old. We analysed medical records of KT recipients from January to December 2017, with a follow-up time of one year after KT. We used multivariable logistic regression to estimate odds ratios for elderly vs younger recipients, controlled for demographic, clinical, laboratory, data pre- and post-KT, and death. Results. 18 elderly and 100 younger KT recipients were included. Pretransplant immune variables were similar between two groups. No significant differences () between groups were observed after KT on laboratory data means and for the prevalences of diabetes mellitus, hypertension, acute rejection, cytomegalovirus, polyomavirus, and urinary infections. One year after KT, the creatinine clearance was higher (P = 0.006) in youngers (70.9 ± 25.2 mL/min/1.73 m2) versus elderlies (53.3 ± 21.1 mL/min/1.73 m2). There was no difference in death outcome comparison. Multivariable analysis among covariables predisposing chronic kidney disease epidemiology collaboration (CKD-EPI) equation <60 mL/min/1.73 m2 presented a statistical significance for age ≥60 years (P = 0.01) and reduction in serum haemoglobin (P = 0.03). The model presented goodness-fit in the evaluation of artificial intelligence metrics (precision: 90%; sensitivity: 71%; and F1 score: 0.79). Conclusion. Renal function in elderly KT recipients was lower than in younger KT recipients. However, patients aged ≥60 years maintained enough renal function to remain off dialysis. Moreover, a learning machine application built a robot (Elderly KTbot) to predict in the elderly populations the likelihood of worse renal function one year after KT.","4.1. Biological Plausibility of the Model
The increased prevalence of CKD is associated with the aging population in developed countries, with an increasing number of patients on a waiting list and a higher frequency of KT in elderly people [29]. It has been demonstrated that elderly transplant patients have a better survival rate in comparison to elderly people on a waiting list [30, 31]. Increasing the acceptance of deceased donors, using ECD organs, has become a strategy to increase the number of KT and to reduce the number of patients on HD [31]. According to Knoll et al. [13], the decrease in mortality compared to patients on HD may range from 41% to 60%.

The number of ECD in KT has increased over recent years, even though the survival rate is lower than that of kidneys from standard criteria deceased donors (SCDD) (16). In the present study, patients from both groups were transplanted predominantly with kidneys from SCDD (83.3% and 85.0% in elderly and younger groups, respectively). There was a higher frequency (P = 0.021) of patients in the elderly group (16.7%) transplanted with a kidney from an ECD in relation to the younger group (3.0%).

Immunosuppression protocol used by RHP/PE allocates ECD kidneys primarily to patients aged 50 years and elderly, based on studies that demonstrate a lower survival of ECD grafts [32, 33]. Moreover, while 12% of the KT performed on the younger group was living donor transplants, none of the transplants in the elderly group was performed with a graft from this type of donor. Most elderly patients received the kidney from standard deceased donors in a similar way in despite the immunosuppression protocol adopted.

DM is a multifactorial disease, and its prevalence increases with advancing age, exceeding 10% in patients aged over 60 years [34]. All patients in the present study were immunosuppressed with a regimen that included tacrolimus and prednisone. Both of these drugs are known to induce DM [29], especially during the first six months when they are used in higher doses. Calcineurin inhibitors [31] may cause NODAT with an incidence rate of up to 20.5% [32]. This diabetogenic effect seems to be influenced by aging. NODAT may occur at a relative risk of 90% in patients aged between 45 and 59 years and can reach 160% in recipients aged 60 years and over [29]. Glucocorticoids are involved through increased insulin resistance and hepatic gluconeogenesis. Calcineurin inhibitors, especially tacrolimus, induce a defect in insulin secretion, interfering with the activation of pancreatic beta cells [31]. In spite of this, there was no higher incidence of NODAT among elderly patients, which occurred in one patient (6%), when compared to younger patients. It is possible that the lower number of cases in the elderly group may have influenced this result.

Mismatch analysis demonstrates a statistically significant increase in HLA-A locus incompatibilities in elderly patients. Perhaps, this result may express only a higher population prevalence of the antigen. No biological plausibility was found for such statistical result, and it was also not evidenced in multivariate analysis.

The influence of proteinuria on the progression of kidney disease is well-known in nephrology [35, 36]. A higher occurrence (P = 0.017) of proteinuria (>0.3 g/24 h) was observed in the elderly recipients (56.3%) in comparison to the younger group (24.7%). The aetiology of proteinuria may be related to mTOR inhibitors, chronic graft dysfunction, calcineurin inhibitor nephrotoxicity [37], and diabetic nephropathy [29]. Posttransplant proteinuria is an independent risk factor for cardiovascular risk and a recognized cause of loss of renal function [38]. In the combined analysis of proteinuria and risk factors for proteinuria after KT, we observed that the prevalence of DM in the pre-KT phase was the most statistically significant factor for proteinuria in the elderly group. This result corroborates the need to further intensify the approach to combat DM during the pre-KT phase. Although in the logistic regression, proteinuria has not shown a significant role for the decline kidney function on CKD-transplant patients one year after KT. For this reason, proteinuria was not used in construction of the Elderly KTbot. In addition, the Elderly KTbot modelling demonstrated that proteinuria is not important in the former for renal function of the elderly. Despite this, it does not mean that proteinuria should not be monitored and treated.

Finally, the results of logistic regression show that the probability of having a worse kidney function one year after KT is higher for the elderly and lower for patients with higher haemoglobin. Elderly patient has a 4.67 greater chance of having worse kidney function one year after KT when compared to young patients. On the other hand, the chance of a person having worse kidney function is 1.35 greater for a decrease of one unit of haemoglobin in serum haemoglobin levels.

With regard to renal function one year after KT, the elderly group presented with a mean lower level of CrCl than the younger group but with adequate reserve to remain off RRT. This result is consistent with previous data [39] where levels of CrCl in young KT patients are higher in relation to the outcomes (survival and renal functions) when compared to the elderly patients in the long-term. Nevertheless, as previously discussed, transplantation in elderly patients outweighs the risks related to remaining on HD [40].

In Brazil, Gouveia et al. [41] demonstrated that when compared to the cost of HD modalities, treatment becomes cost-effective after two years of successful KT. The number of KT performed in Brazil is still below the needs of the growing waiting list, including for elderly patients. In 2018, 5,923 KT were performed, equivalent to just 28.8 pmp. Additionally, 81.5% of the 5,486 KT on adults was performed on patients aged between 18 and 59 years, while only 18.4% was on patients aged 60 years and over [12]. Data from SBN reported that 34.3% of patients on chronic HD aged 65 years and over in 2017 [12].

4.2. Kidney Transplantability in the Elderly
There is no formal transplantation barrier for elderly patients. It is not the chronological age that determines the condition of patient but, rather, patient’s performance status. Baby boomers are growing elderly, and therefore, becomes elderly boomers. Concerns regarding the “excess” of children are giving way to the “excess” of elderly people. It is necessary to learn to deal with the aging population [42].

In this context, monitoring and following appropriate risk stratification, preoperative protocols, and pre-KT consultations should be directed towards patient transplantability. Most elderly patients are not placed on the list of recipients simply because they are not even referred for pre-KT consultations. It may even be suggested that prejudice and involuntary discrimination are thus being exercised, based on the age of patients, presenting a genuine social barrier against elderly people, and thereby a reflection of ageism [43, 44].

For elderly people, “aging associated with RRT is a daily challenge in the search for quality and time of life” [45]. Within this context, they may be victims of ageism, a term coined in 1969 by Robert Neil Butler, in an article published in The Gerontologist, to identify discriminatory practices against elderly people, including institutional and political intolerances. These positions may perpetuate stereotypes about elderly people, with little regard for their individual competences [43, 46].

According to data from the Brazilian Ministry of Health provided and extracted from the Brazilian SNT until December 2019, of the 25,163 patients on the waiting list for KT, only 2911 (9.5%) were aged 60 years or over (Supplementary Material SM2). This proportion varies according to the region of the country, from 11.5% in the Midwest to 19.7% in the Southeast, directly proportional to the per capita income and life expectancy of the region. Although this difference may be related to the historical coexistence of socially active elderly individuals in more developed regions [5]. Brazilian population, as in other countries, is in the process of aging, and this phenomenon is irreversible [8, 47]. Efforts should be made to increase the frequency of KT in this population, and in this sense, our results corroborates others [48] and demonstrated that it is possible to transplant elderly people without damage.

4.3. Ageism, New Concepts, and New Best Practices on Elderly Patients
Ageism (current term) leads to the exclusion of elderly people within their communities. It is everywhere and perhaps the most “normalized” form of prejudice, since, unlike racism or sexism, it is not widely opposed [44]. Due to this perspective, elderly people are denied work and are restricted to social services and stereotyped in the media [41]. Brazil needs to learn from other countries to see elderly as an integral and active part of society [8].

However, learning to deal with the geriatric patient should be part of the current situation in all countries that are experiencing the same aging characteristics of Brazil. In recent years, several guidelines have been developed to assess the patient globally in surgical procedures. Multiprofessional teams in satellite clinics should be trained to use geriatric and frailty screening and start to understand considerations of applications of preoperative frailty scores in clinical practice. In case of doubt or lack of training, the transplant centre should be able to assess and apply the geriatric performance assessment scores [49]. What should be avoided is the impossibility of elderly patient to have access to the benefit of KT. By virtue of insufficiency or absence of technical knowledge about age and aging [50], advanced age should not be an absolute contraindication for KT in carefully selected patients with ESRD. KT offers a higher survival coefficient than HD treatment even in the elderly. Another point that should be taken into account is elderly recipients on the waiting list have a higher mortality rate when compared to KT recipients of similar age [51].

According to this context, new concepts on geriatric patient assessment have been proposed by the societies of surgeons and anaesthesiologists. On the base of evidence from the preoperative evaluation of the elderly are the well-established forecast surgical outcomes including complications, length of stay, functional dependence, and death [52]. A comprehensive geriatric assessment offers a much more exhaustive picture of the individual physiological fitness and functional reserves in comparison with traditional evaluation. According to the guidelines [52–56] and principles of geriatric domain assessment, a minimum preoperative assessment for the elderly should include functional assessment for activities of daily living [57], instrumental activities of daily living [58], neurocognitive [59], mini-mental state exam [60], and psychoaffective screening [61], nutritional status [62], comorbidity cumulative illness rating scale for geriatrics [63], identification of potentially inappropriate medications, risk of postoperative delirium, risk of fall, availability of social support, and frailty.

In summary, clinicians, nephrologists, and multiprofessional teams should be able to include elderly patients on the waitlist of KT recipients, assessing their clinical, psychic, and social conditions. The path to transplantation in elderly people, therefore, depends on their being viewed as socially active. It is necessary to replace ageism with new guidelines and preoperative geriatrics best practices in the evaluation of geriatric population. Age is not a risk factor for rejection in the first year. Elderly should be given the right to receive a KT or at least be registered on the waiting list. Even with socioeconomic inequalities, it is still possible to offer quality medicine to the population. The principles present in many National Health Systems are universality (everyone should have the right) and equity (equal treatment for equals and unequal treatment for unequal’s), and those who need more help should have more support. That is, the main point is the elderly needs to be treated with more equity both in the indication of the KT and in the inclusion of the elderly in the waiting list of the KT.

4.4. Logistic Regression Multivariate Modelling, Machine Learning, and Small Data
Logistic regression modelling by stepwise forward is able to identify with a degree of precision and expressive criticality and the probability of elderly recipients to develop renal function CKD-EPI < 60 mL/min/1.73 m2 one year after KT. The model that gives rise to the Elderly KTbot has a precision (positive predictive value) of 90% and a recall (sensitivity) of 71%, as it combines the harmonic average of precision and recall.

The F1 measure score is considered one of the best discriminants of the model’s power artificial learning and was 0.76 in this research. The value closer to 1 demonstrates greater predictive power of the machine learning model. It is worth mentioning that despite the limited number of patients coming from the waiting lists, the result of the centre was quite expressive without deaths in the elderly up to 1-year-olds and with the ability to remain in outpatient conservative therapy.

The main point of the study is to demonstrate a way to study its population of transplant recipients and using methodologies that combine clinical experience, biological plausibility, and the learning machine. It is possible to create tools to aid in prediction and bedside treatment. The construction of robots can help to understand the dynamic behaviour of the data, even in small unbalanced groups and with restricted numbers of samples. Artificial intelligence is capable of handling both big data [64] and small data [65], provided that supervised and unsupervised learning processes have human participation. Clinical thinking is able to enhance the fundamental and explanatory random covariates that relate to the primary outcomes [65].

4.5. Limitations
It is a single-centre cohort using secondary data and restricted number of elderly patients. This fact is probably related to nonreferral of elderly to pretransplant consultations, which consequently leaves them out of the waiting list for KT. This study tries to demonstrate not only the possibility but also the social need to register elderlies on the waiting list. In this sense, our group guaranteed confidentiality through encrypted platforms and presented a model that can be reproduced as a predictive tool for the elderly.

In addition, the research used a predominantly mixed population of Brazilian Amerindians, multiracial (afro descendants), descendants of Germans, Dutch, British, Jews, Arabs, and Portuguese according to the characteristics of colonization [66], and phylogeography of Pernambuco state of Brazil [67]. On the other hand, the introduction of probabilistic metrics for machine learning analysis increased the safety of the developed multivariate model, as well as enhances the safety of using the Elderly KTbot for elderly KT populations.
",https://www.hindawi.com/journals/jar/2020/7413616/
Tang et al. 2017,Application of Machine-Learning Models to Predict Tacrolimus Stable Dose in Renal Transplant Recipients,"Tacrolimus has a narrow therapeutic window and considerable variability in clinical use. Our goal was to compare the performance of multiple linear regression (MLR) and eight machine learning techniques in pharmacogenetic algorithm-based prediction of tacrolimus stable dose (TSD) in a large Chinese cohort. A total of 1,045 renal transplant patients were recruited, 80% of which were randomly selected as the “derivation cohort” to develop dose-prediction algorithm, while the remaining 20% constituted the “validation cohort” to test the final selected algorithm. MLR, artificial neural network (ANN), regression tree (RT), multivariate adaptive regression splines (MARS), boosted regression tree (BRT), support vector regression (SVR), random forest regression (RFR), lasso regression (LAR) and Bayesian additive regression trees (BART) were applied and their performances were compared in this work. Among all the machine learning models, RT performed best in both derivation [0.71 (0.67–0.76)] and validation cohorts [0.73 (0.63–0.82)]. In addition, the ideal rate of RT was 4% higher than that of MLR. To our knowledge, this is the first study to use machine learning models to predict TSD, which will further facilitate personalized medicine in tacrolimus administration in the future.","Compared with traditional dosing strategies in clinic, the current study was successful in providing a novel approach that can predict TSD more accurately and conveniently. In general, the performances of the 9 algorithms were similar in predicting TSD. While the best performance was observed in RT model in this study, comprehensive evaluation of these algorithms in various studies is needed to come to a final conclusion. It should also be noted that the current study was performed in Chinese, studies in other ethnic groups may come to different results.

The most influential factor in this study was CYP3A5 genotype. The SNP 6986 A > G on the CYP3A5 gene results in absence of function protein. Carriers of homozygous 6986 G allele (designated as CYP3A5*3) have no CYP3A5 activity, which impair the whole-blood concentration of tacrolimus24 and subsequently the time required to reach target concentration3. None of the included ABCB1 SNPs were found any significant impact on the algorithm. In fact, previous researches checking the association between ABCB1 genotypes and tacrolimus dosage have come to conflicting results24,25,26.

Our results indicated that the intermediate dose range exhibited better accuracy (lower MAE and higher ideal rate) than that in the high- and low- dose ranges. Nevertheless, patients in this dose range are less likely to benefit from statistical models based on pharmacogenomics. In practice, patients who require extreme dose administrations (or whom grouped in the high- and low- dose ranges) are more likely to face overdose or underdose and hence suffer from adverse clinical consequence22. Therefore, better prediction of extreme dose ranges are needed to present real help to those patients.

Whilst machine learning techniques demonstrated their capability in solving inferential problems by self-adjust their structure when encounter errors, as well as dealing with numerous variables simultaneously20, we should be noted that they are still far from omnipotent in clinical use. The relationship between dependent variables and independent variables are very complicated in all these statistical algorithms, and the existence of gene-gene and gene-environment interactions bring more challenge to the researchers27,28,29. Inclusion of larger number of genotypic variables in a predictive model may be helpful to obtain a better performance, but this may lead to addition of redundant data and may hinder its application in clinical practice21. The complicated situation of real patients should be well considered, as additional comorbidity and interacting drugs are always the case, which may not be completely included in the models30. Therefore, even the statistical models are utilized to increase the predictive accuracy of TSD, continuous monitoring of drug concentration is still needed at the moment.

There are some limitations in this study, no other potentially important factors were included, such as smoking, alcohol consumption and other genetic factors; secondly, data regarding tacrolimus initial doses or adverse reactions were not gathered in the study, only data about stable therapeutic doses were considered; in addition, using of p-value threshold to select significant SNPs may be not enough to generate most complementary SNP set21.",https://www.nature.com/articles/srep42192#Sec7
Gandelman et al. 2019,Machine learning reveals chronic graft-versus-host disease phenotypes and stratifies survival after stem cell transplant for hematologic malignancies,"The application of machine learning in medicine has been productive in multiple fields, but has not previously been applied to analyze the complexity of organ involvement by chronic graft-versus-host disease. Chronic graft-versus-host disease is classified by an overall composite score as mild, moderate or severe, which may overlook clinically relevant patterns in organ involvement. Here we applied a novel computational approach to chronic graft-versus-host disease with the goal of identifying phenotypic groups based on the subcomponents of the National Institutes of Health Consensus Criteria. Computational analysis revealed seven distinct groups of patients with contrasting clinical risks. The high-risk group had an inferior overall survival compared to the low-risk group (hazard ratio 2.24; 95% confidence interval: 1.36-3.68), an effect that was independent of graft-versus-host disease severity as measured by the National Institutes of Health criteria. To test clinical applicability, knowledge was translated into a simplified clinical prognostic decision tree. Groups identified by the decision tree also stratified outcomes and closely matched those from the original analysis. Patients in the high- and intermediate-risk decision-tree groups had significantly shorter overall survival than those in the low-risk group (hazard ratio 2.79; 95% confidence interval: 1.58-4.91 and hazard ratio 1.78; 95% confidence interval: 1.06-3.01, respectively). Machine learning and other computational analyses may better reveal biomarkers and stratify risk than the current approach based on cumulative severity. This approach could now be explored in other disease models with complex clinical phenotypes. External validation must be completed prior to clinical application. Ultimately, this approach has the potential to reveal distinct pathophysiological mechanisms that may underlie clusters.","Seven unique chronic GvHD patients’ phenotypes were revealed through a machine-learning workflow and successfully recapitulated with a clinically applicable decision-tree tool. The revealed groups of patients were stratified for overall survival and a unique sclerotic phenotype with different time from stem cell transplantation to development of chronic GvHD was found. The clusters of patients we describe may overcome the limitations of the current NIH classification system of disease severity which does not account for combinations of organ involvement and did not stratify survival in this cohort.

The process of applying this computational workflow to chronic GvHD patients yielded clinically applicable insights. Training analyses revealed that symptom-based lung score did not contribute to clustering and that cluster stability was improved without the lung score (Online Supplementary Figures S2 and S3). In the NIH symptom-based lung score, a score from 0-3 is assigned based on the degree of activity needed to cause dyspnea with a requirement for oxygen being scored 3.3 The fact that this symptom-based lung score did not contribute to patient clustering may be due to the subjective nature of the score and suggests that it reflects overall well-being rather than organ-specific involvement. However, it is important to note that the NIH symptom-based lung score has been associated with patients’ outcomes, including non-relapse mortality and overall survival, in an analysis that also included chronic GvHD Consortium patients.27

Clusters of patients identified by the computational workflow were associated with different clinical risk, demonstrated by differences in overall survival. Clusters of patients in the high-risk group were enriched for skin and liver involvement. A skin score of 3 and liver score of 3 have previously been shown to be associated with non-relapse mortality in an analysis that included patients in this cohort.10

Groups identified by the decision tree continued to stratify survival, with patients in the intermediate-risk group having a 1.8-fold higher risk of mortality compared to those in the low-risk group and patients in the high-risk group having a 2.8-fold higher risk of mortality. Individual high-risk clusters, i.e., Clusters 6 and 7, also independently stratified overall survival when identified by the decision tree. Importantly, the decision tree stratified risk of mortality independently of previously defined risk factors for chronic GvHD, including NIH-Severity. Notably, platelet count was a risk factor that continued to stratify risk significantly. Overall, the decision tree has the potential to be applied in the clinical setting to assess patients’ phenotypes, once further validation in prospective, independent cohorts has been completed. Additionally, this decision tree can be applied in the research setting to large cohorts of patients.

Disease trajectory differed in the decision-tree-identified clusters, most notably for Clusters 2, 6 and 7. The time from stem cell transplantation to development of chronic GvHD was different in Cluster 2, a sclerotic phenotype. This is a clinically relevant and potentially biologically distinct cluster of patients. Longer time to chronic GvHD development is a known clinical finding in patients with sclerotic chronic GvHD.285 Previous work defined patients with sclerotic chronic GvHD as having at least one of the following: sclerosis, fascia or joint involvement.3029 This literature did not comment on the sclerotic phenotype as one with “de-enrichment” of liver and mouth involvement or take into account the combination of multiple sclerotic features.3029 The combination of enriched and de-enriched features we describe may enable better association with biomarkers and treatment response.

Cluster 6, a mixed phenotype, high-risk cluster, was a novel high-risk cluster revealed by the decision tree. This cluster was defined by enrichment for mouth, eye, and gastrointestinal tract involvement. Notably, this cluster required the highest number of questions on the decision tree to reach, indicating that it was poorly defined and required that other clusters were ruled out to find patients in this phenotypic group. Patients in this cluster had significantly worse overall survival when compared to all those in all other clusters combined. A caveat is that, in stability analysis of the machine-learning workflow, Cluster 6 was not highly stable, but it did recur through all repetitions of analysis (Online Supplementary Figure S5). The combination of these areas of organ involvement has not been previously cited as a risk factor for adverse outcomes in chronic GvHD and should be further explored through cellular analyses for biomarkers and evaluated in continued validation cohorts.

Patients in Cluster 7 derived from the decision tree, a liver predominant-severe phenotype, also had a different disease trajectory when compared to patients in other clusters in that they had a significantly worse overall survival than patients in all other clusters combined. This decision-tree-derived cluster is supported by previous research showing that severe elevation of liver enzymes is a known risk factor for adverse outcomes in chronic GvHD.10

Prognostication by clustering is distinct from prognostication by individual organ scores alone. For example, in the machine-learning analysis, Cluster 5 lacked liver involvement and was a high-risk cluster, while high-risk Cluster 6 and Cluster 7 were specifically enriched for liver involvement. This supports the concept that this single organ score does not confer unidirectional low or high risk within the clusters. Furthermore, Liver enrichment was seen in multiple low-risk clusters and one high-risk cluster. Clustering is unique in that it is not an individual organ score or characteristic but rather combinations of organ involvement and the specific absence of organ involvement that drive cluster formation and likely prognosis. Another example of this is that mouth enrichment was seen in both an intermediate-risk cluster (Cluster 4) and high-risk cluster (Cluster 6). Cluster 6, a high-risk cluster, comprises mouth, eye and liver enrichment; these individual enrichment types appear in low-risk clusters but it is perhaps the combination that makes this a high-risk cluster. However, we cannot rule out that gastrointestinal tract enrichment, uniquely present in Cluster 6, is not the driving force of adverse outcomes.

A limitation of the machine-learning approach is that it is not possible to add new patients to this analysis without shifting the current clusters. This was overcome by the decision-tree approach. Validation with an external cohort as well as comparison with other risk stratification tools for chronic GvHD31 should further strengthen the findings of the computational and decision-tree analyses. We were unable to analyze whether clusters predicted response to therapy, as this was an observational cohort in which patients were on any systemic therapy at study entry. Thus, treatment response is an outcome of interest in assessing the utility of machine learning for chronic GvHD outcome stratification. An external validation cohort is pending for this analysis. External validation of machine-learning approaches is the gold standard, and external validation is necessary prior to clinical application of the findings.

These results have the potential to be applied to stratify risk in the clinical setting, enhance the current chronic GvHD classification system, refine inclusion criteria for phase 2 trials, and guide biomarker discovery for more specific therapeutic targets. The distillation of machine-learning knowledge into a decision tree increases the feasibility of clinical application of the clusters. However, the clusters have not been externally validated, and this step should be explored before clinical application.

Lastly, this a flexible machine learning-inspired work- flow with numerous potential applications. The stability of the clusters suggests that this approach will be highly useful in revealing groups not only for this disease but for others that have complex phenotypes. Although the endpoint for this analysis was overall survival, this workflow could be applied to explore whether clusters of patients differ in treatment response or composite chronic GvHD endpoints, such as failure-free survival. Additionally, this workflow has the potential to be applied to other human diseases with complex classification systems such as myelodysplastic syndrome and brain tumors. This approach may change the classification of human disease by revealing otherwise unapparent, clinically relevant patterns.",https://haematologica.org/article/view/8736
Pérez-Sanz et al. 2021,Efficiency of Machine Learning Algorithms for the Determination of Macrovesicular Steatosis in Frozen Sections Stained with Sudan to Evaluate the Quality of the Graft in Liver Transplantation,"Liver transplantation is the only curative treatment option in patients diagnosed with end-stage liver disease. The low availability of organs demands an accurate selection procedure based on histological analysis, in order to evaluate the allograft. This assessment, traditionally carried out by a pathologist, is not exempt from subjectivity. In this sense, new tools based on machine learning and artificial vision are continuously being developed for the analysis of medical images of different typologies. Accordingly, in this work, we develop a computer vision-based application for the fast and automatic objective quantification of macrovesicular steatosis in histopathological liver section slides stained with Sudan stain. For this purpose, digital microscopy images were used to obtain thousands of feature vectors based on the RGB and CIE L*a*b* pixel values. These vectors, under a supervised process, were labelled as fat vacuole or non-fat vacuole, and a set of classifiers based on different algorithms were trained, accordingly. The results obtained showed an overall high accuracy for all classifiers (>0.99) with a sensitivity between 0.844 and 1, together with a specificity >0.99. In relation to their speed when classifying images, KNN and Naïve Bayes were substantially faster than other classification algorithms. Sudan stain is a convenient technique for evaluating ME in pre-transplant liver biopsies, providing reliable contrast and facilitating fast and accurate quantification through the machine learning algorithms tested.","Our goal was to develop an application which is able to establish an objective and reliable value of macrovesicular steatosis from representative sections of pre-transplant liver donor biopsies stained with Sudan—a fat-specific staining procedure—with minimum requirements, in terms of image quality and processing time. For this purpose, we tested several classification machine learning algorithms, in order to determine which algorithm is the most suitable for application. To the best of our knowledge, this is the first report in which several machine learning algorithms were tested for the automatic analysis of fat-specific dye stained sections for biomedical purposes. Moreover, we developed a graphical user interface (GUI) implementing the algorithms discussed in this work. It also allows for the the training of new models, based on the same algorithms. The development framework used was Shiny—a web development framework based on R—which allows for near-native integration of all Python code necessary for generating the models and analyzing the images. The simple and intuitive design makes it easy for the end-user to quickly quantify steatosis.
Although the number of potential donors for liver transplant has increased, the number of canceled transplantations due to a high grade of ME have also risen [20]. As this parameter still must undergo a subjective evaluation, the possibility of an error of criteria cannot be excluded; even in the case of analysis by an expert pathologist. Liver transplantation is an extremely complex surgery, the success of which depends on the time consumed between organ extraction from the donor and its reperfusion into the patient. Thus, the intraoperative histopathologic evaluation—which usually involves sampling, sectioning, staining, examination, and diagnosis—must be assessed in less than 30–45 min [21]. As the fastest fixation and paraffin embedding procedures usually require 3–4 h, the use of frozen samples is mandatory in this case. H & E is usually the standard procedure for general evaluation, which is easy and quick to perform and usually provides a good contrast to evaluate many parameters used to establish the quality of the graft for transplant. Nevertheless, this procedure does not stain fat, and the possibility of overestimating ME due to artifacts produced during the processing of the frozen biopsies (e.g., water droplets, holes, and so on) may be not discarded [3,10]. Taking this into account, coupled with the fact that ME determination is strongly observation-dependent, we find that the risk of error of judgement can increase significantly, with severe consequences, regardless of the final decision.
Thus, improvement of the staining procedure and the accuracy of the steatosis determination, by transforming an estimated determination into a quantitative one, in the shortest time possible may allow for a drastically diminished possibility of error, an increase in the number of viable organs, and the establishment of more accurate outcomes, in terms of viability of the graft.
As the use of frozen sections to the immediate diagnosis is mandatory, our first goal was to use an alternative staining procedure, which may replace the H & E stain and allow for fat to be stained specifically. To this end, we decided to use the Sudan stain, as it can be performed on frozen sections, is a fast and easy stain procedure, and is fat-specific, making it chromatically easy to differentiate fat from non-fat structures. A possible disadvantage of this stain procedure is that the value of steatosis can be overestimated by direct examination, especially when the analysis depends on inexperienced pathologists. We did not observe significant variations in ME values during the validation process, probably due to the use of two experienced pathologists specialists in liver transplantation.
Once we had solved this problem, our next issue was to determine which is the best machine learning algorithm for use in the development of our analytic tool. As all reported image analysis tools have been based on the analysis of H & E stained sections [11,12,13,14], these algorithms are focused on the measurement of numerous parameters, which try to differentiate structures (i.e., fat vacuoles vs. non-fat vacuoles and unspecific structures) with similar shape and color (i.e., round unstained/white structures). As there have been no previous reports considering the use of Sudan stain for the automatic determination of ME, we decided to use six of the most-used algorithms for image analysis [22], in order to determine which is the best option—in terms of efficacy and time—for use in a new and specific image system based on Sudan-stained section analysis. Additionally, we took into account the time used for the analysis, as this parameter should not be extended, in order to assure the efficiency of the procedure. Thus, the use of high-resolution scanned images may be not useful in this particular case, as the time required to obtain and process such images (which are near 1 GB in size each) may be not applicable to study one parameter, which must be objectively determined in 5–10 min at maximum. For this reason, our tool does not currently use whole slide images, although we are considering their use as future work, provided that their processing time can be optimised. Therefore, our goal was to develop an image analysis application with the best machine learning algorithm, which is able to establish an objective value of ME using the lowest image resolution possible, in order to optimize either the processing time and/or the requirements of the system employed in the analysis (potentially even providing the possibility of performing through an on-line web application). The evaluated algorithms showed high performance, in terms of image classification. In the training/testing phase, the AUC obtained in all cases was significantly high (>0.98), which was virtually unaffected by the number of pixels used. Only the 1000 pixel data set decreased the AUC of the Keras algorithm. In the trials carried out, it was shown that the AUC for 1000 pixels was more affected by the pixels selected in the random sampling than by the number of pixels used.
Concerning the time spent by each classifier to be trained, it is worth mentioning the outstanding difference between Keras and the remaining algorithms. Furthermore, for each classifier, the time increase was more significant from 50,000 pixels onwards; except with KNN and NB, whose times were barely affected. Thus, between 10,000 and 50,000 pixels, a compromise can be achieved between time spent in training and robustness in the random sampling of pixels.
In the classification step of a real image, KNN and NB were the fastest, regardless of the number of threads. Even in the case of an image with 4 times more pixels than another, the duration was shorter than 1 s. On the other hand, RF and SVM were significantly influenced by both the size of the image and the number of threads involved in the classification. Nevertheless, from six threads on, there was no significant reduction in the time spent classifying the image on the equipment used; thus, it may be unnecessary to invest more computational resources, when the performance is not going to be enhanced substantially.
The global result, when comparing the manually classified and the same automatically classified image with the different classifiers, yielded good results overall. The ratio of positives over the total number of images (i.e., accuracy) was close to 1 in every case. With regard to the sensitivity of Keras, it was the most accurate, with 100% success; while KNN, with 0.844 accuracy, was the most mistaken. The specificity, similar to the accuracy, remained very high for all the classifiers. The main limitation regarding the use of these algorithms was observed in those cases with an extremely high infiltration of fat (70–80%). In those cases, the sensibility of the algorithms to pixel selection was increased, possibly due to the fat infiltration observed within portal and stromal areas. In such cases, alternatives like the use of morphological segmentation algorithms would be helpful to establish accurate values of ME. Another limitation was based on the fact that we did not compare the accuracy of these classification algorithms with other lipid staining methods, such as the oil red procedure; however, our results indicate that the use of specific fat staining procedures, such as Sudan, may be a good choice for the automatic determination of ME in pre-transplant liver biopsies, using minimal requirements with optimal results for those cases with low–average amounts of fat infiltration. Such cases are those in which the pathologist may experience problems in establishing an accurate value of ME.
We conclude, based on these results, that Sudan stain is a suitable and value tool for the evaluation of ME in pre-transplant liver biopsies, as it is suitable for frozen sections, quick, fat-specific, and offers good contrast, which can allow for easy differentiation of fat vacuoles from non-fat vacuoles and unspecific structures; while H & E stain can be used for the evaluation of all other parameters, such as inflammation, infection, necrosis, tumors, and pigment deposits. We propose the introduction of this stain as the technique to use in the evaluation of ME in such biopsies. As our goal was to develop an automatic analytic system which may determine the amount of ME in a stained section, this also may save valuable time for the pathologist, in terms of evaluating the quality of the graft, and minimize (or even eliminate) the possibility of criteria error in the evaluation of this important parameter. Additionally, we developed our application based on the optimization of the quality and the size of the images, in order to optimize the time required for analysis and the computational cost. Regarding the algorithms analyzed, Naïve Bayes and KNN were the best algorithms for the data set on which they were evaluated. Both displayed remarkably high levels of accuracy, sensitivity, and specificity, while also proving to be the fastest in both the training and classification steps, with minimal consumption of hardware resources.
In the future, these algorithms may be implemented in specialized and automatized image analysis applications for liver transplantation, with specific use in Sudan-stained sections.",https://www.mdpi.com/1424-8220/21/6/1993
Hallaji et al. 2021,Adversarial Learning on Incomplete and Imbalanced Medical Data for Robust Survival Prediction of Liver Transplant Patients,"The scarcity of liver transplants necessitates prioritizing patients based on their health condition to minimize deaths on the waiting list. Recently, machine learning methods have gained popularity for automatizing liver transplant allocation systems, which enables prompt and suitable selection of recipients. Nevertheless, raw medical data often contain complexities such as missing values and class imbalance that reduce the reliability of the constructed model. This paper aims at eliminating the respective challenges to ensure the reliability of the decision-making process. To this aim, we first propose a novel deep learning method to simultaneously eliminate these challenges and predict the patients' survival chance. Secondly, a hybrid framework is designed that contains three main modules for missing data imputation, class imbalance learning, and classification, each of which employing multiple advanced techniques for the given task. Furthermore, these two approaches are compared and evaluated using a real clinical case study. The experimental results indicate the robust and superior performance of the proposed deep learning method in terms of F-measure and area under the receiver operating characteristic curve (AUC).","This paper addressed data complexities such as class imbalance and missing values within the raw medical data, while predicting the survival chance of the liver transplantation patients. This facilitates the process of training a computational model used in automated liver allocation systems and enhance their reliability. To this aim, a novel deep learner is proposed, by resorting to adversarial learning, to eliminate these challenges simultaneously within an integrated scheme. In addition, a multi-step hybrid framework is designed by employing multiple advanced techniques for CIL, imputation, and classification within different modules. A comparison on the combinations of these techniques is performed to determine the best hybrid combination for the task at hand. Furthermore, the proposed deep learner is compared with these techniques in terms of post-imputation AUC and F-measure. Either of these models can be used to confidently predict the survival chance of patients, which in turn increases the efficiency of the scoring systems. For the sake of evaluation, a real clinical case study is selected on patients registered for liver transplantation. Finally, the experimental analysis indicates the superior performance of the proposed AICN method. Future works can extend AICN for recommender systems of different applications. Furthermore, AICN can be improved in terms of computational efficiency, and, thus, be used for big data analytics. Besides, despite the success of AICN compared to its rivals, robustness against missing value can still be improved to reach an F-measure close to 100 percent even for high missing ratios.",https://ieeexplore.ieee.org/document/9432931
Gupta et al. 2022,Prolonged hospital length of stay after pediatric heart transplantation: A machine learning and logistic regression predictive model from the Pediatric Heart Transplant Society,"Background
Heart transplantation (HT) is the gold standard for managing end-stage heart failure. Multiple quality metrics, including length of stay (LOS), have been used in solid organ transplantation. However, limited data are available regarding trends and factors influencing LOS after pediatric HT. We hypothesized that various donor, peri-transplant and recipient factors affect LOS after pediatric HT.

Methods
We analyzed patients <18years at time of HT from January 2005 to December 2018 in the Pediatric Heart Transplant Society database, and examined LOS trends, defined prolonged LOS (PLOS = LOS>30days after HT), identified factors associated with PLOS and assessed outcomes.

Results
Of 4827 patients undergoing HT, 4414 patients were discharged and included for analysis. Overall median LOS was 19days[13,34]. Median LOS was longer in patients with congenital heart disease(CHD = 25days[15,43] than with cardiomyopathy(CM = 17days[12,27] across all ages. Median LOS in age <1year was 26-days[16,45.5] and in age >10year was 16days[11,26]. PLOS was seen in 1313 patients(30%). Patients with PLOS were younger, smaller and had longer CPB times. There was no difference in utilization of VAD at HT between groups, however, ECMO use at listing(8.45% vs 2.93%,p < 0.05) and HT was higher in the PLOS group(9.22% vs 1.58%,p < 0.05). PLOS was more common in patients with previous surgery, CHD, single ventricle physiology, recipient history of cardiac arrest or CPR, end organ dysfunction, lower GFR, use of mechanical ventilation at HT and Status 1A at HT.

Conclusion
We present novel findings of LOS distribution and define PLOS after pediatric HT, providing a quality metric for individual programs to utilize and study in their practice.","Discussion
Our study is the first to provide insight into LOS trends after pediatric HT. The use of LOS as a quality metric is clearly important as it encompasses various domains of quality.1,24,25 Furthermore, an improved understanding of factors affecting LOS can allow transplant centers to estimate their expected LOS, identify patients at risk for PLOS and counsel families accurately. Due to a paucity of LOS data in pediatric HT, heterogeneity of diagnoses and unique characteristics of pediatric patients unlike adults; extrapolation of adult data to estimate pediatric LOS trends is inherently flawed. Herein, we provide a rational basis for selecting 30 days as a “cut-off” for defining PLOS, evaluate the factors affecting LOS and develop a model to predict the probability of PLOS in pediatric HT patients.

Rising healthcare costs have led to the inclusion of quality metrics by payors for reimbursement decisions, leading to an increased emphasis on adapting cost-effective approaches with best clinical outcomes. Overall, HT is a resource intense field with a reported 3.7-fold variation(range:$329,477-$1,226,507) independent of center volume in HT hospitalization costs across the USA.26 With inpatient hospitalization accounting for 25%-50% of total costs, LOS becomes a critical modifiable factor in reducing the cost of hospitalization. Our study improves the understanding of factors affecting LOS, allowing development of targeted approaches to minimize the factors leading to PLOS and thereby reducing costs.

We identified factors which impact LOS directly and/or indirectly, including patient age, BSA, and diagnosis (CHD and single ventricle physiology). These nonmodifiable factors may relate to increased peri-transplant clinical complexity in younger patients with CHD predisposing to longer recovery times. Not surprisingly, patients with PLOS were a sicker cohort, with higher priority listing statuses, end-organ dysfunction, greater reliance on mechanical ventilation, and greater ECMO utilization. Though we did not identify any significant differences in the distribution of LOS seen in VAD supported patients over the span of the study, it is possible that the risks of increased HT surgical complexity in this VAD population were offset by the rehabilitation and clinical stability that device support offered. A higher incidence of PLOS was seen with use of ECMO at the time of listing or HT. Together, this suggests that early use of VAD over ECMO may help reduce LOS. Overall, the interconnectedness of these factors, a low- and high-risk group being clinically unique, may even suggest that the risk stratification for different programs may need to be individualized to account for the complexity of patients they care for.

In adult analyses, prior sternotomy and renal dysfunction are independently associated with LOS >14 days and discharge either <7days or >14 days after HT was associated with sub-optimal outcomes.27,28 Crawford et.al. derived a Prolonged Hospitalization After HT Score in adults wherein older recipient, females, poor Karnofsky score, presence of diabetes, renal dysfunction, dialysis, mechanical ventilation, ECMO, previous cardiac surgery, VAD, pulmonary hypertension, longer waitlist times, older donor, recipient-donor sex mismatch and ischemic time >4hours were used to predict a longer LOS.4

We identified many overlapping as well as unique factors impacting LOS in pediatric patients. One such factor identified in our model is age at HT, with younger patients being more likely to have PLOS. This finding may relate to increased clinical complexity of lower weight and/or smaller BSA patients. Similarly, patients with CHD, for which a propensity exists as a HT indication in this younger population, are more likely to have previous sternotomies which further increases the risk of PLOS. Single ventricle physiology was associated with PLOS, but it was not an independent factor in predictive modeling due to overlap of various factors in younger children. Conversely, patients with CM are generally older with no previous surgeries, and thus post-operative recovery is shorter contributing to a shorter LOS compared to CHD. Similar to adults, organ dysfunction as depicted by elevated bilirubin and renal dysfunction lengthens the recovery period leading to PLOS. However, in contrast to the adult literature, waitlist duration was not associated with PLOS in our population.

For development of a risk prediction model, we used traditional (SLR) as well as novel machine learning (GB and RF) methods. In our study, the following factors were finally included in the prediction model: recipient age, CPB time, mechanical ventilation at HT, surgeries prior to listing and use of VAD and/or ECMO at time of HT. Though use of machine learning techniques such as GB are often considered superior to the traditional SLR methods for predictive modeling, our analysis showed no such benefit. This risk prediction model, a first for pediatric patients, allows a quick estimation of the probability of PLOS for individual patient use.

Limitations
Limitations of our study include use of 30 days as the cutoff for PLOS. This was chosen after a careful analysis of the LOS distribution of the overall cohort. Knowing these limitations, we chose this cutoff as it also aligns well with other clinically important parameters including 30-day survival and readmission data. This study is additionally subject to limitations inherent to analyses of registry data, as they are bound by preexisting variables. However, PHTS provides a comprehensive, granular database using an event driven approach to minimize this risk. Additional confounders like social risk factors and medical conditions not delineated in the dataset may also have an impact on LOS. Furthermore, the impact of physician behavior, clinical management and other institutional practices on LOS cannot be assessed due to the nature of this database. Though exclusion of patients who died during the HT hospitalization may have impacted the results of this study, the risk of inappropriately truncating LOS data in such a complex group superseded their inclusion.",https://www-sciencedirect-com.myaccess.library.utoronto.ca/science/article/pii/S1053249822019581?via%3Dihub
Alejo et al. 2022,Predicting a Positive Antibody Response After 2 SARS-CoV-2 mRNA Vaccines in Transplant Recipients: A Machine Learning Approach With External Validation,"Background.
Solid organ transplant recipients (SOTRs) are less likely to mount an antibody response to SARS-CoV-2 mRNA vaccines. Understanding risk factors for impaired vaccine response can guide strategies for antibody testing and additional vaccine dose recommendations.

Methods.
Using a nationwide
Using a nationwide observational cohort of 1031 SOTRs, we created a machine learning model to explore, identify, rank, and quantify the association of 19 clinical factors with antibody responses to 2 doses of SARS-CoV-2 mRNA vaccines. External validation of the model was performed using a cohort of 512 SOTRs at Houston Methodist Hospital.

Results.
Mycophenolate mofetil use, a shorter time since transplant, and older age were the strongest predictors of a negative antibody response, collectively contributing to 76% of the model’s prediction performance. Other clinical factors, including transplanted organ, vaccine type (mRNA-1273 versus BNT162b2), sex, race, and other immunosuppressants, showed comparatively weaker associations with an antibody response. This model showed moderate prediction performance, with an area under the receiver operating characteristic curve of 0.79 in our cohort and 0.67 in the external validation cohort. An online calculator based on our prediction model is available at http://transplantmodels.com/covidvaccine/.

Conclusions.
Our machine learning model helps understand which transplant patients need closer follow-up and additional doses of vaccine to achieve protective immunity. The online calculator based on this model can be incorporated into transplant providers’ practice to facilitate patient-centric, precision risk stratification and inform vaccination strategies among SOTRs.","Using a nationwide observational study of 1031 COVID-19–naive SOTRs, we created and externally validated a sophisticated ML model to predict post-vaccine antibody response and the strength of that response. Among the 19 predictors investigated, MMF use, a shorter time since transplant, and older age were key risk factors that collectively contributed to 76% of the model’s ability to predict failure to generate a positive antibody response. Our model showed a moderate prediction performance, with an AUROC of 0.79 in our primary cohort and 0.67 in an external validation cohort of 512 SOTRs from HM. An online risk calculator based on our model is publicly available at http://www.transplantmodels.com/covidvaccine/.

The key clinical factors identified by the ML model are congruent with findings from other studies. For example, the ML model identified a nonlinear association between the odds of a positive antibody response and time since transplant, with an initial rapid increase in the odds that gradually attenuated (Figure ​(Figure1A).1A). In addition, the ML model suggested that the impact of time since transplant varied with the use of MMF, especially after 5 y since transplant (Figure ​(Figure2B).2B). These associations have been previously reported (MMF use, age, time since transplant) to correlate with antibody response rates, but our ML model provides a more nuanced description of these associations, adding clinical relevance with the easy-to-use online clinical calculator.2,16-20

In addition to identifying key clinical factors associated with antibody responses, the ML model allows us to precisely predict the post-vaccine antibody response, identifying factors associated with the different categories of antibody response (negative, low-positive, and high-positive). An example of this was a longer time since transplant, which was associated with higher odds of a high-positive response, and has been previously described but until now not quantified using a large, diverse SOTR population.20 Age had an inverse relationship with antibody response; older participants had higher odds of a low-positive antibody response. This suggests that the attenuation of the association between age and positive antibody response among older recipients, shown in our binary analysis (Figure ​(Figure2B),2B), might be a result of increasing low-positivity and decreasing high-positivity in this age group. The association between younger age and positive antibody response was more pronounced in patients under 65 y of age, corroborating previous preliminary findings of an association between older age and shorter time since transplant with seronegativity.2,20 Interestingly, male sex was found to have a small association (<3% in variable importance) with a positive seroresponse in this study. Although sex differences in vaccine-induced humoral responses have been described with other vaccines, there is a paucity of evidence to suggest any sex-based differences in COVID-19 vaccine immunogenicity.21 Reporting of sex/gendered disaggregated data is an underdeveloped area of research within the scope of COVID-19 vaccine safety and efficacy. Further exploration of sex/gender differences in vaccine efficacy may be helpful in further clarifying mechanisms for vaccine responses, but this should not be specific to transplant patients.

ML has been used to predict vaccine immunogenicity and reactogenicity against other pathogens, but has not to our knowledge been applied to understand SARS-CoV-2 vaccine responses in SOTRs.7 ML is particularly useful within the scope of vaccine development in instances where the pathogen displays genetic diversity, as SARS-CoV-2 has. Within the context of the SARS-CoV-2 pandemic, ML has the potential to augment predictions and our understanding of antibody response to current or future variant-specific vaccine formulations, patient-level risk factors (eg, medications and comorbidities), and identify high-risk individuals who may need more frequent, higher, or lower booster dosages to elicit an antibody response. Given that ML proved a useful tool to predict antibody response to 2 mRNA vaccines, further exploration into applying this method to predict third- and fourth-dose responses, including heterologous dosing strategies, is warranted.

Despite the strengths of our study, with internal prediction performance and external validity, there remain inherent limitations to consider. First, because of the observational design of our study, the associations and interactions characterized in our analyses may not necessarily represent causality between the predictors and the outcome. Second, the predictors included in this analysis were ascertained via self-report. However, we assess that the risk of information bias is relatively low, because this analysis only included basic clinical factors that were likely well-understood by the study participants. It is also important to remember that variable importance to the model does not equate to significance of certain clinical characteristics to the outcome. In addition, the C-statistic for our models was 0.79 in our cohort and 0.67 for the validation cohorts, indicating that they equally, if not out-performed, most conventional regression models that we use to predict other outcomes in transplantation.22 Furthermore, these models analyze only responses to 2 mRNA vaccines in use in the United States (BNT162b2 and mRNA-1273) and may not directly translate to third doses, or those receiving non-mRNA vaccines, or heterologous vaccine combinations (though these were not recommended and should be limited for the first and second vaccine doses). Therefore, given the numerous additional vaccine platforms available internationally, the global applicability of our model is limited. Although all SOTRs are now recommended to undergo supplemental vaccination, these recommendations are blanket policies largely reflective of a lack of large-scale supportive data. Current US policy now recommends all SOTRs obtain 3 primary vaccine doses plus a first booster (fourth dose) with provisions for a second booster (fifth dose).23 There is some encouraging evidence to support heterologous vaccination as a strategy to improve immune response rates among poor responders to the initial series, but further exploration of this strategy is needed.24

With this novel approach to predicting antibody response to mRNA SARS-CoV-2 vaccination in SOTRs, MMF use, older age, and a shorter time since transplant were the strongest factors associated with failure to generate an antibody response. In an era where universal antibody testing among transplant recipients has not yet been adopted, this model provides guidance for risk-based antibody testing. Given that a threshold for protective immunity has not yet been established, patients who exhibited a positive response may remain at risk for breakthrough SARS-CoV-2 infections. Patients and providers must remember that having an antibody response measuring “high” on a given immunoassay should not be equated with having strong protection against COVID-19 infection. Indeed, vaccination of SOTRs results in poorer protection against SARS-CoV-2 infection and mortality compared with the general population.25 The antibody titers above which transplant patients would need to be protected from death due to SARS-CoV-2 still need to be identified. Further investigation into sequential dosing, immunosuppression modulation, and breakthrough infections in vaccinated SOTRs can guide vaccination policies in this at-risk population. This model both presents a mechanistic framework for evaluating, understanding, and better predicting future mRNA vaccine-induced immune responses in transplant patients and brings this model to the bedside via our online calculator, providing an opportunity for more precise vaccination strategies in this population.",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9521390/
Arai et al. 2019,Using a machine learning algorithm to predict acute graft-versus-host disease following allogeneic transplantation,"Acute graft-versus-host disease (aGVHD) is 1 of the critical complications that often occurs following allogeneic hematopoietic stem cell transplantation (HSCT). Thus far, various types of prediction scores have been created using statistical calculations. The primary objective of this study was to establish and validate the machine learning–dependent index for predicting aGVHD. This was a retrospective cohort study that involved analyzing databases of adult HSCT patients in Japan. The alternating decision tree (ADTree) machine learning algorithm was applied to develop models using the training cohort (70%). The ADTree algorithm was confirmed using the hazard model on data from the validation cohort (30%). Data from 26 695 HSCT patients transplanted from allogeneic donors between 1992 and 2016 were included in this study. The cumulative incidence of aGVHD was 42.8%. Of >40 variables considered, 15 were adapted into a model for aGVHD prediction. The model was tested in the validation cohort, and the incidence of aGVHD was clearly stratified according to the categorized ADTree scores; the cumulative incidence of aGVHD was 29.0% for low risk and 58.7% for high risk (hazard ratio, 2.57). Predicting scores for aGVHD also demonstrated the link between the risk of development aGVHD and overall survival after HSCT. The machine learning algorithms produced clinically reasonable and robust risk stratification scores. The relatively high reproducibility and low impacts from the interactions among the variables indicate that the ADTree algorithm, along with the other data-mining approaches, may provide tools for establishing risk score.","This machine learning–guided retrospective cohort study investigating risk prediction scores for aGVHD revealed 2 major findings: ADTree provided the most accurate predictive model for aGVHD among various machine learning algorithms, and the established ADTree model clearly distinguished among 5 subgroups, based on the incidence of aGVHD, which was closely related to OS rates and the incidence of TRM. This study marked the first time that machine learning has been used to predict the risk of aGVHD.

No standard procedures have been established for evaluating and selecting which machine learning models are appropriate for use in registry database analyses.8 A report from the European Society for Blood and Marrow Transplantation (EBMT) showed that the ADTree model was selected without comparing its power of prediction with other machine learning models.8 Another study compared various machine learning models, such as ADTree, LR, MLP, NB, and RF. That study concluded that ADTree was the second best model (after LR) in terms of predicting early TRM.9 We used ADTree to create our prediction model because it showed the highest AUC for GVHD prediction. As many as 15 variables were included in the model to create the most precise prediction; the lower number of variables was related to the inferior accuracy, although the study from EBMT successfully established the overall mortality model using 10 variables.8 The prediction of GVHD might be more difficult than that of total mortality, which required the higher number of variables. Furthermore, the visually comprehensive structure of ADTree makes it an optimal model for use in the medical field compared with other models like MLP, which is composed of the “black box” process and does not a provide clear calculation process.9

In addition to demonstrating the strongest power of prediction and an easy-to-understand structure, an important advantage of the ADTree algorithm is its ability to detect interactions between variables. For instance, our ADTree-based model discovered an interaction between sex mismatch and HLA mismatch. HSCT involving female patients with male donors was known not to be associated with poorer prognosis,27 but the present study revealed that this effect of higher aGVHD risk in male-to-female HSCT was apparent only in the setting of HLA-mismatched HSCT. In the previous study from Europe, Gahrton et al28 reported that female-to-female HSCT had a significantly lower nonrelapse mortality and better OS compared with male-to-female HSCT. Their report did not show significant differences in the incidence of GVHD, and our study is the first to report this point. Unfortunately, already-established biological data cannot interpret this phenomenon; therefore, we expect that our findings can generate new biological studies of sex-related minor histocompatibility antigens (eg, to explain this phenomenon). As another example, HLA disparity, which is a major contributing factor for aGVHD, had the largest effect among aplastic anemia patients. These data demonstrate the importance of deliberately selecting donors for aplastic anemia patients, even if that increases the time spent on donor coordination. The ADTree-based model revealed that the period between disease onset and HSCT was irrelevant to predicting aGVHD risk among these patients. The ADTree model is adept at identifying specific characteristics of unique subpopulations, whereas these characteristics are usually neglected in conventional statistical analyses.29 Compared with the conventional model (eg, LR), ADTree did not find “brand new risk factors” for GVHD; however, a more accurate (higher AUC) and interaction-familiar prediction model was provided.

To take full advantage of machine learning models, it is important to know how to manipulate raw data generated from this technique. In this study, we stratified aGVHD risk scores (continuous variables) into 5 subgroups (categorical variables) to evaluate the expected hazards among each group. Because it is difficult to use raw prediction scores to evaluate aGVHD risk in the clinical field, the expected hazards in each stratified group can give the attending physicians more comprehensive clues for determining the risk of aGVHD during selection of donor graft selections, determining aGVHD prophylaxis regimens, and, moreover, identifying rapid diagnostic or therapeutic interventions in higher-risk subgroups. Translating the raw scores into subcategories might lead to the loss of important clinical information,8 but we suspect that categorization may be necessary to allow this model to be used in making clinical decisions. Division into 5 subgroups was adopted in this study to allow for the most accurate analysis using a database that included a large number of patients; division into 3 subgroups may identify the differentiated risk groups more clearly; however, this “rough prediction” provides only a limited prediction capacity and can be less informative in the clinical field.

The present study revealed the utility of machine learning as a prognostic tool for aGVHD after allogeneic HSCT. Using 10-fold internal cross-validation guaranteed the robustness of the established predictive model,9 and using machine learning prevented any bias from researchers in terms of variable selection and statistical calculations. However, there are some limitations to this study that must be addressed. For instance, we treated HLA disparity between donors and recipients as binary data (ie, mismatched or matched); however, the degree of HLA disparity may not be equivalent between each combination of raw HLA data. For example, the difference between HLA-A02:01 and HLA-A02:02 may not always be the same as that between HLA-A02:01 and HLA-A11:01, although there is very little biological data to support this view.30 Including all data for HLA antigen or allele combination in the model requires a larger number of patients/donors and will significantly increase the burden of calculation. Once this limitation is overcome in the future, this model will provide more information on how the degree of HLA disparity affects the risk of aGVHD. Nevertheless, our analysis is valid as long as each combination of HLA mismatch is reasonably categorized.

Another limitation is that our outcome measure, the incidence of aGVHD, was also treated as a binary variable in all of the machine learning models that we used in this study. Information on the onset time for cases of aGVHD was not included in the process of model building, even though onset data were included in multivariate statistical models during validation analyses. To our knowledge, no machine learning model has succeeded in handling the time-dependent outcomes. In this study, the onset of aGVHD is limited to a small window (usually 30-100 days after HSCT); therefore, the effect of dismissing information regarding the time of onset is suboptimal.

Overfitting is the conventionally discussed limitation in machine learning,31 and our algorithm is not completely free of this limitation, even though we limited the number of variables to 15 in our model. Therefore, international validations (using EBMT or Center for International Blood and Marrow Transplant Research cohorts in the current system) should be performed in the future. Comparing our model with the EBMT or Center for International Blood and Marrow Transplant Research model, which can be established with the same method using their cohort, can be the strongest validation and provide scientifically essential discussion.

In conclusion, we demonstrated the feasibility of using machine learning algorithms to predict aGVHD. ADTree established a clinically reasonable and accurate predictive model using a nationwide transplant database in Japan. This study will be helpful in establishing various other prognostic predictive models in the field of HSCT, as well as using machine learning to analyze other big data sets, such as those containing biological or genetic information. Greater accuracy in prediction (ie, higher AUC values) will likely be achieved if big data, such as HLA, killer immunoglobulin-like receptor, and single nucleotide polymorphisms, can be properly integrated into future models. Distributing these established models to clinicians via Web tools or mobile applications will provide strong instruments to determine the most appropriate therapeutic strategies in the clinical practice.",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6880900/
Nosoudi et al. 2022,The predictive value of serum bilirubin in the post-liver transplant de novo malignancy: A data mining approach,"Post-transplantation de novo malignancy in immunosuppressed organ recipients has become a major source of death, making early cancer surveillance through diagnosis and detection important in drastically improving survival rates. The focus of this work is on predicting de novo malignancy after liver transplantation using machine learning. Patients were chosen as those having developed malignancy after liver transplantation, with no history of cancer prior to transplantation, with donors being cancer-free as well. We analyzed a large volume of patients with post-transplant malignancy from the US Organ Procurement and Transplantation Network (OPTN). Several popular data mining methods were employed to characterize de novo malignancy after liver transplantation. Recipient’s bilirubin, creatinine, weight, gender, number of days in wait on the transplant list, Epstein Barr Virus (EBV), International normalized ratio (INR), and ascites were found to be among the most important factors affecting post-liver-transplantation de novo malignancy occurrence.","In 1987 when Stocker, et al. discovered that bilirubin had antioxidant properties, we no longer consider bilirubin as a waste product. It is important to note that even very small changes in serum bilirubin concentrations have been associated with developing various diseases [51].

The effect of bilirubin in post-transplant malignancy has previously been studied, however, its role as a predictor of malignancy is unknown. Serum bilirubin levels have been shown to be inversely associated with graft loss and graft failure, independent of transplant function [52]. An increase in bilirubin post-transplant in a stable patient can be the earliest sign of rejection [53].

In this research, we found that those with lower-than-normal bilirubin at the time of transplant had a higher chance of de novo malignancy post-transplant. Slightly increased plasma bilirubin seems to have a protective effect in various human pathologies, whereas, a decrease is associated with metabolic diseases [54]. As previously discussed, at normal serum concentrations, bilirubin can scavenge singlet oxygen molecules and act as an effective antioxidant. On the other hand, bilirubin is an endogenous ligand that binds directly to the peroxisome proliferator-activated receptor alpha (PPAR
) [55], including PPAR
 and PPAR
, which play a central role in regulation of atherosclerosis, diabetes, obesity, and cancer. There is increasing evidence supporting PPAR
’s role in inhibiting the progression of cancerous cells [56]. Therefore, it is reasonable to consider that lower bilirubin concentration can cause lower activity of PPAR
 and, consequently, higher cancer probability. Bilirubin levels may decrease with declining kidney function, liver disease, obesity, and smoking. Although none of these problems in post-transplantation patients can be resolved, administering exogenous bilirubin might be helpful to those with less than normal bilirubin levels. In in vivo studies using mice, using exogenous bilirubin, showed that administration of bilirubin increased renal transplant acceptance [57], [58]. In another in vivo study using a rat coronary ischemia/reperfusion model, intraperitoneal bilirubin administration minimized the infarct area [57].

We also found that being male has a higher risk of malignancy post-transplant. Males have a higher incidence of cancer with the exception of thyroid, gallbladder, and anal cancer; however, our data focused on de novo malignancy after transplantation [47], [59].

Previous immunosuppression guidelines given to liver transplant recipients proposed routine evaluating rules and immunosuppression conventions fundamental for high-risk gatherings based on the predictions by the machine learning algorithms at the population level [60]. These algorithms helped make recommendations by analyzing longitudinal pre and post follow-up transplant data to obtain major outcomes of the transplant (i.e., malignancy, cancer, infection, or graft failure). By following these protocols, care providers could help identify individuals who require specialized clinical care. Similar guidelines can be suggested for routine screening of bilirubin after liver transplantation.

This study is not without limitations. First, this study based on machine learning only relies upon pre-transplant data while post-transplant data, such as immunosuppression and tobacco use, weighs heavily on de novo malignancy. Future research can focus on predicting de novo malignancy using both pre- and post-transplant data. Second, the nature of de novo malignancies is not considered in the current study. Future studies can apply the models presented here to examine different types of de novo malignancies and translate those insights into more practical recommendations. Third, future research could apply additional predictive models and feature selection techniques to improve de novo malignancy detection. For instance, deep learning techniques such as Convolutional Neural Networks (CNNs) or Recurrent Neural Networks (RNNs) can be tested and compared with the models presented in the current study. Finally, future research could focus on determining the efficacy, and cost-effectiveness of potential preventative care for organ transplant recipients who are at risk of malignancy.
",https://www.sciencedirect.com/science/article/pii/S2772442522000405?via%3Dihub
Nitski et al. 2021,Long-term mortality risk stratification of liver transplant recipients: real-time application of deep learning algorithms on longitudinal data,"Background
Survival of liver transplant recipients beyond 1 year since transplantation is compromised by an increased risk of cancer, cardiovascular events, infection, and graft failure. Few clinical tools are available to identify patients at risk of these complications, which would flag them for screening tests and potentially life-saving interventions. In this retrospective analysis, we aimed to assess the ability of deep learning algorithms of longitudinal data from two prospective cohorts to predict complications resulting in death after liver transplantation over multiple timeframes, compared with logistic regression models.

Methods
In this machine learning analysis, model development was done on a set of 42 146 liver transplant recipients (mean age 48·6 years [SD 17·3]; 17 196 [40·8%] women) from the Scientific Registry of Transplant Recipients (SRTR) in the USA. Transferability of the model was further evaluated by fine-tuning on a dataset from the University Health Network (UHN) in Canada (n=3269; mean age 52·5 years [11·1]; 1079 [33·0%] women). The primary outcome was cause of death, as recorded in the databases, due to cardiovascular causes, infection, graft failure, or cancer, within 1 year and 5 years of each follow-up examination after transplantation. We compared the performance of four deep learning models against logistic regression, assessing performance using the area under the receiver operating characteristic curve (AUROC).

Findings
In both datasets, deep learning models outperformed logistic regression, with the Transformer model achieving the highest AUROCs in both datasets (p<0·0001). The AUROC for the Transformer model across all outcomes in the SRTR dataset was 0·804 (99% CI 0·795–0·854) for 1-year predictions and 0·733 (0·729–0·769) for 5-year predictions. In the UHN dataset, the AUROC for the top-performing deep learning model was 0·807 (0·795–0·842) for 1-year predictions and 0·722 (0·705–0·764) for 5-year predictions. AUROCs ranged from 0·695 (0·680–0·713) for prediction of death from infection within 5 years to 0·859 (0·847–0·871) for prediction of death by graft failure within 1 year.

Interpretation
Deep learning algorithms can incorporate longitudinal information to continuously predict long-term outcomes after liver transplantation, outperforming logistic regression models. Physicians could use these algorithms at routine follow-up visits to identify liver transplant recipients at risk for adverse outcomes and prevent these complications by modifying management based on ranked features.","Our deep learning algorithm to predict mortality associated with the most common post-transplantation complications is, to our knowledge, the first of its kind in transplant medicine. We provide evidence that it is feasible to continuously predict outcomes in transplant recipients using longitudinal follow-up data via deep learning algorithms. Our study is unique in three aspects: the capability of our model to capture longitudinal relationships in a continuous manner, the capability to predict multiple outcomes, and a long-term outcome prediction that exceeds most previous work.13, 14, 15 Therefore, transplant recipients can be provided with 1-year and 5-year outlooks for the four major causes of mortality at any time point after transplantation. Our findings suggest that machine learning can potentially improve post-transplant care, where there are very few guidelines to support clinical decisions due to the paucity of definitive research evidence.5

For cardiovascular mortality, we found serum creatinine to be among the most impactful variables, which was consistent with previous literature showing the relationship of serum creatinine at 12 months after liver transplantation with the risk of long-term cardiovascular events and mortality.1, 16, 17

Sirolimus and tacrolimus trough levels in the UHN dataset were important variables for cancer-related mortality, which was consistent with previous studies. Calcineurin inhibitors are notorious for their association with hepatocellular carcinoma recurrence and de-novo cancers after liver transplantation in a dose-dependent manner, while sirolimus has been shown to be protective against cancer appearance and its related mortality.18, 19, 20

For graft-related mortality, we predicted those who died due to graft failure as one of the outcomes. Patients who received a second transplant were saved by this intervention, and hence were not included in this analysis, which represented a small number of individuals. HCV as an indication for liver transplantation, donor age,21 rejection after transplantation, and post-transplantation diabetes, hypertension, and renal insufficiency have been shown to be among predictive variables for graft-related mortality.1, 22

For infection-related mortality, serum creatinine was identified as an important predictor variable, probably due to sepsis-related acute kidney injury as has been previously documented.23 The number of years since transplantation was among the top ten predictive variables for infection-related mortality in our study, which could be explained by the diminished risk of infection beyond 6 months from transplantation, since immunosuppressive therapy is usually tapered in recipients with a healthy allograft.24

Low levels of literacy among liver transplant recipients has previously been associated with treatment non-adherence based on lower serum tacrolimus levels.25 Also, recipients with low socioeconomic status are less likely to survive for 2 years after liver transplantation.26 Our study findings also indicate that recipient education level along with the primary payment method (private or public insurance, specifically in the USA) are among the main predictive variables for post-liver transplantation mortality. This could reflect the effect of socioeconomic status on post-liver transplantation outcomes, supporting the need to particularly support patients with low socioeconomic status in improving post-liver transplantation outcomes.

The use of deep learning algorithms allowed us to develop and validate highly effective 1-year and 5-year outcome predictions in transplant recipients at any given point in time after transplantation. Outcome prediction post-liver transplantation was feasible due to the highly non-linear framework of machine learning algorithms, which can detect relationships between features that are not detectable by standard biostatistics. The accuracy of machine learning algorithms in predicting solid organ transplant outcomes has been evaluated in recent years.13, 27 In liver transplantation specifically, short-term graft and patient survival after transplantation have been accurately predicted using these models.14 An artificial neural network using 15 top-ranked features from donors and recipients yielded a better AUROC of 0·83 in predicting graft failure as compared with a random forest classifier (0·81), Donor Risk Index (0·68), and Survival Outcome Following Liver Transplant score (0·63).14 However, these studies considered only pre-transplantation variables to predict post-transplantation outcomes. By contrast, we developed continuously updating predictive deep learning models based on longitudinal data (from both pre-transplantation and post-transplantation periods), and were also able to probe the most salient input variables for each major cause of mortality after liver transplantation, thereby informing measures to prevent these complications. However, the slight variation in salient features between the UHN and SRTR datasets could have been due to lack of availability of a matching variable between the datasets. Within a dataset, slight variation of salient features could pertain to differences in each cause of mortality. Furthermore, certain inputs might be highly salient to a given outcome not because their presence correlates with it, but because their presence or absence can help to narrow down an uncertain classification. Deep learning allows more complex decision boundaries to be learned than traditional statistical methods by training complex multivariate functions to fit data. These multivariate relationships are capable of modelling underlying mechanisms that simpler methods cannot approximate well.28 Furthermore, time-series deep learning models are capable of learning longitudinal relationships, including which type of changes over time have predictive power.11, 29 The comparison of multilayer perceptron to logistic regression shows the increased efficacy of deep learning, while comparing newer time-series deep learning models to multilayer perceptron shows improved performance by considering longitudinal data. Moreover, our models were consistent when validated on external data, although using the UHN database to fine-tune the model primarily trained by SRTR database provided only a slight increase in the AUROC, mainly due to the small number of common variables between the two databases.

Limitations common to registry studies, such as representative samples, missing data, and quality of the data, apply to this study. Machine learning prognostic abilities can be limited by the quality of the dataset. With any administrative dataset, as with the SRTR, the phenotyping of complex patients could be blunted, reducing the power of statistical models. Accordingly, one of the most noticeable limitations was regarding coding of the primary cause of mortality. Usually, the mechanisms that result in mortality are interwoven with each other and it can be difficult to determine the main cause of death for a patient. For example, patients with recurrent cirrhosis were also at higher risk of infection-related mortality. Nonetheless, we were able to validate our algorithms with the more granular institutional data in the UHN database that permitted more accurate cause-of-death classifications, thus strengthening our conclusions.

A limitation of all machine learning algorithms is an inability to find causal relationships. This difficult task has always required human assessment. Additionally, it is not possible to determine how the direction of change of a specific parameter (eg, increasing or decreasing the tacrolimus trough level) will affect the outcome. Nonetheless, our calculator prediction could be rerun—for example, with a modified tacrolimus level entered—to see whether the risk of an outcome is diminished or increased.

We acknowledge that a direct test on a separate dataset will validate the generalisability better. However, this is practically difficult, mainly because different datasets usually have very different variables to document, which makes a direct inference on a trained model impractical. This is also reflected in the fact that the SRTR and UHN data have a very small percentage of overlapping variables (29 [15%] of 190). Despite lacking sufficient overlaps between these two datasets to be able to apply the model to the UHN dataset without training, we were able to provide a reasonable model design for clinicians and researchers to fine-tune their model given their own data. Notably, race and ethnicity data, which were available in the SRTR dataset, were not available in the UHN dataset. Moreover, variables specific to a dataset's population, such as the method of payment in SRTR, which is meaningless in the publicly funded health-care system in Canada, do not transfer between datasets. These variables can serve as proxies for relevant indicators such as socioeconomic status; despite not being transferable, either owing to cultural or database differences, they are useful for improving performance within a dataset.

Due to some causes of death being omitted, when testing the algorithms in a real-world setting, patients might die from causes other than the four causes predicted by our method. Therefore, these other causes of death are considered as a competing risk for our four major causes of mortality and were not included in our model. However, we believe that the risk represented by the probability of mortality still represents presence of valid indicators of risk, despite the model being biased to the four predominant causes of death.

Although the model performed well in cohorts both before and after the development of HCV treatment, we used databases from an era with high incidence of HCV-related liver transplantation, and the model might not be able to predict 5-year survival outcomes for patients receiving transplants now or in the future with high level of accuracy. Therefore, our models will require further fine-tuning prospectively to adapt to the new era of liver transplantation where NAFLD predominates30 and where long-term complications tend to be cardiometabolic or malignant.31 Replacement of ciclosporin with tacrolimus as the main immunosuppression and reducing the target trough immunosuppression levels are two other main clinical practice changes during the past two decades.5

Consensus guidelines on immunosuppression in liver transplant recipients have recommended general screening guidelines and immunosuppression protocols appropriate for different high-risk groups of recipients, based on predictors detected using linear logistic regression analysis at a population level. Our machine learning models can personalise these recommendations by analysing longitudinal follow-up data in a non-linear manner to extract features predicting each transplant recipient's major outcomes (mortality caused by cardiovascular events, cancer, infection, or graft failure). By doing so, clinicians could identify subgroups of recipients who need specialised clinical care (for example, modifying the immunosuppressant regimen and doses to increase the optimal trough level in patients more at risk of graft rejection or to decrease it in patients more at risk of cardiometabolic, infectious, and malignant complications). We are also working on more transfer learning arrangements and a web-based risk calculator using the best-performing models from this study to predict different complications after transplantation and propose management recommendations to improve long-term health. Future studies will evaluate the effects and cost-effectiveness of preventive care in the transplant clinic guided by these algorithms, with the ultimate goal of helping transplant recipients to live better for longer.",https://www.sciencedirect.com/science/article/pii/S2589750021000406
Andres et al. 2018,A novel learning algorithm to predict individual survival after liver transplantation for primary sclerosing cholangitis,"Deciding who should receive a liver transplant (LT) depends on both urgency and utility. Most survival scores are validated through discriminative tests, which compare predicted outcomes between patients. Assessing post-transplant survival utility is not discriminate, but should be “calibrated” to be effective. There are currently no such calibrated models. We developed and validated a novel calibrated model to predict individual survival after LT for Primary Sclerosing Cholangitis (PSC). We applied a software tool, PSSP, to adult patients in the Scientific Registry of Transplant Recipients (n = 2769) who received a LT for PSC between 2002 and 2013; this produced a model for predicting individual survival distributions for novel patients. We also developed an appropriate evaluation measure, D-calibration, to validate this model. The learned PSSP model showed an excellent D-calibration (p = 1.0), and passed the single-time calibration test (Hosmer-Lemeshow p-value of over 0.05) at 0.25, 1, 5 and 10 years. In contrast, the model based on traditional Cox regression showed worse calibration on long-term survival and failed at 10 years (Hosmer-Lemeshow p value = 0.027). The calculator and visualizer are available at: http://pssp.srv.ualberta.ca/calculator/liver_transplant_2002. In conclusion we present a new tool that accurately estimates individual post liver transplantation survival.","This paper describes three main results: 1. A novel tool for survival prediction, PSSP; 2. A novel (appropriate) method to evaluate individual survival curves, D-calibration; and 3. A demonstration that this PSSP tool works effectively on the task of predicting post-LT survival time for PSC patients.

Below we discuss the variables selected and compare our PSSP model to the more common risk models (such as Cox proportional hazard).

Variables selected
We used the standard Cox filtering approach to select variables. This pre-processing step led to a model that used only four variables, and as expected, focused on variables already used in other LT analyses. High recipient age is associated with worse survival in PSC and is included in the Mayo, SOFT and BAR scores [7, 8, 19]. Other measures of severity, such as the need for hospitalization on a medical ward or intensive care unit (ICU) modify the risk of early post-transplant mortality, independent of MELD score [3, 20]. Low serum albumin level is associated with worse post-LT outcome [21] and reflects the nutritional status, which is an additional factor of post-LT morbidity [22]. Diabetes is an independent predictor of poor post-LT survival [23].

The MELD score, as well as its 3 component variables, were available to the learning algorithm, but none were found to be statistically significant. This result is not surprising: while MELD has proven effective in predicting urgency [24], it is poor in predicting post-LT survival [25, 26].

PSSP vs “Risk score”
Many tools, including the simple Cox proportional hazard model, provide a risk score for each individual patient–a single value with the intent that patients with larger risks should die earlier than patients with smaller risks. These scores are useful for discriminative tasks, such as the Prioritization in Fig 1. They are not useful for our Screening task, as we need to know the actual P (t | x) value, for various times t, for a patient x–e.g., to decide whether P (5 years | x) > 0·.75. The risk scores, themselves, do not provide this P (t | x) information. (See Paragraph A in S1 File).

Additionally, most risk scores do not depend on time: If a model predicts that xa is less likely than xb to survive for 3 months, then it must also predict that xa is less likely to survive for 5 years. PSSP, in contrast, can allow the hospitalization status to influence the chance of dying in the months immediately after LT, but be irrelevant at 5 years; it can also allow diabetes to have minimal peritransplant influence, but have a major impact on 5 year survival. This is essential in LT, where factors can influence the post-operative survival at different times. Note that Cox-KP inherits Cox’s “constant variable importance”.

While c-statistics (Table A in S1 File) reveals that both Cox and PSSP have mediocre discriminative capacity, recall that discrimination is not the goal for our screening test, but is instead calibrated survival prediction.

We anticipate this general PSSP approach will be applicable widely in the transplantation field and beyond–to any situation when we need to predict a patient’s future chance of survival. For example, LT for hepatocellular carcinoma should be performed only for those patients who have a high chance of survival. Many use the Milan Criteria [27] to identify patients whose survival is above 70% at 5 years post-LT. However, it only uses certain cancer factors; while they anticipate these factor will be dominant, they are not the only predictors of post-LT survival. It would be better to use PSSP here, to learn from these, and other patient variables, a model that accurately estimates survival probabilities, at this 5 year time, and also at other times.

This study focused on the LT Screening task–determining which PSC patients should be added to the wait-list, based on the utility of LT for each specific patient. We first note that this depends on the estimated survival probability, for each candidate patient, at several times. As this is a calibration task, not a discrimination task, the standard c-index measure is not appropriate for evaluation. This motivated us to design a measure that embodies “utility”, for evaluating such survival curves–“D-calibration”. We then introduce a new tool, PSSP, that can learn a general “survival model” from a survival dataset (here of PSC patients), which can then automatically produce survival curves for novel patients.

We ran this PSSP-learner on 2 tasks: with or without donor information. Our empirical results show that the individual survival distributions produced by these models are well calibrated, which means they can be used for this screening task of deciding whether a candidate should be added to the LT waitg list as they can help predict the survival of a possible recipient (or of a donor/recipient pair).

We also compared the calibration scores of our novel PSSP with the established Cox-KP system, and found that PSSP showed better calibration than Cox-KP at longer times–and in particular, that Cox-KP failed at 10 years post-LT, while PSSP was acceptable at all 4 times considered.

These results show that PSSP can accurately estimate the survival probability over time for an individual undergoing a complex intervention, based on a model learned on a survival database of prior patients with the intervention.

This technology can be applied to any medical situation where one needs to accurately estimate survival distributions for individual patients and would help us move toward evidence-based medicine on an individual level.",https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0193523#abstract0
Kawakita et al. 2020,Personalized prediction of delayed graft function for recipients of deceased donor kidney transplants with machine learning,"Machine learning (ML) has shown its potential to improve patient care over the last decade. In organ transplantation, delayed graft function (DGF) remains a major concern in deceased donor kidney transplantation (DDKT). To this end, we harnessed ML to build personalized prognostic models to predict DGF. Registry data were obtained on adult DDKT recipients for model development (n = 55,044) and validation (n = 6176). Incidence rates of DGF were 25.1% and 26.3% for the development and validation sets, respectively. Twenty-six predictors were identified via recursive feature elimination with random forest. Five widely-used ML algorithms—logistic regression (LR), elastic net, random forest, artificial neural network (ANN), and extreme gradient boosting (XGB) were trained and compared with a baseline LR model fitted with previously identified risk factors. The new ML models, particularly ANN with the area under the receiver operating characteristic curve (ROC-AUC) of 0.732 and XGB with ROC-AUC of 0.735, exhibited superior performance to the baseline model (ROC-AUC = 0.705). This study demonstrates the use of ML as a viable strategy to enable personalized risk quantification for medical applications. If successfully implemented, our models may aid in both risk quantification for DGF prevention clinical trials and personalized clinical decision making.","With the growing use of ECD fueled by a donor organ shortage, DGF has become a more significant concern among the transplant community2. To this end, several groups have developed scoring systems that enable clinicians to identify patients at higher risk of developing DGF at an early stage6,11,12,13. While multivariate LR and Cox regression are considered standard methods to develop a scoring system for risk quantification, ML is another predictive modeling approach. We would like to clarify that throughout the manuscript, LR is referred to as a ML algorithm, however, the appropriate classification of LR is context-dependent and depends upon whether it is used for prediction (ML) or inferential statistics to evaluate associations between the independent variable(s) and dependent variable (non-ML). ML has recently seen a surge of interest in various industries, including the healthcare industry, owning to advances in Big Data technology and computing power15. In a recent study, the authors compared the predictive ability of LR with that of several ML algorithms for DGF and showed that support vector machine (SVM) with a linear-basis function kernel had superior performance compared to the rest of the algorithms. However, the study used data collected from a single center (n = 497) and therefore, there is a possibility of overfitting by the model, rendering its generalizability questionable19. In the current study, we developed ML models using the United Network for Organ Sharing/Organ Procurement and Transplantation Network (UNOS/OPTN) registry, a national-scale database for organ transplantation (n = 61,220) and performed comprehensive validation of the models. To our knowledge, this is the first study to develop multiple ML models for DGF prediction using a dataset of this size and features selected via RFE-RF. Moreover, we included patient subpopulations that were excluded in the previous study by Irish et al. and therefore, our models may be applicable to a larger patient population. We did not include SVM in the final model development process as our preliminary results indicated that SVM only performed marginally better on a similar, but smaller dataset27. Furthermore, we experienced extremely long model training time due to the size of our dataset and computational complexity of SVM, which is known to be ≈ O(n2), where n is the sample size28.

After training the ML algorithms, we assessed each model for three performance measures: discrimination, calibration, and clinical utility, with the latter two being less common but essential for clinical model validation29. All of the algorithms trained with the new predictors performed better or equally well in these aspects compared to the BL model, especially ANN and XGB. It is noteworthy that better model discrimination did not always indicate superior clinical utility as observed for XGB. This may be explained by the fact that the decision curve analysis as proposed by Vickers et al.30 does not consider the net benefit of those who are not treated based on the models. Consistent with our findings, ANN has previously been demonstrated to be superior to LR in predicting transplant outcomes including DGF using single-center data21,31,32. ANN with one or more hidden layers is different from LR in that the hidden layers in ANN perform data abstraction and send the output to a final classification layer. This makes the algorithm capable of “learning” non-linear relationships between the independent and dependent variables33. On the other hand, LR traditionally is an algorithm of choice for linear classification problems34. This is one plausible explanation as to why our ANN model surpassed the baseline and new LR models. Likewise, XGB is an ensemble learning method, which assembles decision trees as its building blocks to build a strong learner that is able to learn nonlinear relationships between predictors and outcome35. XGB has recently been shown to have superior predictive performance to other ML algorithms in various contexts36,37,38,39.

Another important factor is the feature selection step. Previously, selection of risk factors was done generally by assessing preselected features in generalized linear models such as multivariate LR and generalized additive models, which is another statistical method capable of modeling non-linearity40,41. Here, we utilized RFE-RF instead, which allows for extraction of relevant features from a large pool of features in order to optimize the final predictive performance. Further, RF have a non-linear decision boundary and are considered to be a non-parametric method that is relatively robust to outliers making RFE-RF a versatile technique for feature selection42,43. Therefore, the success of our ML models is presumably attributed to the minimal yet sufficient manual elimination of features from the candidate pool and the subsequent feature selection by RFE-RF, which minimizes our dependence on a priori knowledge.

The feature selection process with RFE-RF revealed a total of 26 features as predictors of DGF. The most potent predictor was recipient pretransplant dialysis for which studies have shown a significant association with elevated risk of DGF6,7,44. Interestingly, there are some factors that are not found in the baseline predictors, but were identified as strong predictors of DGF and ranked within top 10 based on the VIS. These new predictors include recipient serum creatinine, donor BUN, and kidney biopsies done at recovery. Serum creatinine is widely used as an indicator of renal function in clinical practice and serves as a biomarker to monitor the allograft status45. While elevated levels of serum creatinine are often associated with compromised renal function, patients with a higher pre-transplant serum creatinine level, which is a surrogate of larger muscle mass, tend to have better post-transplant graft and patient survival46. Similar to serum creatinine, BUN is commonly used clinically as a measure of renal function, and higher BUN concentrations are indicative of kidney dysfunction47. Procurement biopsies are performed in about 50% of deceased donor kidneys in the Unites States for DDKT to assess the quality of donor organs, and needle biopsies are thought to increase the risk of bleeding post-transplantation48,49. Irish et al. excluded machine-perfused kidneys from their study cohort as they may complicate the analysis of risk factors. However, we found that the use of kidney pump is predictive of DGF (VIS = 2.21, Rank = 19), and prior studies reported a decreased risk of DGF associated with machine-perfused kidneys50,51,52. We did not consider any feature sets larger than 30 features in our study as we realize the concept of model parsimony is one of the critical aspects of building clinically useful models53. It is also important to remember that correlation does not always indicate causation, and the feature selection method only suggests that these features are predictive of DGF with a “potential” causal relationship with the outcome.

While ML has gained increasing attention in the healthcare industry, there are concomitant bioethical concerns surrounding the use of complex ML algorithms as they tend to have poor interpretability22,24. This has led to the preferred use of algorithms with high model transparency such as decision trees and LR. However, more complex models have been shown to predict clinical outcomes with higher accuracy and are capable of handling unstructured data such as images and electric medical records more efficiently54. Thus, more research is needed to better ascertain AI’s capability and delineate where AI fits in medicine. AI has the potential to assist in the areas of diagnosis, treatment, and clinical workflow to augment the work of clinicians54,55. This synergy between human and ML in clinical settings suggests that the implementation of AI may be key to making high quality patient care more accessible to a larger population. Establishing the right balance of human intervention and AI will likely be of utmost importance to maximize AI’s potential in this field. In addition to the healthcare arena, ML models may become valuable tools for the pharmaceutical industry and clinical researchers in order to increase success rates of clinical trials56. Clinical trials for drug development consist of lengthy processes that consume substantial amounts of resources and efforts. Consequently, strategies to reduce trial failures are imperative. ML algorithms, if trained and validated properly, may be part of such strategies to aid in patient stratification, treatment response identification, and/or subgroup identification57.

One of the limitations of this study is that we were unable to include warm ischemia time and peak calculated panel reactive antibody (cPRA) in our baseline model, which could be another explanation for its lower predictive score observed in our study. Furthermore, Irish et al. included recipients transplanted in a different time period rendering apple-to-apple comparisons impossible. However, it needs to be emphasized that the primary objective of this study is not to demonstrate one approach is better than the other, but to propose ML as an alternative method to build a clinically useful tool. In fact, external validation studies of existing predictive models for DGF were conducted in Dutch58 and Chinese59 cohorts separately, and the model developed by Irish and his colleagues outperformed the other models in both studies. Our study would also benefit from external validation in non-UNOS/OPTN data and further analysis with external validation data is forthcoming. Another potential limitation of this study is that we did not perform in-depth analyses of the algorithms and selected predictors when both of the best performing models in our study (ANN and XGB) are considered black box algorithms. Therefore, the future direction is to further ensure that the predictions are sensible and that the models are explainable using techniques such as local interpretable model-agnostic explanations and Shapley additive explanations among others60. Furthermore, we will assess how model performance changes with fewer predictors in an attempt to reduce the number of predictors needed and improve model parsimony.

We have demonstrated here that ML is a valid alternative approach for prediction and identification of predictors of DGF, adding an important piece of evidence to support the use of ML to drive medical advancements. Additional effort to improve model interpretability and transparency will be essential to expedite the successful implementation and use of complex yet high-performing ML algorithms for clinical applications. If properly implemented, our prognostic systems may potentially be used to augment the workflows in clinics and drug development for DGF.",https://www.nature.com/articles/s41598-020-75473-z
Sharma et al. 2023,Point-of-care detection of fibrosis in liver transplant surgery using near-infrared spectroscopy and machine learning,"Introduction
Visual assessment and imaging of the donor liver are inaccurate in predicting fibrosis and remain surrogates for histopathology. We demonstrate that 3-s scans using a handheld near-infrared-spectroscopy (NIRS) instrument can identify and quantify fibrosis in fresh human liver samples.

Methods
We undertook NIRS scans on 107 samples from 27 patients, 88 from 23 patients with liver disease, and 19 from four organ donors.

Results
Liver disease patients had a median immature fibrosis of 40% (interquartile range [IQR] 20–60) and mature fibrosis of 30% (10%–50%) on histopathology. The organ donor livers had a median fibrosis (both mature and immature) of 10% (IQR 5%–15%). Using machine learning, this study detected presence of cirrhosis and METAVIR grade of fibrosis with a classification accuracy of 96.3% and 97.2%, precision of 96.3% and 97.0%, recall of 96.3% and 97.2%, specificity of 95.4% and 98.0% and area under receiver operator curve of 0.977 and 0.999, respectively. Using partial-least square regression machine learning, this study predicted the percentage of both immature (R2 = 0.842) and mature (R2 = 0.837) with a low margin of error (root mean square of error of 9.76% and 7.96%, respectively).

Conclusion
This study demonstrates that a point-of-care NIRS instrument can accurately detect, quantify and classify liver fibrosis using machine learning.","We use a point-of-care NIRS instrument to scan fresh pathological and normal liver tissue from explanted livers, all of which are then compared to findings of conventional histopathology, with the following key findings. First, the NIRS spectra for diseased (F4 cirrhosis) and normal (F0) are unique at a crude visual level. Second, when we combine the spectral data with machine learning algorithms, NIRS has an accuracy of 96% in identifying cirrhosis and greater than 93% in grading or classifying the degree of fibrosis. Third, we find that we find that NIRS can accurately predict both immature and mature fibrosis (R2 > 0.80) to within 10% (RMSE < 10%).

This study is the first to describe point-of-care NIRS in fresh human tissue to detect hepatic fibrosis. Current data for use of spectroscopy for analyzing fibrosis in biological tissue are emerging, but the majority use bulky benchtop instruments that are restricted to laboratory use.10-18 This is one of a small number of studies where miniaturized handheld instruments that carry point-of-care potential. However, most studies have used Raman spectroscopy, where we use NIR spectroscopy which benefits from greater penetration depth and reduced fluorescence of thermal perturbance of underlying tissue. Furthermore, the majority of previous studies were performed on animal tissue, fixed tissue or were based on less than 10 samples. In scanning 107 fresh samples with a 3-s NIRS scan with a high degree of accuracy, we believe we are the first to demonstrate NIRS as a potential point-of-care clinical instrument for hepatic fibrosis, especially in transplant surgery.

The clinical implications for such a tool are considerable. At the time of retrieval, point-of-care NIRS could provide a rapid assessment of fibrosis, potentially predicting biopsy results, and therefore risk of adverse events; findings of viability may help to increase the number of livers available for transplantation (i.e., transplanting livers that would otherwise be rejected). The accuracy point-of-care NIRS exceeds other clinical assessments, such as Fibroscan which has a reported AUC of 0.70–0.89.47 Furthermore, our findings are from a heterogenous range of aetiologies, including hepatitis, alcoholic liver disease and nonalcoholic steatohepatitis; all of which have historically been difficult to diagnose using other imaging modalities. As samples have been obtained using a fibreoptic probe with a diameter less than 10 mm, with the technique being nonperturbative and autoclave safe, it can be safely applied to whole organs without the need for tissue excision. This has the potential for use with a minimally invasive laparoscopic approach.

Our study is unique due to the large number of fresh pathological and control human liver samples studied, which are typically challenging to obtain.36. NIR instrumentation is readily available, as it is already used extensively in industrial chemistry, with multiple combinations available for spectrometers, light sources, and fiber-optic probes.37 Costs are under 20,000 USD, which are still cheaper than many of the imaging instruments used in clinical practice. Training to use instruments is becoming increasingly easier, with some instruments now available via smartphone applications with a “single-click” user interface. The spectral range from 900 to 1700 nm and 785 scans, provides ample data for machine-learning algorithms to yield high levels of accuracy. This allows us to not only diagnose fibrosis, but also quantify and typify it. With larger-scale studies with in vivo validation, this is a tool that has immediate clinical applications. Future studies would also benefit from multi-modal spectroscopic unsupervised evaluation and, that could compile a complementary array of data from Raman and Infrared (mid-IR and near-IR) spectroscopy. External validation at other centers with other instruments would also be required to validate the findings from this report.",https://onlinelibrary.wiley.com/doi/10.1002/hsr2.1652
Ershoff et al. 2020,Training and Validation of Deep Neural Networks for the Prediction of 90-Day Post-Liver Transplant Mortality Using UNOS Registry Data,"Prediction models of post-liver transplant mortality are crucial so that donor organs are not allocated to recipients with unreasonably high probabilities of mortality. Machine learning algorithms, particularly deep neural networks (DNNs), can often achieve higher predictive performance than conventional models. In this study, we trained a DNN to predict 90-day post-transplant mortality using preoperative variables and compared the performance to that of the Survival Outcomes Following Liver Transplantation (SOFT) and Balance of Risk (BAR) scores, using United Network of Organ Sharing data on adult patients who received a deceased donor liver transplant between 2005 and 2015 (n = 57,544). The DNN was trained using 202 features, and the best DNN’s architecture consisted of 5 hidden layers with 110 neurons each. The area under the receiver operating characteristics curve (AUC) of the best DNN model was 0.703 (95% CI: 0.682-0.726) as compared to 0.655 (95% CI: 0.633-0.678) and 0.688 (95% CI: 0.667-0.711) for the BAR score and SOFT score, respectively. In conclusion, despite the complexity of DNN, it did not achieve a significantly higher discriminative performance than the SOFT score. Future risk models will likely benefit from the inclusion of other data sources, including high-resolution clinical features for which DNNs are particularly apt to outperform conventional statistical methods.","The results demonstrate that a DNN can be used to predict 90-day post-liver transplant mortality using UNOS registry data. While the AUC for the best performing DNN (DNN with OFS + softbin) was the highest among the tested models, significantly outperforming the BAR score, it did not achieve significantly higher performance compared to the SOFT score. Similarly, the DNN’s maximal F1 measure, which reflects a balanced valuation of sensitivity and specificity, was not significantly different from that of the SOFT score. At the thresholds that maximized the F1 measures for the DNN with OFS + softbin and SOFT score, the DNN model had significantly higher specificity with fewer false positive (990 vs 1258). However, the SOFT score had more true positives (223 vs 185), reflecting the higher sensitivity of the SOFT score. It is important to note that by adjusting the threshold value, arbitrarily high sensitivities or specificities can be achieved for both models with a consequent decrease in the complimentary metric. While the F1 measure values sensitivity and specificity equally, the relative costs of a false positive (i.e., failing to transplant a patient who otherwise would live) versus the cost of a false negative (transplanting a patient who will die) is a decision that must be made by the transplant community. Rana et al argue that a SOFT score greater than or equal to 40 may indicate futile transplantation [9]. However, in our cohort, a threshold of 40 for the SOFT score carried a sensitivity of only 0.025 (95% CI: 0.014-0.038), raising questions about its clinical utility.

While several predictive models exist, we chose to compare the DNN to the BAR score and SOFT score as they were both derived from UNOS registry data and have the highest AUC in predicting 90-day post-transplant mortality. While both models report an AUC of 0.7, in our study the calculated AUC were slightly lower at 0.66 and 0.69 for the BAR score and SOFT score, respectively. These differences may be explained by differing exclusion criteria with the dataset used to derive the BAR score excluding split livers and donation after cardiac death donors. The SOFT score in our dataset was based on 17 of the original 18 features, as the variable indicating portal bleed within 48 hours of transplantation was not available in the UNOS dataset.

Given the scarcity of organ donors, when adverse outcomes occur, the logical question is whether the organ would have been better served by being allocated to another recipient. As such, many have questioned whether to transplant a patient based solely on need or whether to do so based on expected outcomes [2]. The concept of futile transplantation is not new, and defining futility is difficult [35]. An underlying theme, however, points to the need to estimate postoperative mortality and not solely focus on preoperative survival. Authors have suggested models that account for both waitlist mortality and the probability of post-transplant survival [36], and some have called for novel liver allocation models that achieve collective survival benefits [37]. Given the success that DNNs have had in various classification tasks, we tested the hypothesis of whether they could perform superiorly in this classification problem and therefore be an important step to ultimately achieving better allocation models.

Machine learning algorithms can model more complex interactions and nonlinearities among the input features and often achieve higher predictive performance than conventional statistical models. To date, though, few groups have explored these methods to predict post-liver transplant morbidity and mortality. Lau et al recently used a random forest to classify graft failure within 30 days following liver transplantation using a study sample of 180 recipients from institution-level data and achieved an AUC of 0.818, although performance was significantly diminished when applying the model to the validation set. [38]. While some have explored using neural networks to predict liver transplant mortality, most were based on a small number of patients at individual institutions [[39], [40], [41]]. Raji et al applied a neural network using UNOS level data to predict post-transplantation graft failure, but the authors only included a few hundred patients in the model [42].

While DNN have achieved improved performance in various classification tasks, there are several possible reasons why the DNN failed to significantly outperform a logistic regression model in this study. There are likely features that are predictive of post-transplant mortality that were not included in this risk model. Multiple cardiac risk factors, for example, have been found to be associated with adverse events including survival, and several studies have shown that cardiac morbidity is 1 of the leading causes of post-transplant mortality [43]. Single-center studies have identified cardiovascular risk [37], preoperative troponin levels [44], coronary artery disease [45], and echocardiographic measures [46,47] as predictors of survival. As these data are not included in the UNOS database, we were unable to account for this variability in the outcome. It is possible that other machine learning algorithms, either alone or in combination with a DNN, may be able to achieve superior performance given the same training data. While a DNN can, in theory, approximate any complex function that maps the predictors to the response variable, given limited training data this may not be achieved, and other machine learning algorithms may achieve better discriminative performance.

As researchers are using machine learning more frequently, an emerging theme is how these sophisticated algorithms do not always outperform conventional statistical models such as regression. In a recent study, our group applied deep learning to the prediction of postoperative mortality using institution-level data and found that it did not outperform logistic regression [28]. Similarly, machine learning algorithms failed to outperform logistic regression in the prediction of heart failure readmission [26]. Machine learning algorithms such as DNNs are more likely to excel in the analysis of complex, high granularity data that is lacking from the UNOS database. Finally, all machine learning models are limited by whether relevant features can be appropriately encoded in such a way that can be included as a variable in the model. Several tacit knowledge variables, such as the physical appearance of a patient, are difficult to quantify and therefore include in a DNN model. The future may allow such variables to be represented in models, but for the foreseeable future, the clinician will be involved in risk assessment.",https://www.sciencedirect.com/science/article/pii/S0041134519309315
Schleich et al. 2013,A neural network-based approach for predicting organ donation potential,"This research proposes a potential organ donor prediction model using an artificial neural network-based approach to forecast the amount of daily incoming organ referrals and their medical suitability. The daily amount of incoming organ referrals and their medical suitability indicate organ donation potential. Predicting organ donation potential is vital for organ procurement organizations (OPOs) to improve staffing and scheduling practices. As a result, the objective of this study is to develop an accurate organ donation potential prediction model that can help the OPOs in achieving their mission to save lives. Several supervised artificial neural networks were designed, tested and compared with each other to identify best prediction accuracy. The experimental results and analyses indicate that the prediction accuracy for organ donation potential depends not only on the network type, but also on the architecture selection and the choice of inputs and time frame. The constructed neural network model shows good organ potential prediction accuracy with a R-squared value of 0.73. Hence, implementing such a model can support organ procurement organizations in their mission to save lives.","This research studied neural networks using the backpropagation algorithm, radial basis function networks, modular neural networks, multi-linear regression models and committee networks for time series forecasting, in particular predicting daily organ donation potential. The independent variables for this study were number of daily open medical suitable referrals, weekday, number of organ donor cases receieved per day and number of all referrals received per day. It is shown that different numbers of hidden neurons as well as different prediction time frames are highly correlated with prediction accuracy based on root mean squared error, R-Squared value, mean absolute percentage error and normalized mean squared error for the NN-BP. Based on the model performance on the validation set, MLR and NN-BP both provide good prediction accuracy on daily organ donation potential. A CNN that is composed of the MLR and NN-BP provides comparable prediction accuracy. The objective of this study was to provide a decision support tool for the OPOs by predicting daily organ donation potential. The results show that the NN-BP, MLR and CNN can be used for accurately predicting organ donation potential. Based on these results, a decision support system can be implemented for staffing and scheduling processes to contribute to the OPO's mission to save lives. Future work could enhance the proposed prediction models by applying different features and/or the use of meta-heuristics for NN training. Trying to distinguish and analyze seasonality and breaking organ donation potential down to different areas and hospitals is another option among several others.",https://www.proquest.com/docview/1471958899?fromopenview=true&pq-origsite=gscholar
Misiunas et al. 2015,DEANN: A healthcare analytic methodology of data envelopment analysis and artificial neural networks for the prediction of organ recipient functional status,"The problem of effectively preprocessing a dataset containing a large number of performance metrics and an even larger number of records is crucial when utilizing an ANN. As such, this study proposes deploying DEA to preprocess the data to remove outliers and hence, preserve monotonicity as well as to reduce the size of the dataset used to train the ANN. The results of this novel data analytic approach, i.e. DEANN, proved that the accuracy of the ANN can be maintained while the size of the training dataset is significantly reduced. DEANN methodology is implemented via the problem of predicting the functional status of patients in organ transplant operations. The results yielded are very promising which validates the proposed method","The ANN provides a good baseline for the predictions of the functional status of patients, providing acceptable accuracy considering the complex relationship amongst the variables and the high volume of records. On the other hand, DEA efficiency levels effectively separate records based on correlation of inputs and outputs. Individual levels have varying levels of significant positive and negative correlation between inputs and outputs compared to the original dataset which has very low correlation. Consideration of specific level sums maintains an accuracy metric while at the same time reducing the size of the dataset being considered by the ANN which, among other things, reduces the training time of the ANN considerably. This study presents a hybrid methodology, i.e. DEANN, that integrates these two data analytic methods and collectively utilizes the abovementioned features of the two. The viability of the complementary nature of ANN and DEA is presented in this study along with a complex, large, US-based nation-wide healthcare dataset. Although the proposed DEANN method is validated here via a healthcare-based dataset due to its recent popularity in literature, the generic nature of the method renders it viable and practically applicable to other settings that deploy large datasets in a similar fashion. It would hypothetically provide more efficiency in computation of prediction and would be an effective way to deal with such voluminous datasets.

Since conventional DEA approaches work best with continuous data, this current study has utilized only continuous variables available in the UNOS database. However, much research has been done in DEA recently to consider other variable types and it is the authors׳ intention that this work will be expanded to consider both ordinal and binary data types to better classify the transplants at another study. Future research directions also include the implementation of a modified DEA which considers ordinal and binary values, large scale pruning of the ANN, and reduction of oversampled outputs to further improve training of the ANN. Nevertheless, this study itself provides a strongly acceptable baseline for which these future research goals would improve upon.",https://www.sciencedirect.com/science/article/pii/S0305048315000626
Börner et al. 2022,A Novel Deep Learning Model as a Donor–Recipient Matching Tool to Predict Survival after Liver Transplantation,"Background: The “digital era” in the field of medicine is the new “here and now”. Artificial intelligence has entered many fields of medicine and is recently emerging in the field of organ transplantation. Solid organs remain a scarce resource. Being able to predict the outcome after liver transplantation promises to solve one of the long-standing problems within organ transplantation. What is the perfect donor recipient match? Within this work we developed and validated a novel deep-learning-based donor–recipient allocation system for liver transplantation. Method: In this study we used data collected from all liver transplant patients between 2004 and 2019 at the university transplantation centre in Munich. We aimed to design a transparent and interpretable deep learning framework to predict the outcome after liver transplantation. An individually designed neural network was developed to meet the unique requirements of transplantation data. The metrics used to determine the model quality and its level of performance are accuracy, cross-entropy loss, and F1 score as well as AUC score. Results: A total of 529 transplantations with a total of 1058 matching donor and recipient observations were added into the database. The combined prediction of all outcome parameters was 95.8% accurate (cross-entropy loss of 0.042). The prediction of death within the hospital was 94.3% accurate (cross-entropy loss of 0.057). The overall F1 score was 0.899 on average, whereas the overall AUC score was 0.940. Conclusion: With the achieved results, the network serves as a reliable tool to predict survival. It adds new insight into the potential of deep learning to assist medical decisions. Especially in the field of transplantation, an AUC Score of 94% is very valuable. This neuronal network is unique as it utilizes transparent and easily interpretable data to predict the outcome after liver transplantation. Further validation must be performed prior to utilization in a clinical context.","This study represents a novel deep-learning-based prediction model for survival after liver transplantation. The model was trained on 529 transplantations including 1048 donors and recipients. Further, it aims to be interpretable and transparent, especially in its process of data utilization. It managed to perform with an AUC of 0.940 that, in a clinical context, represents a very strong prediction. We chose this method because of the nature of the dynamic interaction between the donor and recipient. It is built in a modular way, where the user can train this specialized neural network on any transplant data with little effort. Previous studies have applied similar methods to accomplish predictions of outcome. Since we did not utilize the same data and used different outcome parameters a direct comparison between different prediction models is difficult. However, Ayllon et al. achieved an AUC of 0.82 for prediction of 12-month survival after liver transplantation [20]. Ershoff et al. achieved an AUC of 0.703 [21] for the prediction of 90-day post-transplant mortality. Our model achieved comparable and higher accuracies to those shown above. Even though these models also seem to be performing reasonably well, they can only predict one timepoint, whereas the model presented here can predict multiple timepoints.
The European General Data Protection Regulation of 2018 stated reasonable concerns with black-box predictions. The concerns not only include the opaqueness of the model itself but also the necessity to have control over the data, the processing and the interpretation of the results obtained [22]. Our data selection and processing were specifically set out to meet these concerns. As missing data is omnipresent within medical archives, we developed our own imputation method, which proved to be more accurate than readily known imputation algorithms (Boerner et al. under review).
Regarding the interpretation of the resulting predictions, we propose to create an AI-assisted utility-based allocation concept. AI offers the chance of utilizing the vast amount of data in the field of transplantation to optimize organ utility. The state-of-the-art allocation target metrics such as the DRI, MELD score, SOFT score, and BAR score offer some success in predicting the most favourable outcome. However, these scores are criticized for being inaccurate, untransparent and static [23,24,25]. An AI-assisted utility-based allocation concept using gain-of-survival as the target metric would be more flexible and could represent the best approximation towards a perfect allocation practice [7,26].
This study has some limitations that are inherent to its design. First of all, this is a retrospective study with data from two German transplant centres. We have mitigated the possible biases by utilizing cross-validation to make our results more generalizable. Multiple models have been developed over the past years and have aided the process of discussion on how to incorporate machine learning and neuronal networks into our clinical decision process [27,28]. As mentioned above, we have strived for maximum transparency, however, the very nature of a black-box model could not be fundamentally changed.
This study is intended as a proof of concept. It represents a novel deep learning model that was trained and tested. Such a model could potentially be used as a part of a utility-based allocation concept. Before this model or any of its kind can be used as a bedside tool, the results need to be externally confirmed in a randomized clinical trial, ideally in a multicentre setting.",https://www.mdpi.com/2077-0383/11/21/6422
Kim et al. 2005,Appropriateness of a Donor Liver with Respect to Macrosteatosis: Application of Artificial Neural Networks to US Images—Initial Experience,"PURPOSE: To retrospectively compare performance of artificial neural networks (ANNs) applied to ultrasonographic (US) images with that of radiologists for prediction of appropriateness of a donor liver with respect to macrosteatosis before liver transplantation.

MATERIALS AND METHODS: Institutional ethics committee approved study; written informed consent was obtained. ANNs, constructed with three-layered 15-neuron back-propagation algorithm, were trained to predict appropriateness of a donor liver with respect to macrosteatosis by using statistically significant laboratory and US parameters derived from univariate analyses, together with correct diagnosis. Input variables for ANNs were alkaline phosphatase, glutamic oxaloacetic transaminase, glutamic pyruvate transaminase, γ-glutamyltransferase, hepatorenal ratio of echogenicity, and tail area ratio and tail length of portal vein wall echogenicity. Three radiologists graded US images in 94 potential donors (71 men and 23 women) on the basis of four degrees of hepatic steatosis. After training and testing of ANNs, performance of ANNs and radiologists in predicting appropriateness of potential donors was evaluated with receiver operating characteristic (ROC) analysis and compared by means of univariate z score test.

RESULTS: Among 94 potential donor livers, 76 were normal or had mild steatosis, and 18 had moderate or severe macrosteatosis at histopathologic examination. Area under ROC curve (Az) of ANNs (Az = 0.9673) was significantly greater than that of radiologists (faculty, Az = 0.9106, P = .048; fellow, Az = 0.9038, P = .044; resident, Az = 0.8931, P = .038). No statistically significant difference in sensitivity for predicting appropriateness as a liver donor with respect to macrosteatosis was found between ANNs (88.9%) and radiologists (P > .05). However, specificity of ANNs (96.1%) was significantly better than that of radiologists (P < .003).

CONCLUSION: ANNs might be a useful tool to categorize whether a donor liver is appropriate for transplantation with respect to macrosteatosis on the basis of multiple variables related to laboratory and US features. Further study is needed.","Our currently constructed ANNs show high performance in terms of predicting liver donor appropriateness with respect to macrosteatosis on the basis of its use of multiple variables related to laboratory tests and newly developed US parameters. The Az value of the developed ANNs was significantly greater than that of all radiologists, and its sensitivity was as good as that of three readers. The high performance of the ANNs developed in our study is perhaps explained by the fact that radiologists estimate the degree of hepatic steatosis subjectively and probably do not consider the entire laboratory and US features systematically; therefore, they cannot effectively organize or categorize all the data. Radiologists may also overestimate their knowledge and experience. On the other hand, ANNs consistently and comprehensively respond to all inputted data. Therefore, we are not surprised at the excellent performance shown by the ANNs for prediction of liver donor suitability with respect to macrosteatosis preoperatively. Similar results have been reported in the field of radiology and other fields, where ANNs have been used for the diagnosis and the differential diagnosis of various diseases (,21,,22,,34).

Another important result is that the specificity of ANNs was significantly better than that of radiologists. Because liver transplantation has become the standard treatment for patients with various liver diseases and its success rate has increased dramatically, the demand for liver transplantation has also increased greatly in most countries. For example, about 13 000 patients are currently waiting for a liver transplantation in the United States, while only 4000 organs are available each year (,35). In such a situation, higher specificity is crucial to enable expansion of the pool of organ donors. In this context, the results obtained during the present study, particularly in terms of the high specificity of the developed ANNs, are both meaningful and extremely encouraging.

To detect and quantify the fat content of a liver in vivo, various imaging modalities, including US, CT, and MR imaging, have been used (,12–,20,,25–,27,,36). They provide reproducible noninvasive measures of global hepatic fat content, with no risk of intrahepatic sampling error. At US, fatty droplets within hepatocytes scatter US beams to produce the phenomenon of a bright echotexture, in which the liver is more echogenic than the adjacent kidney.

Freese and Lyons (,13) postulated that US is a simple, noninvasive, and quantitative test for fatty infiltration of the liver and found a linear relationship between the total lipid concentration and US backscatter coefficient. However, the dependence of US on subjective interpretation is a major limitation in terms of assessing the degree of hepatic steatosis. The various levels of interobserver agreement (0.636–0.823) between readers observed in the present study testify to this shortcoming.

The quantitative evaluation of echogenicity on the basis of histograms has been proposed as a means of overcoming variability due to subjective evaluation (,27,,36). In these studies, however, other important factors such as laboratory data were not considered, and only the acoustic intensity of the liver (,36) or ratios and differences of liver and renal cortical echo amplitude (,27) were analyzed. In the present study, to measure the brightness of hepatic parenchyma objectively with US, we estimated the ratio of portal vein wall intensity and liver-to-kidney intensity ratios. Given that impaired visualization of the intrahepatic portal vein borders and a diffuse increase in hepatic echogenicity are important diagnostic criteria of hepatic steatosis at US (,12), we believe that the quantification of the impaired visualization of the intrahepatic portal vein border by using tail area ratio and tail length of portal vein echogenicity could add confidence to US diagnoses of hepatic steatosis, as opposed to measuring the brightness of hepatic parenchyma only. As was expected, the values of all US parameters were significantly different between the two groups of normal findings or mild macrosteatosis (appropriate group) and moderate or severe macrosteatosis (inappropriate group). Therefore, we suggested that US parameters such as portal vein wall intensity should be used as additional tools for quantifying hepatic steatosis, together with liver-to-kidney ratio.

Increased hepatic echogenicity, the so-called bright liver, is also found in patients with increased fibrous content, and it is not possible to differentiate between fatty infiltration and fibrosis by means of hepatic intensity alone (,14,,37). However, because most living liver donor candidates are healthy and usually have no other medical problems, increased hepatic echogenicity in a potential liver donor can be assumed to be caused by fatty infiltration.

One of the interesting findings in the present study is that US grades for fatty liver, as determined by radiologists, are significantly correlated only with the grade of macrovesicular fat accumulation in hepatocytes and not with microvesicular fat accumulation. This result is in line with a report by Pamilo et al (,38). They found that increased hepatic echogenicity is caused only by large fat droplets exceeding 100 μm in diameter. Thus, it is likely that microvesicular fat accumulation has little effect on increased hepatic echogenicity; however, it has not been established how microsteatosis of moderate or severe degree could influence US findings. Further prospective study of a larger population with profound microsteatosis is needed.

The ability of ANNs to learn specific patterns between input and output data strongly depends on the quality of input data. It should be noted that in the present study, we used only objective laboratory and US data as input variables and not subjective values determined by radiologists. Therefore, the quality of input data for ANNs can be constant, regardless of other subjective factors. When compared with other studies in which many subjective ratings by radiologists or other physicians were used, our methods and results can be applied more easily and objectively to computer-aided diagnosis for the characterization of diffuse liver disease at US. We appreciate that ANNs are not a cure-all for complex data analysis, and several criticisms may be encountered with these techniques. These include the empirical nature of choosing network parameters and the complexity of the inner processes, which are not explained easily. Despite such limitations, ANNs provide us with a predictive ability that can be demonstrated clearly and understood with a thorough understanding of the processes involved.

Some limitations of our study should be mentioned, which are derived primarily from fundamental problems of US studies. Although many confounding factors, including the effect of focusing and distance from the transducer, can make echogenicity difficult to quantify objectively, we tried to minimize these factors by performing US with constant settings and by applying ROIs to the liver and right kidney at the same distance from the transducer. However, problems due to the use of different US equipment remain to be overcome. In addition, biopsies of the liver are subjective to a well-recognized sampling error due to the inhomogeneous distribution of fat; moreover, the process of fat infiltration is continuous, and there may be temporal variations in fat content. The results of the 14 patients who underwent both percutaneous and surgical biopsy indicated that temporal variability was not a serious problem. However, the small number of cases and the short interval (mean, 20.3 days) between the two biopsies prevent generalization of this result.

Finally, we used the same data set for both training and testing of the ANNs. This limitation is also related to the small number of cases deemed inappropriate as donor livers. Although the validity of training and testing by using the same data set is widely accepted in medical and medical engineering fields (,21–,24,,34), it could inflate the performance of the ANNs on the test set. Accordingly, further prospective studies with a larger number of cases, allowing adequate training and independent testing, are needed to assess the performance of ANNs for prediction of the appropriateness of a donor liver. If such a study produces encouraging results, the possibility of applying this method to clinical practice should be explored.

In conclusion, ANNs might be a useful tool to categorize a donor liver as either appropriate or inappropriate for transplantation with respect to macrosteatosis on the basis of multiple variables related to laboratory and US features. Although ANNs showed better performance than that of radiologists, further comparative studies with larger cases are needed before clinical application.",https://pubs.rsna.org/doi/full/10.1148/radiol.2343040142?casa_token=9sQg6qAwJ2wAAAAA%3A-6uoiq62jQQKr840sg6c_iEZLFS-TuA_P6felZGor1Qs4l-KokOWgWP4P0J8UeuuFMm3FIMcFA
Molinari et al. 2019,Prediction of Perioperative Mortality of Cadaveric Liver Transplant Recipients During Their Evaluations,"Background. 
There are no instruments that can identify patients at an increased risk of poor outcomes after liver transplantation (LT) based only on their preoperative characteristics. The primary aim of this study was to develop such a scoring system. Secondary outcomes were to assess the discriminative performance of the predictive model for 90-day mortality, 1-year mortality, and 5-year patient survival.

Methods. 
The study population was represented by 30 458 adults who underwent LT in the United States between January 2002 and June 2013. Machine learning techniques identified recipient age, Model for End-Stage Liver Disease score, body mass index, diabetes, and dialysis before LT as the strongest predictors for 90-day postoperative mortality. A weighted scoring system (minimum of 0 to a maximum of 6 points) was subsequently developed.

Results. 
Recipients with 0, 1, 2, 3, 4, 5, and 6 points had an observed 90-day mortality of 6.0%, 8.7%, 10.4%, 11.9%, 15.7%, 16.0%, and 19.7%, respectively (P ≤ 0.001). One-year mortality was 9.8%, 13.4%, 15.8%, 17.2%, 23.0%, 25.2%, and 35.8% (P ≤ 0.001) and five-year survival was 78%, 73%, 72%, 71%, 65%, 59%, and 48%, respectively (P = 0.001). The mean 90-day mortality for the cohort was 9%. The area under the curve of the model was 0.952 for the discrimination of patients with 90-day mortality risk ≥10%.

Conclusions. 
Short- and long-term outcomes of patients undergoing cadaveric LT can be predicted using a scoring system based on recipients’ preoperative characteristics. This tool could assist clinicians and researchers in identifying patients at increased risks of postoperative death.","The most significant finding of this study is that a predictive model that can identify patients at increased risk of perioperative mortality after LT is feasible using clinical variables attainable during the early phase of their evaluations. Because LT is a life-saving procedure requiring advanced clinical and technical skills, this predicting model is not meant to be used in isolation or as a substitute for good clinical judgment. Yet, it could be a valuable instrument for clinicians, administrators, and investigators as an instrument that can provide an objective estimate of the risk of suboptimal outcomes after LT.25

The allocation of liver grafts based on MELD score26,27 prioritizes the sickest patients on the waitlist27-29 and has changed the characteristics of recipients undergoing LT in the United States and other parts of the world.30,31 Compared to patients who underwent transplantation before the MELD score was implemented, current candidates are older, with more comorbidities32 and higher acuity of liver disease.1,9,33,34 Recent studies have shown that the average perioperative mortality after LT ranges between 5% and 10%,11 but the risk is more significant in patients with high MELD scores,35 several comorbidities,9 advanced age,36 abnormal BMI,37,38 and low performance status.39

One of the common challenges for transplant specialists dealing with the current allocation system is the selection of appropriate surgical candidates. By selecting only patients at low perioperative risk, transplant programs would decline life-saving operations to many individuals who benefit from LT. On the other hand, the selection of very high-risk patients reduces the number of grafts that could be allocated to recipients with a better chance of survival. Finding the balance between these 2 scenarios can be difficult without an objective instrument to stratify patients during the early phases of their evaluations.25

For our model, the C statistics of perioperative mortality risk ≥10%, ≥15%, and ≥20% were 0.95, 0.93, and 0.86, respectively, and for patients with 3 or more points, sensitivity and specificity were 91% and 82%, respectively. These findings are relevant because patients with irreversible liver diseases can only be cured by LT unless unsuitable for surgery. Consequently, the most critical decision to be made is whether or not LT should be performed based on the probability that the patient would survive the operation or not. In many circumstances, this decision is rather straightforward, but for marginal recipients it can be difficult, and despite the best clinical acumen, it can be biased and inconsistent over time. Consequently, a scoring system like the one we are proposing could assist healthcare providers in making more objective decisions during patient selection or in allocating appropriate resources to patients at high perioperative risk.

Several other investigators2,4,5,9,12,13,15,16,35,40-43 have proposed predictive models to identify LT candidates at increased risk of postoperative death. These existing scoring systems include characteristics that are pertinent to recipients, donors, and quality of the graft and require operative variables that become available only once surgery is completed.5 Therefore, most transplant centers do not rely on these models for the listing potential candidates. Another reason is that the predictive performance of current models is only modest with C statistics ranging from 0.63,7 to 0.7.5,8,10,31 Compared with existing models, ours has higher C statistics for the identification of patients at the increased risk of 90-day mortality and the advantage of being usable when patients are initially referred for LT. Therefore, it can be used to counsel patients and their families before surgery regarding their specific probabilities of short-term outcomes and expected survival up to 5 years after LT. Last, our scoring system was developed using a large national dataset that makes it more generalizable than other models developed using single-center datasets.

Despite all these advantages, the results of our study should be interpreted with some caution, because the scoring system has not been tested and validated using other cohorts of patients yet. Although the model performed well in identifying high-risk patients, several limitations are worth mentioning in addition to its retrospective design. First, we could only analyze and subsequently develop the model using variables collected in the STAR files. Because the risk of postoperative death depends on other factors that are not collected in the STAR files, we could not study the impact of malnutrition,44 sarcopenia,45 or frailty,46 which were identified as negative prognostic factors by other groups.

Another limitation is that the STAR files did not provide enough information to determine the sequence of events that led to postoperative death. Therefore, we could not assess whether patients expired from complications directly related to technical problems during surgery or had complications caused by preexisting conditions.

From the methodological point of view, our model might be less accurate when used in other populations.47 To address this issue, we are currently evaluating its validity and performance in a cohort of patients who underwent transplantation between June 30, 2013, and December 31, 2017. We also acknowledge that this model was developed using data from patients who underwent transplant surgery. Therefore, our findings might be mitigated by the inevitable selection bias, because only patients who were deemed surgical candidates were included. In addition, the model includes only preoperative variables and consequently does not incorporate the role of decisions made by surgeons and physicians before proceeding to each transplant. These complex decisions could plausibly modify the risks of perioperative mortality as many transplant programs commonly allocate the best grafts to patients with the highest perioperative risk and vice versa.

Also, the primary diagnosis of liver disease and recipient sex were intentionally excluded from the variables used to develop the predictive model, because the inclusion of these characteristics would disadvantage some groups of patients due to their gender or cause of liver failure. Therefore, it is possible that our model might perform differently in females versus males and for different causes of liver disease.

Finally, it is important to point out that this model was not developed for the stratification of patients who are known to be at an increased risk of perioperative death, such as patients who require a redo LT, or patients who undergo split livers or multivisceral transplant surgeries.

In conclusion, using machine learning techniques, we were able to develop a model to stratify the risk of 90-day postoperative mortality of patients referred for cadaveric LT. This model can also predict the risk of 1-year mortality and 5-year survival based only on pretransplant recipients’ clinical and demographic characteristics. Although the model has good discrimination for high-risk recipients, a validation study will be necessary to test its performance in a different cohort of LT recipients.",https://journals.lww.com/transplantjournal/fulltext/2019/10000/Prediction_of_Perioperative_Mortality_of_Cadaveric.23.aspx
Oztekin et al. 2017,A decision analytic approach to predicting quality of life for lung transplant recipients: A hybrid genetic algorithms-based methodology,"Feature selection, a critical pre-processing step for data mining, is aimed at determining representative variables/predictors from a large and feature-rich dataset for development of an effective prediction model. The purpose of this paper is to develop a hybrid methodology for feature selection using genetic algorithms to identify such representative features (input variables) and thereby to ensure the development of the best possible analytic model to predict and explain the target variable, quality of life (QoL), for patients undergoing a lung transplant overseen by the United Network for Organ Sharing (UNOS). The evaluation of three classification models, GA-kNN, GA-SVM, and GA-ANN, demonstrated that performance of the lung transplantation process has significantly improved via the GA-SVM approach, although the other two models have also yielded considerably high prediction accuracies. This study is unique in that it proposes a hybrid GA-based feature selection methodology along with design and development of several highly accurate classification algorithms to identify the most important features in the large and feature rich UNOS transplant dataset for lung transplantation.
","One of the goals of lung transplantation is to assess quality of life after the organ allocation. Therefore, the assessment of the outcomes (following the intended transplantation of the lung) should be considered to sustain the quality of life and survival in the post-transplant period. Amongst hundreds of variables used for decision-making within this domain, it is critical that the medical community and policy makers know which features are the most effective in achieving high levels of quality of life. This study focused on determining such features and deployed three GA-based models to determine the most important features while also sustaining high levels of accuracy. Fig. 11 pictorially demonstrates the common features among these three models. Three features (TX_LNG, TX_TYPE, and CMV_IGG) with the highest frequencies were observed to be common to the three proposed models. On the other hand, four features (CMV_IGM, PST_DRUG_TRT_INFECT, STEROID, and AGE) were common to the GA-SVM and GA-kNN models. Out of all 147 features, only two (PRIOR_CARD_SURG_TRR and INFECT_IV_DRUG_TRR) emerged as common only to the GA-kNN and GA-ANN models. Brief descriptions of the above-mentioned features are presented in Table 5.

The features selected with the decision-analytic methodology described here and depicted in Fig. 11 have been noted by others as important predictors of quality of life post lung transplantation. For example, Argenziano & Ginsburg (2002) found that unilateral lung transplantation, which is related to the two most important features in our study (TX_LNG and TX_TYPE), can lead to acute mediastinal shift with hypoxemia and hypotension. This complication can be best dealt with via independent lung ventilation to intentionally under-ventilate the native, emphysematous lung, while maintaining normal ventilation parameters to the transplant lung. Other problems specific to single-lung transplantation include native-lung diseases, such as pneumothorax and bronchogenic carcinoma. Further, reactivation of dormant tuberculous and nontuberculous micro-bacterial infections in the native lung is possible, as well as contamination of the transplant lung with pathogens colonizing the native lung, such as Aspergillus. In addition, patients receiving unilateral transplants have demonstrated lower survival rates and greater intolerance than bilateral transplant patients (Speicher et al., 2015; Arnaoutakis et al., 2011).

Regardless of transplantation type, cytomegalovirus (CMV) infection occurs in the majority of lung transplant patients (Zamora, 2004). Although typically not problematic in healthy individuals, illnesses caused by CMV can be life-threatening in those with a weakened immune system. As CMV can be managed, certain qualitative and quantitative tests (e.g., IgG antibodies, IgM antibodies, and polymerase chain reaction (PCR)) to confirm the presence of a recent or previous CMV infection should be applied for guiding preemptive therapy after lung transplantation (https://labtestsonline.org/understanding/analytes/cmv/tab/test/.2001-2016). In our study, the CMV_IGG test stands out as one of the top selected features and is common to all three data analytic models. Similarly, the CMV_IGM test was common to the GA-SVM and GA-kNN models.

After finding no significant difference between pre-LAS and LAS-based allocation eras, Wille et al. (2013) argued for the creation of a new lung allocation system. Compared to the available LAS predictors for quality of life in lung allocation, our findings suggest the candidate features to be more medically important than originally thought for recipients’ quality of life.",https://www.sciencedirect.com/science/article/pii/S037722171730855X?casa_token=ldia_G_PwqsAAAAA:2RGDNnmzPPXMiXxmEqaxBeS9BIVMEhU-tCJfmFIubCtIfSGIRM8_uMBdXykVsKcye1zqf_w3
Kwong et al. 2022,Machine learning to predict waitlist dropout among liver transplant candidates with hepatocellular carcinoma,"Background
Accurate prediction of outcome among liver transplant candidates with hepatocellular carcinoma (HCC) remains challenging. We developed a prediction model for waitlist dropout among liver transplant candidates with HCC.

Methods
The study included 18,920 adult liver transplant candidates in the United States listed with a diagnosis of HCC, with data provided by the Organ Procurement and Transplantation Network. The primary outcomes were 3-, 6-, and 12-month waitlist dropout, defined as removal from the liver transplant waitlist due to death or clinical deterioration.

Results
Using 1,181 unique variables, the random forest model and Spearman's correlation analyses converged on 12 predictive features involving 5 variables, including AFP (maximum and average), largest tumor size (minimum, average, and most recent), bilirubin (minimum and average), INR (minimum and average), and ascites (maximum, average, and most recent). The final Cox proportional hazards model had a concordance statistic of 0.74 in the validation set. An online calculator was created for clinical use and can be found at: http://hcclivercalc.cloudmedxhealth.com/.

Conclusion
In summary, a simple, interpretable 5-variable model predicted 3-, 6-, and 12-month waitlist dropout among patients with HCC. This prediction can be used to appropriately prioritize patients with HCC and their imminent need for transplant.
","Better risk stratification can identify those patients with HCC with greater urgency for LT. Our parsimonious 5-variable model predicted 3-, 6-, and 12-month waitlist dropout among patients with HCC with a c-statistic of 0.74. The analysis included all available variables directly collected in a large national database of all LT waitlist events since 2002, as well as a number of calculated features including average, minimum, and maximum, for a total of 1181 features. We evaluated dynamic longitudinal data involving laboratory values and imaging characteristics, so that clinical information could be entered at any timepoint during the waitlist period but also still consider initial tumor characteristics and response to locoregional therapy. The resulting five variables in the final model, including AFP, tumor size, bilirubin, INR, and ascites, represent the most predictive risk factors in this population. These variables are clinically relevant, representing both the severity of liver disease and the burden of HCC.8, 9 Patients with these risk factors are at higher risk of waitlist dropout due to the combined risk of liver failure and/or progression of HCC, with limited options for locoregional therapy due to the risk of hepatic decompensation while awaiting LT.

Together, these variables and their trajectory over time were combined to generate a predictive model to estimate the risk of waitlist dropout at 3-, 6-, and 12-month time horizons. The c-statistic of 0.74 is comparable to other proposed models, including a recently proposed waitlist dropout score by Mehta et al. based on listing variables (0.74 for LWR, 0.71 for MWR, and 0.73 for SWR).10 This model can help to risk stratify those patients with HCC with greater urgency for LT, versus those with more indolent disease who may be able to wait. We propose that a 6-month probability of waitlist dropout of ≤10% would be considered low risk, 10%–15% moderate risk, and ≥15% high risk––wherein a high-risk patient would derive greater benefit from timely LT, whereas a low-risk patient less so. These proposed values reflect the observed range of waitlist dropout risk at 6 months in the development cohort of the abovementioned risk score, which ranged from 3.6% in the lowest risk quartile and up to 28.1% in the highest risk quartile, with an overall dropout risk of 10.8% at 6 months. These thresholds may vary depending on local resources and donor availability, which can be influenced by recipient size, blood type, and geography.

Donor livers are a scarce resource in the United States. This predictive model can help to better risk stratify LT candidates with HCC and define their urgency for LT. Under the current US system, which was implemented in 2018, all local patients are assigned a static score (median MELD at transplant minus 3)––and so all LT candidates with HCC are considered equal. The policy in place prior to this––the “MELD escalator”––was also a fixed scoring system, which increased uniformly based on waitlist time rather than tumor burden or characteristics. However, there is clearly a differential risk of waitlist dropout and urgency based on dynamic patient and tumor characteristics, and those with a higher risk of waitlist dropout could be granted higher allocation priority. Such a system has been proposed and implemented in Québec, Canada, with more exception points granted to those with increased number and/or size of tumor, and no observed adverse effect on graft or patient survival.11 In the context of organ shortage, living donor LT and expanded criteria donor options may also be appropriate options to explore for those identified to have a higher dropout risk and thus greater urgency for LT. Extended criteria, or “marginal” livers, for example, donation after circulatory death or steatotic liver grafts, are potentially associated with inferior posttransplant outcome, but may still confer an overall survival benefit particularly for patients with HCC in greatest need of transplant––who still generally have lower waitlist priority relative to patients with chronic liver disease in the current US allocation system.12, 13

It must be noted that prioritization of those at higher risk of waitlist dropout will need to be balanced against the individual risk of post-LT HCC recurrence. In a recent study considering the effect of listing characteristics including tumor size and number, AFP, CTP, and MELD-Na score on post-LT survival, inferior 5-year post-LT survival was observed in the highest risk stratum.10 However, below this threshold, the risk of waitlist dropout could be stratified without significant differences in post-LT survival. In contrast, our model leverages dynamic waitlist data––representing the evolution of liver disease and response to therapy during the waiting period––to evaluate dropout risk in real time and can be used in conjunction with existing scores such as this to estimate the overall survival benefit.

Traditional linear or survival methods that have been previously applied to this dataset are more limited in scale, require parametric specification, and may not recognize high-order interactions. The random forest method can handle a large number of variables and account for complex interactions and non-linearities in the underlying data. A previous Cox proportional hazards model for waitlist dropout using OPTN data identified MELD, maximum tumor size, and AFP as important predictors of waitlist dropout at 3 months, with a c-statistic of 0.78.14 Duvoux et al. proposed the French AFP model, comprised of AFP, tumor size, and number, to predict HCC recurrence, with an AUC of 0.70.15 A more recent model using competing risk analysis, with an endpoint of waitlist dropout at 3 months, added in age, number of tumors, and etiology of liver disease, resulting in a c-statistic of 0.72.16 Prediction models to stratify longer term dropout risk have not been widely investigated.

Prediction models that use machine learning methods have gained traction in recent years.17 Using a similar tree-based method and a total of 28 variables, Bertsimas et al. demonstrated superior performance of their OPOM model compared to MELD-Na for the prediction of 3-month waitlist mortality.18 In simulation, this model allocated more livers to non-HCC waitlist candidates and improved waitlist outcomes for both HCC and non-HCC patients, compared to MELD-Na. Our proposed model adds to this literature and is targeted for patients with HCC, incorporating tumor-specific variables and a longer time horizon. The model also accounts for dynamic longitudinal data, so that clinical information could be entered from just one timepoint (i.e., listing) but also additional timepoints before or after. Data could be entered during the waiting period, with the relevant longitudinal values (e.g., minimum, maximum, and average) being calculated over all previous timepoints provided, and the prediction would be made for 3-, 6-, and 12-month survival from the current time. This type of risk stratification may help to inform center-level clinical decision-making regarding waitlist management.

Limitations of this study include its retrospective nature and reliance on the UNOS database, which is relatively comprehensive but potentially susceptible to data entry errors and inconsistencies. Uncaptured variables such as PIVKA-II, degree of differentiation, and type of locoregional therapy may also influence the natural history and outcomes of HCC but are not considered in OPTN-derived models. These variables are not readily available for most liver transplant candidates with HCC in the United States and so are not yet practical for use in clinical prediction modeling. Response to locoregional therapy was indirectly represented by tumor size and number over time. Although there is preliminary data to suggest that outcomes may be worse in patients with HCC and liver disease due to nonalcoholic steatohepatitis or hepatitis C virus, the etiology of liver disease was not predictive in our analysis and so was not included in the prediction model.19, 20 Center variation in thresholds and risk tolerance for bridging locoregional therapy, as well as changes in organ allocation and access to transplant, could also influence the predicted waitlist outcome. In addition, patients by design contributed varying amounts of data based on the length of waitlist time, which could have introduced some bias to the model development. The median waiting time in this cohort was 152 days (IQR 50–308), meaning that most patients contributed data from more than one timepoint. Consequently, this model may be more representative of outcomes in regions with longer wait times, whose patients had the opportunity to contribute more waitlist data. Waiting times for LT candidates with HCC have lengthened in recent years, and the majority are now subject to a mandated waiting period of at least 6 months prior to LT, making this aspect of the model relevant to the current allocation system. Much of the study period predated widespread adoption of a standardized downstaging protocol in the United States, and so outcome prediction of larger tumors may be less stable. Finally, while the model was internally validated, external validation is necessary to establish its generalizability, particularly outside of the United States with differing allocation policies for patients with HCC.

In summary, this 5-variable model effectively predicts 3-, 6-, and 12-month waitlist dropout among patients with HCC. These variables identified as the strongest predictors represent both the severity of underlying liver disease and tumor burden. Models developed using machine learning methods can identify important predictors and help to guide center-level clinical decision-making and organ acceptance practices. This calculator can be applied to risk stratify LT candidates with HCC and recognize the patients who are at highest risk of waitlist dropout while awaiting LT.
",https://onlinelibrary.wiley.com/doi/10.1002/cam4.4538
Tolstyak et al. 2021,The Ensembles of Machine Learning Methods for Survival Predicting after Kidney Transplantation,"Machine learning is used to develop predictive models to diagnose different diseases,
particularly kidney transplant survival prediction. The paper used the collected dataset of patients’
individual parameters to predict the critical risk factors associated with early graft rejection. Our
study shows the high pairwise correlation between a massive subset of the parameters listed in
the dataset. Hence the proper feature selection is needed to increase the quality of a prediction
model. Several methods are used for feature selection, and results are summarized using hard
voting. Modeling the onset of critical events for the elements of a particular set is made based on
the Kapplan-Meier method. Four novel ensembles of machine learning models are built on selected
features for the classification task. Proposed stacking allows obtaining an accuracy, sensitivity, and
specifity of more than 0.9. Further research will include the development of a two-stage predictor.","Given the data obtained, it can be argued that six indicators affect the survival of
kidney transplantation in the perioperative period, primarily the lack of previous hemodialysis, high leukocytosis, urea and creatinine in the blood, proteinuria, and erythrocyturia in
the urine. Thus, based on these data, the first signs of possible rejection can be predicted
based on five general screening laboratory tests.
The data preprocessing allows us to increase the quality of analysis. Boruta, RFE, and
information gain are used for feature selection. The results of the selection are formed
based on hard voting of all features selectors.
A stacking classifier for early graft rejection prediction algorithm based on combination
of 6 weak classifiers was developed in this paper. Random forest algorithm is used to
combine the predictions. Using this ensemble allows us to increase the defect prediction
accuracy up to 92%. Therefore, the proposed algorithm shows higher accuracy comparing
to weak classifiers and boosted and bagged ensembles.",https://www.mdpi.com/2076-3417/11/21/10380
Moghadam et al. 2022,A machine learning framework to predict kidney graft failure with class imbalance using Red Deer algorithm,"The prevalence of chronic kidney diseases has raised the demand for kidney replacement therapy over recent years. Although kidney transplantation provides better life quality than other alternatives, the replacement might face rejection and endanger individuals’ lives. Therefore, predicting this rejection is of great importance, and due to the complexity of the human body's immune system, it is a complicated task. Machine learning has been widely used in disease diagnoses such as graft rejection. One of the common issues in disease diagnosis is the class imbalance problem. The predictive model is likely to misclassify all of the minority class samples in such cases. This paper proposes a method that can accurately predict kidney rejection in an imbalanced dataset. We have adopted kidney transplantation data from 378 patients collected from 1994 to 2011. Our main contribution is to develop a novel clustering method using Red Deer Algorithm (RDA), which is later utilized in proposing a three-stage clustering-based undersampling approach to handle the class imbalance problem so that the final predictive models can classify the given data more accurately. The undersampling method includes denoising, RDA clustering, and sample selection. Moreover, the proposed clustering method is also used to reduce the data dimensionality. Subsequently, five different classification algorithms such as Support Vector Machine (SVM), Artificial Neural Network (ANN), K-Nearest Neighbor (KNN), Decision Tree (DT), and an ensemble method are used to predict kidney graft rejection. Then, these classifiers are compared in terms of five performance evaluation metrics: accuracy, sensitivity, specificity, F1 score, and area under the Receiver Operating Characteristic (ROC) curve (AUC). The obtained results indicate that the decision tree model outperformed other algorithms and achieved 0.96, 0.94, 0.97, 0.95, and 0.95 for accuracy, sensitivity, specificity, F1 score, and AUC, respectively. Hence, it is highly recommended to use the proposed method as a decision support system for clinical experts to predict kidney transplantation failure.","Over the recent decades, the end-stage renal disease has been one of the most widespread medical conditions among individuals worldwide. A patient’s kidney usually needs to be transplanted for end-stage kidney diseases. Thus, the demand for kidney replacement has risen. Due to the complex nature of the human body, transplantation might fail and endanger patients’ lives. Therefore, finding an appropriate donor is of great importance. Although prediction of graft failure is a complicated task, machine learning algorithms have shown superior performance in clinical decision-making, such as kidney graft rejection prediction. In disease diagnosis, the class imbalance is a common issue that should be carefully addressed. Otherwise, the model tends to label all instances as majority class samples. In this study, it was desired to predict kidney graft rejection using machine learning algorithms. We adopted a kidney graft dataset of 378 patients who had undergone kidney replacement. A three-phase clustering-based undersampling approach was introduced to address the class imbalance problem.

The undersampling method consists of denoising, RDA clustering, and sample selection phases. In the second phase, a novel metaheuristic-based clustering approach using the red deer algorithm was presented to cluster majority class samples. The results of the cluster validity index showed that the proposed clustering algorithm outperformed other conventional ones like K-means and fuzzy c-means. It was also compared with other metaheuristics, such as PSO and GWO, and the superiority of the proposed clustering algorithm was shown. Employing the undersampling method, the ratio of majority to minority class samples is close to one, and a classification algorithm was expected to perform well. We proposed a feature extraction method based on the RDA-based clustering method to decrease the data dimensionality. This method reduced the dimensionality of data from 19 to 9. Then five classification algorithms, namely SVM, ANN, KNN, DT, and ensemble method, were implemented to predict kidney graft failure. To evaluate the performance of predictive models, accuracy, sensitivity, specificity, F1 score, and AUC of each algorithm were reported. The results showed that the decision tree model outperformed the other algorithms with 0.96, 0.94, 0.97, 0.95, and 0.95 for accuracy, sensitivity, specificity, F1 score, and AUC, respectively. Moreover, the results obtained from decision trees are more understandable and interpretable. Hence, the proposed model in this study could be used as a decision support system for clinical experts to predict kidney graft failure.
",https://www.sciencedirect.com/science/article/pii/S0957417422015949#ab010
Chawla et al. 2022,Predicting the Kidney Graft Survival Using Optimized African Buffalo-Based Artificial Neural Network,"A variety of receptor and donor characteristics influence long-and short-term kidney graft survival. It is critical to predict the effectiveness of kidney transplantation to optimise organ allocation. This would allow patients to choose the best accessible kidney donor and the optimal immunosuppressive medication. Several studies have attempted to identify factors that predispose to graft rejection, but the results have been contradictory. As a result, the goal of this paper is to use the African buffalo-based artificial neural network (AB-ANN) approach to uncover predictive risk variables related to kidney graft. These two feature selection approaches combine to provide a novel hybrid feature selection technique that could select the most important elements to improve prediction accuracy. The feature analysis revealed that clinical features have varied effects on transplant survival. The collected data is processed in both training and testing methods. The prediction model's performance, in terms of accuracy, precision, recall, and F-measure, was examined, and the results were compared with those of other existing systems, including naive Bayesian, random forest, and J48 classifier. The results suggest that the proposed approach can forecast graft survival in kidney recipients' next visits in a creative manner and with more accuracy compared with other classifiers. This proposed method is more efficient for predicting kidney graft survival. Incorporating those clinical tools into outpatient clinics' everyday workflows could help physicians make better and more personalised decisions.","The extensive availability of alternative treatments has increased the life span of patients having end-stage renal disease. The performance of the AB-ANN technique in detecting the survival rate of persons with kidney graft failure was compared with that of other methodologies in this research. The suggested prediction methodology is tested using the UNC dataset, and the results are compared with other recent methods. The predictions generated by ANN were more exact than previous techniques based on the evaluation parameters like accuracy, precision, recall, f-measure, and error rate.

Experiments demonstrated that the newly proposed kidney transplantation survival estimation technique surpassed all previous current strategies, with prediction accuracy and F-measure scores of 99.89 percent and 99.2 percent, respectively. The proposed prediction technique has achieved best accuracy, higher speed, and higher F-measure. Furthermore, the novel feature selection strategy has been successful in speeding up categorization by decreasing the amount of characteristics to a minimum. As a result, it is obvious that the proposed procedure is quite reliable and produces excellent outcomes. The nature of this model allows it to be utilized for both short and long-term forecasting.

Such predictive techniques could aid in the implementation of personalised treatment in kidney transplantation. It is stated that the innovative proposed prediction technique can increase classification accuracy while reducing feature selection complexity. These results show the efficacy of the proposed strategy. The proposed prediction model might be used to a variety of transplant datasets, according to the researchers.",https://www-ncbi-nlm-nih-gov.myaccess.library.utoronto.ca/pmc/articles/PMC9124117/
Paquette et al. 2022,Machine Learning Support for Decision-Making in Kidney Transplantation: Step-by-step Development of a Technological Solution,"Background

Kidney transplantation is the preferred treatment option for patients with end-stage renal disease. To maximize patient and graft survival, the allocation of donor organs to potential recipients requires careful consideration.

Objective

This study aimed to develop an innovative technological solution to enable better prediction of kidney transplant survival for each potential donor-recipient pair.

Methods

We used deidentified data on past organ donors, recipients, and transplant outcomes in the United States from the Scientific Registry of Transplant Recipients. To predict transplant outcomes for potential donor-recipient pairs, we used several survival analysis models, including regression analysis (Cox proportional hazards), random survival forests, and several artificial neural networks (DeepSurv, DeepHit, and recurrent neural network [RNN]). We evaluated the performance of each model in terms of its ability to predict the probability of graft survival after kidney transplantation from deceased donors. Three metrics were used: the C-index, integrated Brier score, and integrated calibration index, along with calibration plots.

Results

On the basis of the C-index metrics, the neural network–based models (DeepSurv, DeepHit, and RNN) had better discriminative ability than the Cox model and random survival forest model (0.650, 0.661, and 0.659 vs 0.646 and 0.644, respectively). The proposed RNN model offered a compromise between the good discriminative ability and calibration and was implemented in a technological solution of technology readiness level 4.

Conclusions

Our technological solution based on the RNN model can effectively predict kidney transplant survival and provide support for medical professionals and candidate recipients in determining the most optimal donor-recipient pair.","Overview

This study focused on the development of an ML-based decision support solution to help kidney transplant practitioners and their patients make informed decisions when a deceased donor kidney becomes available. All stages of the development process are described: data acquisition and preparation, evaluation of existing survival analysis models, development and evaluation of a new survival analysis model, and deployment of the technological solution of TRL-4.

Principal Findings

When building survival analysis models in the context of kidney transplantation, there are several factors that characterize the models and ultimately influence the final quality of the prediction tool.

One factor is the size of the data sets used to build these models. It varies widely between studies, ranging from 80 [39] to 131,709 transplants [16]. It has been demonstrated that large sample sizes improve the predictive performance of ML models [40]. Another important factor is the period for which the risk of mortality or graft failure is predicted. This may depend on data availability and duration of the observation period. Mark et al [22] built an ensemble model to predict patient survival throughout the first 5 years following kidney transplantation. Luck et al [16] evaluated the graft survival probability at each anniversary date of the graft for 15 years following transplantation. Our study was based on the most recent available data and included up to 19 years of observations of 180,141 transplant procedures. The models presented here evaluate graft survival probabilities at each quarterly anniversary of the graft for 15 years. To the best of our knowledge, this is the largest data set with the longest observation period used to build ML models for predictions in the kidney transplantation area.

The performance of a predictive model is also strongly dependent on incorporating prognostically significant variables into the models. The number of variables used for survival analysis in the literature ranges from 6 to several hundred [16,21,41,42]. Selection of a very small number of variables may lead to the exclusion of important factors that may influence the outcome of the transplantation, whereas including a very large number of variables may increase the sparsity of the data, which in turn may cause overfitting. In this study, variables were selected based on medical expertise, previous studies [18,22], and characteristics such as data completeness and data duplication for the first step (35 variables).

The choice of a survival analysis model is also critical. Multiple options have been described in the literature, such as the Cox regression model [18], decision trees [43], support vector machines [44], Bayesian belief networks [12], RSF [22], and artificial neural networks [16,21].

In this study, 5 different models were explored: a regression-based Cox proportional hazards model; RSF; and 3 neural network models, namely, DeepSurv, DeepHit, and a proposed RNN. To the best of our knowledge, the latter was used on kidney transplantation data for the first time in this study. These models were evaluated on the task of predicting kidney graft survival throughout the first 15 years following transplantation. Three metrics were used to evaluate each model: the C-index, IBS, and ICI, along with calibration plots.

Evaluation of ML Models

The results for the C-index metric shown in Table 1 indicate that the neural network–based models (DeepSurv, DeepHit, and RNN) had better discriminative ability than the Cox model and RSF. In fact, the DeepHit model and our proposed RNN model performed best with a C-index of 0.661 and 0.659, respectively. This indicates their ability to discern groups of donor-recipient pairs that were at a higher risk of experiencing graft failure after transplant from groups that had a lower risk. The improvement compared with the widely used Cox model (C-index of 0.646) may be because of the higher capacity for feature extraction by the neural networks.

The main drawback of the Cox proportional hazards model and DeepSurv is the assumption that the computed hazard ratio is time invariant. In contrast, DeepHit and RNN make no assumptions about the distribution of time-to-event data and can learn the time-varying effects of covariates, making them more flexible. This is important when evaluating survival over a wide time frame, as in our study, over 15 years. For example, a covariate could have a negative effect on survival in the first few years after transplantation but no impact in the later years.

Previously published articles on the prediction of survival of kidney grafts from deceased donors often described different evaluation metrics, such as accuracy [15,44], mean relative absolute error, root mean square error, mean absolute error [15], and C-index [14,16,18], which makes it difficult to perform a comparison between the studies.

The performance of the proposed DeepHit and RNN models evaluated with the C-index is comparable with the previously published iChooseKidney technological solution (0.6640 at 3 years after transplantation) [14] and slightly exceeds the performance of the deep learning survival model described by Luck et al [16] (0.6550). However, the comparison of models based on the C-index alone is limited to the evaluation of their discriminative ability and does not consider the average accuracy of the survival predictions. Making use of ICI and smoothed calibration curves [31,32] helped shed light on the model’s predictive quality.

From the results presented in Table 1 and Figure 3, we can see that there is often an imbalance between a model’s discriminative ability and its calibration. As discriminative ability is required to differentiate between high-risk and low-risk kidney transplants, one might prefer a model with a higher C-index if a comparison of donor-candidate pairs is to be performed, for example, in the case of organ allocation. In contrast, as good calibration is required to provide reliable graft survival predictions, a model with better calibration may be preferable in cases where personalized expected graft survival distributions are to be presented, for example, to a transplant candidate.

Characteristics of the Developed Technological Solution

We developed a client web application to predict organ survival probability for each potential kidney donor-recipient pair for a period between 1 and 15 years after the transplantation. We opted to use the proposed RNN model to deploy our prototype application. This model offers a compromise between the good discriminative ability and the calibration necessary for the purpose of our application. Indeed, one of the main uses of the decision support application is to simultaneously present graft survival probabilities to a kidney transplant candidate and to offer a point of comparison by presenting graft survival predictions that the patient could expect with other potential donors.

It would be possible to use an alternative approach for computing the predictions at the time now + average time before a new kidney is available. To achieve this purpose, it would be necessary to compute the survival for every possible additional wait time and the probability of that wait time occurring, along with the patient survival to that wait time. This could be an objective for future studies.

The presented choice of approach to evaluate the average donor predictions at the same time now as the predictions for the offered donor kidney is a matter of simplicity and an effective way for patients without statistical background to look at 2 options (accept or refuse the transplant) and understand the possible outcomes.

The client application is at the prototype stage (TRL-4), aiming to demonstrate the capabilities of the ML predictive model. The following information about the candidate recipient is entered in the first step of the application: age, height, weight, ethnicity, sex, diagnosis, number of years on dialysis, presence of diabetes, and presence of angina. The details about potential donor that are entered in the next step are donor’s age, height, weight, ethnicity, donation type, creatinine level, history of diabetes, hypertension diagnosis, hepatitis C diagnosis, and smoking habit. These covariates are used as input for the trained RNN model. In the next step, the user selects the number of years for the prediction target. The output page displays the probability of survival of the transplant for the given donor-recipient pair and specified period as well as for the candidate recipient and average NDD and DCD donors for comparison. It is also possible to expand the result boxes to obtain a detailed view of the results for any specific transplant prediction.

Future Perspectives

The current application is recipient-oriented and specific to kidney transplantation. Future research could expand this application to other transplanted organs and nonrecipient users. For example, if connected to a candidate database, the application can produce an ordered list of optimal donor-recipient matches when an organ becomes available. The Expo.io development environment for the client was chosen for its capability to support web, Android, and iOS environments, leaving many options open for the distribution and accessibility of the service. The client also connects to the model by using an application programming interface. Thus, although the initial prototype was entirely run in a local environment, the solution could easily be transferred to a cloud-based environment.

In the future, the application could also be extended to include additional predictive models to further inform patients. For example, when a kidney is offered to a patient, it would be instructive to predict the expected waiting time before a better kidney becomes available should the patient decide to remain on the waiting list. The solution could also be upgraded to enable the recommendation of the best candidate recipient for each newly available kidney from the existing candidate waiting list based on the predicted graft survival.

Limitations

Our study has certain limitations, which are important to mention. A built-in selection bias exists in the SRTR data set. It is evident that deceased donor kidneys accepted for transplantation have superior characteristics than those that were never used for transplantation and therefore do not appear in the data. The data were imbalanced according to different age, sex, and racial groups. These selection biases may negatively affect the accuracy of predictions made for candidate recipients or donors who fall into underrepresented populations.

Another limitation is the level of detail available in the data set. The registry-level data from the SRTR certainly does not encapsulate all the characteristics of the clinical and functional status of donor-recipient pairs. Consequently, there must be factors that influence graft survival that were not present in the data. We also did not consider HLA typing, an important variable when matching donors and recipients, because of the complexity of modeling HLA mismatches. We must also consider the population of the United States, on which the models were built. Multiple factors, such as age, race, and state of residency, may reflect the socioeconomic status of patients, which itself may affect access to health care. To use the models built in this study in other countries, for example, in Canada, one must consider that some factors may differently affect graft survival.

Conclusions

We analyzed and tested 5 ML models to predict kidney graft survival for a period of up to 15 years after transplantation. This study focused on patients who received deceased donor kidney transplants in the United States between 2000 and 2019 and included both NDD and DCD transplants. The resulting RNN predictive model was integrated into a decision support application designed to help kidney transplant practitioners and their patients make informed decisions regarding transplant options.",https://www-ncbi-nlm-nih-gov.myaccess.library.utoronto.ca/pmc/articles/PMC9240927/
Naqvi et al. 2021,Predicting Kidney Graft Survival Using Machine Learning Methods: Prediction Model Development and Feature Significance Analysis Study,"Background: Kidney transplantation is the optimal treatment for patients with end-stage renal disease. Short- and long-term kidney graft survival is influenced by a number of donor and recipient factors. Predicting the success of kidney transplantation is important for optimizing kidney allocation.

Objective: The aim of this study was to predict the risk of kidney graft failure across three temporal cohorts (within 1 year, within 5 years, and after 5 years following a transplant) based on donor and recipient characteristics. We analyzed a large data set comprising over 50,000 kidney transplants covering an approximate 20-year period.

Methods: We applied machine learning-based classification algorithms to develop prediction models for the risk of graft failure for three different temporal cohorts. Deep learning-based autoencoders were applied for data dimensionality reduction, which improved the prediction performance. The influence of features on graft survival for each cohort was studied by investigating a new nonoverlapping patient stratification approach.

Results: Our models predicted graft survival with area under the curve scores of 82% within 1 year, 69% within 5 years, and 81% within 17 years. The feature importance analysis elucidated the varying influence of clinical features on graft survival across the three different temporal cohorts.

Conclusions: In this study, we applied machine learning to develop risk prediction models for graft failure that demonstrated a high level of prediction performance. Acknowledging that these models performed better than those reported in the literature for existing risk prediction tools, future studies will focus on how best to incorporate these prediction models into clinical care algorithms to optimize the long-term health of kidney recipients.","Principal Findings

The cross-cohort prediction results (Table 6) confirm the efficacy of the classifiers—the prediction model for cohort 3 (ie, AdaBoost) correctly offers a high prediction score for data from cohort 1 (72%) and cohort 2 (75%). The prediction model for cohort 2 offers a high prediction score for cohort 1 data (79%) but a low prediction score for cohort 3 (58%) data. The classifier for cohort 1 (ie, SVM) gave low prediction scores for data from cohort 2 (42%) and cohort 3 (29%). Interestingly, the highest prediction score by a cohort-specific classifier was always achieved for data from its respective cohort. The prediction modeling results confirmed that the prediction models were highly sensitive to their respective cohorts.

Comparing Prediction Performance With Prior Studies

We compared the prediction performance of our ML-based prediction models with comparable organ transplant studies that involved similar-sized observations and temporal windows. Table 9 summarizes the findings of the two studies for each cohort. There have been several other studies [19,31,32,35,37,38] to predict the short-term graft status of different organ transplants, but because of their small data set, these do not serve as a meaningful comparison. When comparing our results with prior studies, it is noted that although our cohort 2 prediction performance (ie, graft status prediction over a 5-year period) is lower than that of Lin et al [16], it was based on a much smaller data set that included 10,641 survivals and 7215 failures, whereas we analyzed 23,475 failures and 29,352 survivals. Similarly, Tiong et al [39] analyzed a smaller sample of 20,085 living donor transplant recipients to achieve a concordance index of 71%. Our cohort 3 prediction performance is marginally lower compared with Lin et al [16], who predicted a 7-year graft survival with an 82% AUC score, whereas our cohort 3 prediction model covers a much longer (17 years) temporal window and achieves a comparable prediction score. Using a similar number of transplants, Luck et al [40] achieved a much lower concordance index between 63% and 66% for 14-years graft survival.

Limitations and Future Work

A limitation of our research lies in the removal of censored instances. We removed all successful cases that were censored before 8 years following transplant. Although this type of approach has previously been used, including censored cases is a potential consideration for future analyses.",https://www-ncbi-nlm-nih-gov.myaccess.library.utoronto.ca/pmc/articles/PMC8433864/
Fouad et al. 2015,"Prediction of Long Term Living Donor Kidney Graft Outcome: Comparison Between Rule Based, Decision Tree and Linear Regression","Predicting the outcome of a graft transplant with high level of accuracy is a challenging task In medical fields and Data Mining has a great role to answer the challenge. The goal of this study is to compare the performances and features of data mining technique namely Decision Tree, Rule Based Classifiers with Compare to Linear Regression as a standard statistical data mining method to predict graft survival period of kidney transplants over a 5-year horizon. The dataset was compiled from the Urology and Nephrology Center (UNC), Mansoura, Egypt. classifiers were developed using the Weka machine learning software workbench by applying Rule Based Classifiers (M5Rules), Decision Tree Classifiers (REPTree) and Linear Regression. Further from Experimental Results, it has been found that Rule Based classifiers and Decision Tree are providing improved Accuracy and interpretable models compared to other Classifier.","In this paper, we  have discussed the need  for  data  mining  in the medical field especially in prediction of kidney transplant outcomes in UNC Mansoura, Egypt. In  this  context  we  compared between classification Rule Based Classifiers (M5Rules), Decision Tree Classifiers (REPTree) and Linear Regression as a standard statistical data mining method to predict the outcome of kidney transplants  over  a  5-year  horizon  using  the  patient  profile information prior to the transplantation. and found that classication predictive accuracy of rule based classifiers (M5Rules ) model was superior other models in  predicting 5-year graft survival when run against kidney transplanation dataset obtained from urology and nephrology center, mansoura, egypt.further we  have found rule set containing some interesting rules which were easy to interpret and familiar to represent them in spreadsheet were obtained from rule based classifiers and decision tree classifiers. The experimental results  also  reveal  that  rule based  classifiers and  decision tree classifiersare efficient approaches for extraction of patterns from kidney transplanation dataset. In a future project, we shall implement the rule based classifiers model that was developed in this  study in a form of web based application   to  make  it  available  to  estimate  survival  and prognosticate individual transplant recipients outcomes",https://www.researchgate.net/publication/283501232_Prediction_of_Long_Term_Living_Donor_Kidney_Graft_Outcome_Comparison_Between_Rule_Based_Decision_Tree_and_Linear_Regression
Goldfarb-Rumyantzev et al. 2003,Prediction of 3-yr cadaveric graft survival based on pre-transplant variables in a large national dataset,"Pre- and post-transplant predictive factors of graft survival for optimal and expanded criteria grafts have been studied in the past. The goal of our study was to evaluate the recent large set of United Network of Organ Sharing records (1990–1998) to generate a prediction algorithm of 3-yr graft survival based on pre-transplant variables alone. The dataset of patients with end-stage renal disease and cadaveric kidney or kidney–pancreas transplantation (1990–1998) used in the study consisted of 37 407 records. Logistic regression (LM) and a tree-based model (TBM) were used to identify predictors of 3-yr allograft survival and to generate prediction algorithm. Donor and recipient demographic characteristics (age, race, and gender) and body mass index showed non-linear, while human leukocyte antigen match showed strong linear relationships with 3-yr graft survival. Prediction of the probability of graft survival from the model, achieved a good match with the observed survival of the separate dataset, with a correlation of r = 0.998 for LM and r = 0.984 for TBM. The positive predictive value (PV) of allograft survival with LM and TBM was 76.0% and the negative PV was 63 and 53.8% for LM and TBM, respectively. Both LM and the TBM can potentially be used in clinical practice for long-term prediction of kidney allograft survival based on pre-transplant variables.","Factors affecting kidney allograft survival were evaluated previously based on local datasets and national databases. We looked at a large national dataset which includes relatively new collection of data covering all renal and kidney–pancreas transplants between 1990 and 1998 and, using strict criteria, eliminated records with incomplete information and made careful imputation of some variables. While cleaning the data, we encountered certain problems with missing and poorly reported values. The extent of the missing data for some variables is presented in Table 2. The amount of misreported or missing information in 1990–1998 UNOS dataset can be explained by several factors that need to be considered by researchers analyzing the data. As we mentioned in the methods section, certain variables may not have been collected over the entire time period of the cohort. For example, donor history and duration of diabetes and HTN, recipient most recent serum creatinine and donor terminal creatinine were collected only since April of 1994. Additionally, voluntary data submission via paper form (with some fields not mandatory) account for much of the missing information. It has been speculated that this is the reason that height and weight are not populated well (e.g. recipient height is missing approximately 90% of the values). In some cases, the field may not be relevant in that particular instance, so the member may choose to leave it blank [e.g. data for panel reactive antibodies (PRA) tends to be entered only if the patient is sensitized: PRA 80 or above, otherwise it is left blank]. One may expect improved quality of data in the future. The number of outstanding forms has been steadily declining in recent years (there were almost 150 000 outstanding forms in April of 2000 and 56 000 in September 2002) (UNOS, personal communications). UNet, the online transplant data entry system, was implemented in October 1999. The new system of on-line data entry employed real-time data quality control, forcing the user to enter the data in a correct and unified format. Therefore, the quality of information should substantially improve. Thus far, the effect of the new system on the individual variables over time has not been studied closely. In this study, missing categorical variables were coded as new categories and missing continuous variables were replaced using appropriate data imputation methods. In particular, a tree-based algorithm was used for height and weight imputation. The algorithm that we developed has been shown to have good precision when validated on a separate database derived from USRDS DMMS Wave 3 and 4 study patients. Tree-based imputation can be a useful tool for the researchers analyzing the datasets with missing values of the anthropometric characteristics. After careful imputation the results and conclusion of our analysis should not be affected by various causes of missing data (UNOS not collecting it vs. poor reporting). During cleaning of the initial dataset we tried to preserve as much useful information as possible and at the same time eliminate potentially erroneous, incomplete, or unreliable information. A significant number of records had to be eliminated as some critical information was missing or deemed unreliable (Fig. 1).

Our bivariate and multivariate analyses demonstrated the importance of several pre-transplant donor, recipient, and procedure variables in predicting 3-yr graft survival: the number of previous kidney transplants in recipients has a direct relationship with the transplant failure rate, diabetes and HTN worsen the outcome. The relationship between donor and recipient age, race, gender, and 3-yr graft survival previously reported (7, 30), and is non-linear. Number of HLA matched/mismatched antigens has a very strong linear relationship with the percent 3-yr graft survival. The effect of cold ischemia time on the other hand is much less dramatic by bivariate analysis than we initially expected and that was previously reported (11). The transplant center effect was studied before (22) and showed only a very slight difference between the large and small centers. In our study centers with higher number of transplants have more similar outcome, while the outcome of the smaller centers has a lot of variability. This may represent either a regression to the mean or true phenomenon of more uniform outcome that comes with greater experience. LM model selected transplant center volume as a predictor of the outcome, centers with less experience increasing the risk of 3-yr allograft failure. Some of the causes of ESRD in a recipient as a predictor of the outcome were described before (16). In addition, we evaluated all the diagnoses with known 3-yr outcome that we could derive from UNOS database. We confirmed previously reported beneficial effect of preemptive transplantation (21, 31, 32). BMI of donor and recipient as well as recipient obesity in relation to outcome discussed in literature and found to have an important role in the prediction of the kidney allograft outcome in some studies (12, 33), while in others obese (high BMI) transplant recipients have similar outcomes to non-obese patients (34). In our study, both donor and recipient BMI found to be good predictors of the outcome by means of bivariate and multivariate analysis. As successful transplantation of adult living donor kidneys into infants and small children with good long-term outcome has been shown in a small study (35), the unexpected almost bell-shaped curve (Fig. 3, panel A) describing relationship between donor-to-recipient BMI and graft survival is surprising. That may represent either the deleterious effect of donor obesity (33) or the impact of poor recipient nutritional status. This relationship needs to be further evaluated in prospective study and may be an important factor affecting the selection of the donor.

The novel part of this study is the predictive model. The time period of interest covers the ‘post-cyclosporine era’, however the 1990s were associated with changes in immunosupression protocols and surgical technique, and therefore the database represents very heterogenous population. This heterogeneity may potentially affect the performance of the prediction models, especially as we included in the analysis only a limited number (26) of pre-transplant independent variables. In designing the study, our intent was to develop a prediction model for use prior to transplantation, therefore we excluded post-transplant variables, that are not available until after the transplant procedure, we did not analyze the impact of immunosuppressive therapy, immediate post-transplant graft function and episodes of acute rejection as this information is not available prior to transplant. Along with conventional LM model, we used a TBM, never before used to analyze transplant outcome. This model represents a relatively new approach compared with conventional regression analysis of the data. The interest in this statistical approach has been increasing over the last 10 yr. Several features make TBM a powerful tool for building a prediction algorithm that can be successfully used in practice. TBM works when the regression variables are a mixture of categorical and continuous variables, it is often able to uncover complex interactions between predictors which may be difficult or impossible to do using traditional multivariate techniques. The algorithm is non-parametric, so no assumptions are made regarding the underlying distribution of values of the predictor variables. TBM identifies ‘splitting’ variables based on an exhaustive search of all possibilities, even in problems with many hundreds of possible predictors. Simultaneously, it requires relatively little input from the analyst. This graphical algorithm, presented as a collection of simple binary rules, is much simpler to interpret by a non-statistician than the multivariate LM. Thus can be used in the decision making without doing any additional calculations, and therefore is more likely to be followed in clinical practice.

Prediction models using LM and tree-based algorithms are developed in this study on the large set of data, and can potentially be used in the recipient counseling and decision making regarding cadaveric renal transplants. Relatively low area under ROC curves of the initial models suggests that longer list of the potential predictors should be evaluated. However, the prediction algorithms generated on the training dataset can be successfully used in practice to identify the probability of 3-yr kidney allograft survival, as both models achieved good precision in predicting the probability of the graft survival on the separate set of data. There is an experience of using similar data derived from univariate and multivariate analyses in a smaller study in a cadaveric kidney allocation decision making in North Italy Transplant Program (36). The identification of factors that play an important role in graft survival helps to focus efforts of transplant programs on certain individual aspects of patient care. The implementation of the models that were generated in this study in a form of software to make it available for transplant program and prospective transplant recipients may be a subject of future projects.",https://onlinelibrary-wiley-com.myaccess.library.utoronto.ca/doi/full/10.1046/j.0902-0063.2003.00051.x?sid=nlm%3Apubmed
Sauthier et al. 2023,Automated screening of potential organ donors using a temporal machine learning model,"Organ donation is not meeting demand, and yet 30–60% of potential donors are potentially not identified. Current systems rely on manual identification and referral to an Organ Donation Organization (ODO). We hypothesized that developing an automated screening system based on machine learning could reduce the proportion of missed potentially eligible organ donors. Using routine clinical data and laboratory time-series, we retrospectively developed and tested a neural network model to automatically identify potential organ donors. We first trained a convolutive autoencoder that learned from the longitudinal changes of over 100 types of laboratory results. We then added a deep neural network classifier. This model was compared to a simpler logistic regression model. We observed an AUROC of 0.966 (CI 0.949–0.981) for the neural network and 0.940 (0.908–0.969) for the logistic regression model. At a prespecified cutoff, sensitivity and specificity were similar between both models at 84% and 93%. Accuracy of the neural network model was robust across donor subgroups and remained stable in a prospective simulation, while the logistic regression model performance declined when applied to rarer subgroups and in the prospective simulation. Our findings support using machine learning models to help with the identification of potential organ donors using routinely collected clinical and laboratory data.
","We proposed an innovative approach to the problem of organ donor identification. We were able to develop and internally validate a NN model to detect potential organ donors based on routinely collected clinical and laboratory data. We used EHR data with minimal pre-processing and minimal human intervention. We focused our effort on laboratory analyses, which are impartial, standardized, and accessible electronically, even in hospitals without EHRs.

This model is the first evidence supporting the use of real-world data to help screen for potential organ donors. To our knowledge, there is only one related model published34. This model was designed to identify catastrophic neurologic events using specific keyword identification on head CT scan reports. Given that most organ donors suffered a catastrophic neurologic event, this model could potentially be utilized to identify potential organ donors. It reported 77% sensitivity and 66% specificity. This approach required the scans to be interpreted, dictated, and transcribed by a radiologist, and thus required human intervention.

The more complex temporal model using NN marginally outperformed the non-temporal simpler version (LM). We compared the models with a bootstrap approach on a separate test set, instead of a cross-validation. Even if both approaches are similar in performance35, this enabled us to estimate the distribution of the results and apply statistical testing. However, it reduced the amount of data on which both models were compared and doesn’t replace an external validation dataset. The NN seemed to keep good discrimination in patients with more complex clinical patterns. It seemed to outperform the simpler LM when simulating a prospective identification of donors up to 48 h before the time of final donor classification. This could be explained by the added value of the clinical temporal evolution and by the fact that the NN had access to an embedded temporal vector of multiple laboratory data points, while the LM model only had access to the laboratory data measured at the latest time point before the outcome. Nonetheless, further work is needed to improve the model and to reduce the false positive rate. Since a lot of the false positives of the model were not eligible because of known neoplasia, that information could be in the future be used to update the models and improve their performance.

In subgroup analyses, we observed that our NN model performed better on donors that were also identified by the clinicians. These subpopulations represent the largest donor subtype making it likely that the model learned mostly from this subtype. Also, those subtypes may be more clinically distinct with more stable laboratory values, requested because of their donor status, making them easier to detect. Our model was also able to detect a significant proportion of potential organ donors that were missed or not referred by the clinical teams. Although the performance of the model was slightly lower in this group, our findings are of significant clinical interest, since those patients were missed by clinicians and did not have the opportunity to be assessed for donation. Since the detection of even one additional patient is of clinical benefit, we believe that if externally validated, such models could help support clinicians in the screening of potential organ donors. Interestingly, when we conducted a review of the classification errors of the models, the NN model detected two patients that have been missed by both the manual death audit and by the clinicians, potentially suggesting a higher sensitivity than the manual death audit alone.

Our study has a few limitations. First, our design required that a proportion of the data be used to train the autoencoder, reducing the amount of data available for the development of the classifier. Our approach has other advantages such as the possibility of easily merging multimedia information in future iterations of the model (radiology images, CT scans, vital signs, etc.). In addition, the design of our LM did not model the laboratory data evolution and restricted the absolute number of data point available, limiting comparisons of models and possibly explaining the slightly smaller discriminative property of the LM model. However, such limitation does not alter the absolute accuracy we measured of either model. Second, it is a retrospective study based on the data of a single quaternary transplant center, where clinicians are highly trained in the detection of potential organ donors. Truly missed organ donors are rare. In most of the non-referred patients, we found that organ donation was considered by the clinician and the option was not pursued, often because of family refusal. However, those patients still have a clinical pattern resembling a truly missed potential organ donor. It is unknown how the accuracy of our model would translate in a true, unsimulated, prospective setting, or in a different institution, and will thus require external and prospective validation before being considered for clinical use. Finally, our model requires at least 16 h of temporal data, and as such, does not apply to patients with catastrophic neurological events in whom a rapid decision would be required on a shorter time frame (e.g. in the emergency room). A decision for those patients will still need to be made by the clinician. Alternatively, those patients could benefit from an observation period in the ICU for better neurologic prognostication, as recommended by some experts, where our model would apply36,37.

In conclusion, we demonstrated the performance of two models identifying potential organ donors leveraging routinely collected clinical data. The more temporal NN model demonstrated slightly better and more stable performance. The models identified some patients that were not detected by the medical teams and manual death audits. Further work is required to validate the models externally and prospectively, and to further improve their prediction accuracy.",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10212939/
Barah & Mehrotra 2021,Predicting Kidney Discard Using Machine Learning,"Background
Despite the kidney supply shortage, 18%-20% of deceased donor kidneys are discarded annually in the United States. In 2018, 3569 kidneys were discarded.
Methods
We compared machine learning (ML) techniques to identify kidneys at risk of discard at the time of match run and after biopsy and machine perfusion results become available. The cohort consisted of adult deceased donor kidneys donated between December 4, 2014, and July 1, 2019. The studied ML models included Random Forests (RF), Adaptive Boosting (AdaBoost), Neural Networks (NNet), Support Vector Machines (SVM), and K-nearest Neighbors (KNN). In addition, a Logistic Regression (LR) model was fitted and used for comparison with the ML models' performance.
Results
RF outperformed other ML models. Of 8036 discarded kidneys in the test dataset, LR correctly classified 3422 kidneys, whereas RF correctly classified 4762 kidneys (area under the receiver operative curve [AUC]: 0.85 versus 0.888, and balanced accuracy: 0.681 versus 0.759). For the kidneys with kidney donor profile index of >85% (6079 total), RF significantly outperformed LR in classifying discard and transplant prediction (AUC: 0.814 versus 0.717, and balanced accuracy: 0.732 versus 0.657). More than 388 kidneys were correctly classified using RF. Including biopsy and machine perfusion variables improved the performance of LR and RF (LR's AUC: 0.888 and balanced accuracy: 0.74 versus RF's AUC: 0.904 and balanced accuracy: 0.775).
Conclusions
Kidneys that are at risk of discard can be more accurately identified using ML techniques such as RF.","Recall that KDPI is a key metric in the current KAS. Under KAS policy, kidneys with KDPI > 85% are offered to candidates who have written consent for accepting such kidneys. However, a high proportion of diseased kidneys with KDPI >85% (43.16%) were discarded as “No recipient located – list exhausted.” Kidneys with serum creatinine ≥ 1.5 mg/dL are at a greater risk of discard than kidneys with normal creatinine. In addition, our results indicate that kidneys with creatinine < 0.8 mg/dL are also at a greater risk of discard. Low creatinine values may mean the presence of chronic kidney disease or damage to the kidney because of some other underlying condition such as a muscle or liver disease present at the time of donation.

Predicting Kidney Discards at the time of Match-run in the UNet System
The performance of RF suggests that the model is better in predicting kidney discards not only at the time of match-run but also once the procurement biopsy and perfusion results are available. An OPO, possibly through discussions with transplant centers, makes decisions on performing a kidney biopsy or putting a kidney on machine perfusion. Obtaining biopsy results can add up to 5 hours of cold ischemia time to a donor kidney. The performance of RF model, developed using predictors available at the time of match-run, is nearly as good as the performance of the LR model with biopsy results. This suggests that in a policy that fast-tracks kidneys using a discard probability threshold, the cold time for obtaining biopsy results can be saved by using the RF model, without compromising on the performance of the system. Availability of biopsy results improve discard predictability further and can be incorporated in a secondary fast-tracking decision. RF and LR models are data-driven predictive models. These models are not designed for causal inferencing, thus should not be used to justify a kidney discard decision. The decision to transplant a kidney in a patient is clinical and it needs to account for other factors contributing to the overall well-being of a patient.

Advancements in the Hepatitis C Treatment and Discard Prediction
A review of results in our analysis using the rolling horizon approach showed that the importance of hepatitis C treatment on discard is less pronounced in recent years (i.e., 2017 and 2018), when compared to years 2015 and 2016, thanks to advancements in effective treatments for Hepatitis C.

Kidney Discard Models: Predictability vs Interpretability
Despite the fact that ML models outperform LR in discard predictability, the models do not have an easy interpretation. The interpretability issue of ML models, and specifically RF is because it considers linear and non-linear variable interactions extracted from data to improve model performance37. In our case, the RF model had 500 sub-trees, whose leaves had a predictive model specified for the variable value partition corresponding to that leaf. More interpretable ML models (e.g., KNN) did not perform as well as RF. Thus, there is a tradeoff between the interpretability of a model and the model’s performance. Simpler models (e.g., LR) may be used to develop insights into the effect of a variable on the outcome (discard). However, more complex and less interpretable models are more desirable in practice when these models need to be embedded within an algorithm that is intended to save every life possible through resources that could be wasted otherwise. We note, however, that further external or prospective validation is required for using proposed ML models in fast-tracking algorithms.

While comparisons based on discard probability thresholds provide estimates on the number of organs that will be correctly identified as potential discards, a more accurate estimate of using ML over KDPI in a fast-tracking policy on the number of lives saved or life years added to the current allocation system can be evaluated under a simulated environment. Such a simulation framework may consider additional clinician behavior related factors as part the allocation algorithm.

Note that the standard error for the 2-fold cross validation is greater than that of the 5-fold cross validation (see Figure 1). Additionally, the Receiver Operative Curve (ROC) of 2-fold cross validation is dominated by the 5-fold ROC. Thus, the finding that the RF model outperforms the LR model is even more significant if 5-fold cross validation was to be used.

Other ML Models and Additional Discussion on RF
Other ML methods such as boosted trees and bagged models were also developed but were not found to be as effective. We also developed more recently developed approaches such as Convoluted Neural Networks (CNN). CNN is much harder to train, and its performance was not better than that of RF and Adaboost. CNN is also known to have a tendency of overfitting the data38.

While the detailed results reported in this paper use KDPI, as indicated in the results section, the use of KDRI computed by using variable information available at the time of match-run performed equally well. Several variables (serum creatinine, age, hepatitis C) that are components of the KDRI formula also reappeared on the importance list of predictors for the RF model. It is difficult to interpret the additive effect of these variables on the discard in the framework of the RF model. It is understood that collinearity does not affect a decision tree-based prediction model39, of which RF is an example. Recall that RF grows classification decision trees in which the variables are randomly picked. The colinear variables do not simultaneously appear in a tree unless having them both in the tree improves classification based on an impurity measure, i.e., Gini or Entropy. The fact that KDPI component variables also appear as top predictors in the RF model suggests that there is an additional effect of these variables on discard beyond their contribution through KDPI. However, exploring KDPI component variables’ additional effects requires a study which is a topic for future research.

LIMITATIONS
The models only used the covariates that are available in the UNOS dataset. The analysis used donors’ terminal serum creatinine. Creatinine can sometimes change significantly over the course of time. Though the median absolute deviation of donors’ serum creatinine was in the set of predictors to capture the variability of serum creatinine over time, other models to capture this variability might also be used.",https://europepmc.org/article/med/33534531#S27
Markgraf & Malberg 2022,Preoperative Function Assessment of Ex Vivo Kidneys with Supervised Machine Learning Based on Blood and Urine Markers Measured during Normothermic Machine Perfusion,"Establishing an objective quality assessment of an organ prior to transplantation can help prevent unnecessary discard of the organ and reduce the probability of functional failure. In this regard, normothermic machine perfusion (NMP) offers new possibilities for organ evaluation. However, to date, few studies have addressed the identification of markers and analytical tools to determine graft quality. In this study, function and injury markers were measured in blood and urine during NMP of 26 porcine kidneys and correlated with ex vivo inulin clearance behavior. Significant differentiation of kidneys according to their function could be achieved by oxygen consumption, oxygen delivery, renal blood flow, arterial pressure, intrarenal resistance, kidney temperature, relative urea concentration, and urine production. In addition, classifications were accomplished with supervised learning methods and histological analysis to predict renal function ex vivo. Classificators (support vector machines, k-nearest-neighbor, logistic regression and naive bayes) based on relevant markers in urine and blood achieved 75% and 83% accuracy in the validation and test set, respectively. A correlation between histological damage and function could not be detected. The measurement of blood and urine markers provides information of preoperative renal quality, which can used in future to establish an objective quality assessment.","The present work is the first to classify normothermically perfused kidneys according to their ex vivo functional status using supervised machine learning algorithms based on blood and urine markers. Significant differentiation of kidneys according to their function could be achieved by eight-V˙O2
, DO2, RBF, AP, IRR, TKidney, relative urea concentration and urine production-of the eleven function and injury markers studied. However, based on supervised learning methods and using only the markers RBF, AP, Tkidney, V˙O2
, urine production and relative urea concentration, the classification between functional kidneys from kidneys with impaired function could be ensured. These methods can thus form the basis for the development of new diagnostic tools for the non-invasive assessment of organs.
4.1. Function Markers for Metabolic Activity
Of the four function markers of metabolic activity considered, V˙O2
 and DO2, but not ERO2 and CO2P, varied significantly with renal function (see Figure 3).
The determination of V˙O2
 has been performed in numerous preclinical and clinical NMP studies, which have focused on therapeutic strategies [35,36,37,38,39,40,41], the ideal NMP experimental setup [42,43,44,45,46,47], or the performance of NMP compared to other preservation methods [48,49,50], among others. In these studies, either Fick’s principle (e.g., [35,42]) or the simplified form, the arteriovenous pO2 difference without consideration of hemoglobin concentration (e.g., [37,39]), served as the basis for V˙O2
 calculation.
A correlation between the V˙O2
 and the QAS was previously investigated in three studies [16,51,52]. The level of V˙O2
 was significantly higher in kidneys that were candidates for transplantation (score 1–2) than in kidneys that were no longer suitable as grafts (score 3–5) [16,51]. Moreover, this parameter was associated with a low donor creatinine level before organ retrieval [16]. One study also investigated the dependence of ERO2 on QAS. During NMP, a higher ERO2 value was associated with a higher QAS value [16].
The parameters DO2 and CO2P, which were also investigated in the present work, have not yet been used for functional assessment in other studies. In particular, DO2 showed promising results (see Figure 3a) and should be investigated in future work.
Not only the dependencies between function markers of metabolic activity and renal quality determined in the present work, but also the magnitude of their measured values are supported by the literature. The measured values for V˙O2
, DO2, and ERO2 already published in the literature could be confirmed. Kidneys perfused with autologous whole blood for three hours after 10 min WIT and 17 h CIT, and thus conditioned similarly to functional class 3 kidneys, showed a V˙O2
 = (2.0 ± 0.8) mL/min/100 g, an ERO2 = 60% ± 2%, and a DO2 = (4.4 ± 2.4) mL/min/100 g [42]. In comparison, this work demonstrated for functional kidneys a V˙O2
 = (2.6 ± 0.3) mL/min/100 g, an ERO2 = 52 ± 2 %, and a DO2 = (4.4 ± 0.4) mL/min/100 g.
4.2. Function Markers for Renal Hemodynamics
The function markers RBF, AP, IRR, and TKidney showed significant differences between functional and limited functional or nonfunctional kidneys (see Figure 4). In the context of HMP, the predictive value of hemodynamic data for organ outcome has been part of intensive investigations. In a large randomized controlled trial, IRR at the end of HMP was shown to be an independent risk factor for both delayed graft function and 1-year graft failure [53]. No specific correlations could be found for the markers RBF and AP.
In studies with research focus on NMP, the hemodynamic perfusion markers RBF, AP, and IRR were commonly measured. In a large animal study in which kidneys were preserved during eight hours of NMP, kidneys not exposed to WIT had the lowest IRR, followed by kidneys exposed to 30 and 60 min of WIT [17]. Furthermore, IRR at the beginning of NMP correlated with postoperative renal function [17].
Relationships between the hemodynamic parameter RBF and WIT were determined in a preclinical study. RBF after 60 min of NMP was (47.2 ± 6.5) mL/min/100 g (7 min WIT), (33.2 ± 5.6) mL/min/100 g (15 min WIT), (31.5 ± 6.0) mL/min/100 g (25 min WIT), and (25.6 ± 4.7) mL/min/100 g (40 min WIT) and remained nearly constant during reperfusion up to six hours for all WIT [54]. Nevertheless, RBF was significantly decreased with 40 min warm ischemia compared with seven minutes warm ischemia after one- and six-hours reperfusion [54].
The dependence of WIT on renal hemodynamics and thus on functional impairment induced is confirmed in this work. The only currently existing scoring system for the assessment of renal quality, the QAS, includes the RBF as one of the three assessment criteria [20]. A large multicenter randomized controlled trial is currently underway to confirm the results between the QAS and renal quality [55].
In this study, we did not work with the commonly used arterial pressure of AP = 75 mmHg but set pressures of 100–110 mmHg for kidneys in which AP adaptation was possible [45,56]. There are two reasons for this. First, AP of >75 mmHg in whole blood perfused kidneys showed better overall renal function [57]. Second, physiological AP > 75 mmHg for pigs of different body weights have been reported in the literature [58].
With the AP chosen (100–110 mmHg), it was possible to ensure an NMP of functional kidneys [23]. However, we are aware that other arterial pressures will have to be tested in subsequent studies to ensure optimal perfusion of ex vivo kidneys.
4.3. Function Markers for Renal Filtration
The function markers of renal filtration investigated in this work were urine output and blood urea concentration. Both markers varied significantly between functional and limited functional/nonfunctional kidneys during NMP (see Figure 5).
Urine production is a criterion of QAS [20]. In addition, a preclinical study determined the dependence between WIT and urine production. Kidneys with a 40 min WIT had significantly lower urine output than kidneys with a 10 min WIT [59].
Blood urea concentration is a retention marker for kidneys and has not been determined in any NMP study to date. However, blood urea nitrogen derived from blood urea concentration was analyzed postoperatively after eight hours of NMP by Selzner et al. [17]. The study showed that the nitrogen content of urea in blood differed significantly between kidneys with WIT of 0 min, 30 min, and 60 min [17].
4.4. Injury Marker for Tubular Impairment
GST was found to be significantly associated with allograft outcomes in HMP studies. Associations with GFR and primary nonfunction were found in most prospective studies. [60]
To date, GST has been investigated in only one other study of NMP. Here, however, the isoenzyme GSTA1 was measured in the serum of patients after transplantation of a previously normothermically perfused kidney [61]. Thus, a comparison with the results collected in this paper is not possible. Tubular damage was higher in kidneys assigned as nonfunctional. However, no significant relationship with renal function was observed (see Figure 5).
4.5. Functional Classification Based on Blood and Urine Markers
In the present work, inulin clearance, the clinical gold standard for the assessment of renal function, is used as the basis for classification. Function markers of metabolic activity (V˙O2
), renal hemodynamics (RBF, AP, TKidney) and renal filtration (urine production, relative urea concentration) could be identified as suitable markers that correlate with inulin clearance and thus allow a functional classification of the kidneys (see Supplementary Material Table S3/S4: Functional classification based on blood and urine markers). It should be emphasized that an NMP of three hours is decisive for the assessment of the organs. Blood and urine markers used as input for the supervised learning method algorithms have been used in other NMP studies for the assessment of renal quality [16,17,18,19,20,21,22]. The novelty of our study is the classification of kidneys using algorithms of supervised learning methods based on these markers with ex vivo inulin clearance as reference parameter. Using the SVM, kNN, LOG, and BAY classifiers, correct prediction of function was achieved for 15 of 20 kidneys studied in the training dataset and for 5 of 6 kidneys in the test dataset (see Figure 6, Table 1). However, none of the classifiers could adequately classify nonfunctional kidneys. A comparable study is not known in the literature.
Working groups that focused on organ evaluation in NMP postulate that only a combination of different markers can provide an appropriate quality assessment of kidneys [14,15,16,17]. For their part, these research groups are attempting a functional assessment based on blood and urine markers. This statement can be confirmed by the results shown in this work. No single blood or urine marker could realize a functional classification.
However, a recent study developed a novel evaluation strategy based solely on VIS/NIR spectroscopic tissue properties that achieved better classification results for the dataset that was also used in this study [24]. Using HSI in combination with an appropriate convolutional neural network model, it was possible to predict renal quality during NMP and to classify kidneys into three functional classes [24].
Classification based on urine and blood markers offers potential for optimization. In addition to optimization of supervised learning methods, expanding the dataset is a possible approach for further research.
Optimization of the classifiers used in this work (SVM, LOG, RF, kNN and BAY) might improve the classification result. The supervised learning methods were implemented so that they can be used for ex vivo kidney evaluation. However, a systematic adaption of the classifiers by variation of the hyperparameters with comparison of the classification results was not carried out and should be the focus of further research. In addition, the combination of several machine learning models can also be explored to improve the classification results.
Larger datasets might help classifiers to better generalize to new data. Due to the small amount of data in the present work and the resulting limited number of training (20 kidneys) and test data (six kidneys), the classification quality of the models may be strongly influenced by the effect of outliers. Thus, the selection of the training and test dataset significantly determines the classification result. In addition, more data from nonfunctional kidneys can support classifier modeling to ensure future classification into three classes.
Moreover, in further studies, kidneys treated with NMP should also be re-transplanted after preservation. Thus, it could be determined whether inulin clearance, which was used as a reference for ex vivo renal function in this work, correlates with postoperative in vivo function. If it becomes apparent that inulin clearance is not suitable as a gold standard for ex vivo function assessment, the supervised learning methods presented here could be trained on other reference markers (e.g., histological markers). Thus, the approach presented here is highly adaptable depending on which reference markers are used for renal quality assessment.
4.6. Classification Based on the Preoperative Remuzzi Score
There are different approaches for histopathological evaluation of tissue biopsies in NMP experiments. A unified, standardized evaluation of biopsies to achieve comparability between different research groups has not yet been established. There are research groups that use the Remuzzi score or a modified Remuzzi score [21,42,62]. Other research groups base their histologic evaluation on very different biopsy parameters such as tubular injury, osmotic nephropathy, and hypokalemic nephropathy [34], or glomerular dilatation, tubular dilatation, and tubular necrosis [63].
Because there are a variety of scores for tissue biopsies of kidneys during NMP, we decided to use the Remuzzi score for our evaluation. To date, this score is the only one that has been compared to QAS in evaluating the condition of ex vivo kidneys during NMP [16].
In our study, a correlation between ex vivo function and histological damage based on the overall Remuzzi score or on the tubular atrophy parameter alone could not be detected (see Supplementary Material Figure S1: Classification based on the preoperative Remuzzi score; Table S5: Classification based on the tubular atrophy). However, it must be taken into account that the biopsies were taken from only one site of the organ, as is also common in transplantation. This therefore only allows a statement to be made about this small area of the kidneys and does not guarantee an overall assessment.
Because this is the first time that ex vivo renal function during the preservation phase has been correlated with renal histology, the results cannot be compared with the literature. However, it is well known that the use of histologic features for renal transplant acceptance and allocation criteria has limitations [5,6]. Firstly, these do not allow conclusions to be drawn about the prediction of graft survival. Secondly, histopathologic findings are based on subjective judgment, which leads to variability between pathologic results and thus limits the reliability of this analytical tool for predicting transplant outcome [5]. Objective methods such as those developed in this work could be used in the future to assess renal quality in addition to histological assessment.",https://www.mdpi.com/2227-9059/10/12/3055
Marrero et al. 2021,A machine learning approach for the prediction of overall deceased donor organ yield,"Background
Optimizing organ yield (number of organs transplanted per donor) is a potentially modifiable way to increase the number of organs available for transplant. Models to predict the expected deceased donor organ yield have been developed based on ordinary least squares regression and logistic regression. However, alternative modeling methodologies incorporating machine learning may have superior performance compared with conventional approaches.

Methods
We evaluated the predictive accuracy of 14 machine learning models for predicting overall organ yield in a cross-validation procedure. The models were parameterized using data from the Organ Procurement and Transplantation Network database from 2000 to 2018. The inclusion criteria for the study were adult deceased donors between 18 and 84 years of age that had at least 1 organ procured for transplantation.

Results
A total of 89,520 donors met the inclusion criteria. Their mean (standard deviation) age was 44 (15) years, and approximately 58% were male. Our cross-validation analysis showed that a tree-based gradient boosting model outperformed the remaining 13 models. Compared with the currently used prediction models, the gradient boosting model improves prediction accuracy by reducing the mean absolute error between 3 and 11 organs per 100 donors.

Conclusion
Our analysis demonstrated that the gradient boosting methodology had the best performance in predicting overall deceased donor organ yield and can potentially serve as an aid to assess organ procurement organization performance.","A model to accurately predict overall deceased donor organ yield can serve as an aid to assess organ procurement performance. Previous studies have focused on modeling the overall organ yield with generalized linear models and splines. While these conventional approaches have good predictive performance, we found that alternative methods can achieve higher accuracy.

Our study evaluated 14 statistical models, and tree-based gradient boosting achieved the best overall performance according to the MAE and MSE. Tree-based methods model the association between the predictors and overall organ yield without making any assumptions of their underlying relationship. Besides easily integrating nonlinear relationships, tree-based methods can implicitly capture interactions among predictors. The tree-based gradient boosting algorithm reduces prediction error by averaging over the predictions of many sequentially built trees. If calibrated properly, this modeling methodology has been previously shown to have excellent performance.20,21

Since the non-linear machine learning algorithms, such as gradient boosting, often exhibit high variability in their predictions and require large amounts of data, our main analysis included data from 2000 to 2018. We observed that the variability of most models increased in our 2-year cohort sensitivity analysis. Nevertheless, gradient boosting still outperformed the remainder of the models. By evaluating the performance of gradient boosting and the models previously published in the literature in different eras of our dataset, we noted that data from the more recent era (eg, the 2010–2018 cohort) leads to higher predictive accuracy. However, training machine learning models only on the most recent data may result in reduced capacity to generalize to previously unseen data. In general, the more data the algorithms use during the learning process, the more reliable their predictions become.

The 2 most impactful factors on organ yield in the gradient boosting model were KDRI, which is used commonly in kidney donor evaluation, and lung pO2, which could be a proxy for the cardiopulmonary status of the donor. These predictors were also highly influential in the organ-specific regressions of the adjusted SRTR model. The remainder of the factors, including OPTN region, impacted performance to a lesser extent.

A considerable strength of our work is the examination of the predictive ability of conventional and novel machine learning models in a large dataset containing over 89,000 donors across 19 years. Another strong point of the present study is the use of an innovative approach to predict overall deceased donor organ yield.

Although our study has several strengths, it also contains some limitations. The major shortcoming of this study is the missingness of data inherent to the OPTN database. However, we showed that our results were robust in our sensitivity analysis after imputing missing data. Additionally, we were only able to analyze data collected in the OPTN database. There may be other uncollected donor and transplant-center-specific information that may impact overall organ yield.

Modeling the overall organ yield as counts also presents technical limitations. Though there is only a finite number of organs that can be transplanted per donor, a count model has no theoretical upper bound. However, as our response variable is a non-negative integer, a count model represents overall organ yield better than classification or continuous regression models. Another reasonable alternative is to model overall organ yield as an ordinal variable. Nevertheless, this approach would not allow us to model the magnitude of the difference between the number of organs transplanted per donor.

Another notable limitation of our analysis is that our models were not built to predict organ-specific yield. This was not the objective of our analysis, as the organ-specific yield models developed by the SRTR are updated frequently, making contemporary comparisons difficult.8 The summation of individual organ yield predictions may introduce additional error in estimating the overall yield. This is a potential reason why the direct application of the SRTR model was outperformed by multiple parametric and nonparametric models in the 2-year cohort analysis. Through this analysis, we contribute to the efforts to refine the prediction of expected organ yield and develop more accurate methods to evaluate organ procurement performance. Our study has highlighted the importance of predicting overall yield in the aggregated scale to avoid unnecessary variability and obtain insights into the relationship with its predictors.

An additional limitation of our study is the variability in acceptance patterns of transplant centers. Studies have found that heart and lung acceptance rates vary markedly among transplant centers.22,23 Similar results have also been observed for kidney and liver acceptance rates.24,25 Acceptance patterns and biases of transplant centers are implicitly recorded in the OPTN database. Expected organ yield models trained on historical practice can perpetuate or even reinforce any biases contained in the data.

Future studies aimed at evaluating novel models in individual organ yield prediction may be warranted based on the results of this analysis. Extensions of our work could include the evaluation of other machine learning models, such as deep neural networks and support vector machine models, as well as different encoding mechanisms for categorical variables. Additional future work may also evaluate the predictive accuracy of the models with other performance metrics, such as the mean absolute percentage error and the out-of-sample R-squared. The predictors identified in our work could also be used to develop more accurate models for deceased donor organ yield. Lastly, alternative approaches could be used to design expected organ yield models.

Practical implications on overall organ yield
We showed that model performance has practical implications. Better model performance achieves higher accuracy in predicting organ yield per 100 donors at the individual and aggregated level. Accurate predictive ability is critical when evaluating performance in donor conversion and organ yield. Identifying outliers in deceased donor organ yield and improving the achievement of donor management goals are necessary steps toward increasing the number of organs available for transplantation.26 The increasing availability of novel technologies, such as organ machine perfusion, will likely substantially impact organ yield models in the future.27

To evaluate OPO performance, our model could be used along with the organ-specific models developed by the SRTR. Our gradient boosting model could be used to predict the expected overall organ yield instead of the aggregate of the SRTR models. Similar to the SRTR models, our overall yield model could be implemented as a web-based application. The relative influence of each predictor on the expected organ yield could be measured using variable importance methods. In addition, interactive partial dependence plots could be included in the web application to display the marginal effect of each predictor on the expected number of organs transplanted per donor and their functional relationships. Rather than providing model coefficients to allow the implementation of our model, the code of our model could be made available online. Alternatively, users could choose to run the model on the web application and obtain predictions based on specific donor characteristics.

In conclusion, We showed that gradient boosting is a novel modeling technique that can accurately predict overall deceased donor organ yield and outperforms existing models. Improving the accuracy of donor yield production could allow for a better evaluation of OPO performance in donor conversion. Improving OPO performance is key to mitigating the organ shortage and ultimately reduce morbidity and mortality in patients awaiting organ transplantation.",https://www-sciencedirect-com.myaccess.library.utoronto.ca/science/article/pii/S0039606021005456?via%3Dihub#abs0010
Guijo-Rubio et al. 2021,Statistical methods versus machine learning techniques for donor-recipient matching in liver transplantation,"Donor-Recipient (D-R) matching is one of the main challenges to be fulfilled nowadays. Due to the increasing number of recipients and the small amount of donors in liver transplantation, the allocation method is crucial. In this paper, to establish a fair comparison, the United Network for Organ Sharing database was used with 4 different end-points (3 months, and 1, 2 and 5 years), with a total of 39, 189 D-R pairs and 28 donor and recipient variables. Modelling techniques were divided into two groups: 1) classical statistical methods, including Logistic Regression (LR) and Naïve Bayes (NB), and 2) standard machine learning techniques, including Multilayer Perceptron (MLP), Random Forest (RF), Gradient Boosting (GB) or Support Vector Machines (SVM), among others. The methods were compared with standard scores, MELD, SOFT and BAR. For the 5-years end-point, LR (AUC = 0.654) outperformed several machine learning techniques, such as MLP (AUC = 0.599), GB (AUC = 0.600), SVM (AUC = 0.624) or RF (AUC = 0.644), among others. Moreover, LR also outperformed standard scores. The same pattern was reproduced for the others 3 end-points. Complex machine learning methods were not able to improve the performance of liver allocation, probably due to the implicit limitations associated to the collection process of the database.","To our knowledge, this is the first work that addresses Donor-Recipient (D-R) matching in Liver Transplantation (LT) using the UNOS data set. D-R matching has become one of the most challenging topics in LT in the last years. Unfortunately, standard scorers, such as MELD, SOFT or BAR, fail to consider both mortality in waiting list and benefit survival. These two objectives are difficult to meet, since these metrics pose them as conflicting objectives. A decrease in mortality in the waiting list leads in many cases to worse post-transplant survival results; and, vice versa, obtaining better results may affect the opportunity to be transplanted for the sickest one on the waiting list.

D-R matching is considered as a classification problem, and, for this, variables of the donor, variables of the listed recipients and surgical and logistical aspects are considered to assess the best matching possible [4], which can be based on the survival of the graft, the survival of the recipient, or both. A common problem of the available scores is a remarkably basic statistical methodology that only considers isolated variables with single random graft or patient survival end-points. The combination of several variables and end-points in the setting of artificial intelligence-based decisions that avoid human-guided bias may be the basis for D-R matching and grafts allocation in the future. In 2014, we tested Artificial Neural Networks (ANNs) in the complex scenario of D-R matching with D-R pairs from 11 Spanish transplant units [15]. This study demonstrated that ANNs are a valuable tool for organ allocation to obtain the best benefit of survival. In the current scenario of graft scarcity and waiting list deaths, the absence of a definitive and objective system for liver-donor assignment is unacceptable. After that, we validated ANN methodology in D-R matching in a different health care system (data from King’s College Hospital, KCH), showing that it would be a powerful tool for D-R matching in comparison to other current models [21]. This methodology has been recently validated using gradient boosting and random forest classifiers [22] using data from 272 different centres, denoting that outstanding results could be obtained independently of the population location.

The main goal of this paper was to analyse the behaviour of machine learning techniques applied to the largest liver transplant database, provided by the UNOS [5]. Working with large databases is a great opportunity to achieve a worldwide application of machine learning techniques in the results of LT.

Machine learning methods lead in general to excellent results when combined with a huge amount of information. As an example, Electronic Health Records (EHRs) have been developed to speed up the mechanism for clinician decision making, based on information extracted from these records [23]. However, it has been demonstrated that, for large databases, machine learning algorithms are not always capable of reaching notable results, what can be caused for several reasons [13, 24, 25]: 1) missing values and the imputation techniques used, in combination with the need of clear guidelines regarding how to cope with attributes and patterns with different percentages of missing data, 2) the increasing quantity of different categories for some attributes, which makes the classifier lose accuracy, as well as, 3) the increasing number of Non Specified (NS) cases in this attributes, where some specific information is discarded, since no category matches the particular situation, 4) by contrast, attributes with several categories but a small number of cases per category, make null contribution, and finally, 5) the vast amount of subjective attributes manually introduced may cause incongruities between different expert opinions.

The results we have obtained in the present study include much of the problems described previously. Indeed, most AUCs are over 0.600, being 0.654 in average for LR, the best performance model. These results contrast with those obtained in previous models form the Spanish data set and its subsequent validation in the King’s college Hospital dataset. However, in a similar study made with the UNOS dataset in heart transplantation, Miller et al. [13] have found a lack of improvement of advanced analytic algorithms, as we have described in the present study, concluding that prognostic abilities of machine learning techniques may be limited by quality of the clinical dataset. More recent studies [25, 26] have also demonstrated no evidence of performance benefit for machine learning methods over logistic regression.

In the dataset considered in the present study, a vast amount of missing data were found. Not all the regional centres give the same importance to the data collection step for the database. Hence, the curation of the database is a tedious procedure due to the large percentage of missing values. The imputation of data makes the database lose veracity and robustness, leading to worse performances because of data granularity and quality. Furthermore, the entries of the database are collected by 11 regional centres. This process lacks from consistency, caused by diverse reasons: 1) the administrative centres provides their original data to a global database, being possible to give different formats to the variables from the other centres. 2) In addition, a given variable could be obtained following different procedures or index measures. 3) Finally, the database may include incongruities because a given situation could be categorised contradictorily. The inclusion of the cold ischemia time has arisen much controversy in the literature [27, 28]. Although it has been considered an important variable, it is a post-transplant variable, for which a priori information is not known. Estimating the cold ischemia time to predict donor-recipient matching is a challenging task, because this estimation has to be done with no prior information but the cities of origin of both patients (which in the case of USA is arduous, given the large distances between the hospital of different states) and the strategy to allocate organs between all the regional centres.

Finally, it should be discussed the trajectory of different scores considered along the years [29, 30]. The D-R matching has been performed following the guidelines proposed by scores with different goals published in the literature. With the exception of the LR model, the results obtained in the present study did not differ in essence from those obtained with the classic scores (MELD, BAR, SOFT, etc.) based on conventional biostatistics. This does not mean a lack of usefulness of artificial intelligence in the problem of D-R matching in liver transplantation, but the importance of emphasizing the need for well-designed and well-constructed databases, and, of course, filled with diligence and professionalism.

The main goal of machine learning is to provide the medical community with a tool bridging the gap between the medical decision (subjectivity) and strict mathematical scores (objectivity). For this purpose, a rule-based system is proposed for the management of the waiting list for liver transplant. This system is objective (does not include human subjectivity in the selection of the recipient), optimal (it is able to increase the post-transplant survival rates) and, finally, fair, because, if the model does not appreciate a significant difference between two recipients, the organ is allocated to the recipient with the most advanced disease (highest MELD). A deep analysis have been done to increase the most the understanding of the mathematical model and its consistency with the medical findings so far.",https://www-ncbi-nlm-nih-gov.myaccess.library.utoronto.ca/pmc/articles/PMC8139468/
Khosravi et al. 2015,Five Years Survival of Patients After Liver Transplantation and Its Effective Factors by Neural Network and Cox Poroportional Hazard Regression Models,"Background:
Transplantation is the
Transplantation is the only treatment for patients with liver failure. Since the therapy imposes high expenses to the patients and community, identification of effective factors on survival of such patients after transplantation is valuable.

Objectives:
The current study attempted to model the survival of patients (two years old and above) after liver transplantation using neural network and Cox Proportional Hazards (Cox PH) regression models. The event is defined as death due to complications of liver transplantation.

Patients and Methods:
In a historical cohort study, the clinical findings of 1168 patients who underwent liver transplant surgery (from March 2008 to march 2013) at Shiraz Namazee Hospital Organ Transplantation Center, Shiraz, Southern Iran, were used. To model the one to five years survival of such patients, Cox PH regression model accompanied by three layers feed forward artificial neural network (ANN) method were applied on data separately and their prediction accuracy was compared using the area under the receiver operating characteristic curve (ROC). Furthermore, Kaplan-Meier method was used to estimate the survival probabilities in different years.

Results:
The estimated survival probability of one to five years for the patients were 91%, 89%, 85%, 84%, and 83%, respectively. The areas under the ROC were 86.4% and 80.7% for ANN and Cox PH models, respectively. In addition, the accuracy of prediction rate for ANN and Cox PH methods was equally 92.73%.

Conclusions:
The present study detected more accurate results for ANN method compared to those of Cox PH model to analyze the survival of patients with liver transplantation. Furthermore, the order of effective factors in patients’ survival after transplantation was clinically more acceptable. The large dataset with a few missing data was the advantage of this study, the fact which makes the results more reliable.","Liver transplantation is the only treatment for patients with liver failure (5). Without liver transplantation the patients do not have any chance for prolonged survival. During the last two decades the five-year survival of liver transplant patients increased (34). Liver transplant program was established since 20 years ago in Shiraz Namazee Hospital and it is well developed; therefore, more than six hundred liver transplants are done annually. With the increasing number of patients with liver transplant in Iran, the follow up is very important. Indeed, the important features which affect the survival of patients after the surgery is valuable for pre and post-operative cares. But so far, few rigorous studies are conducted on survival in patients with liver transplant (5-10, 29, 30).

Since this phenomena is affected by the regional status, diet or cultural traditions in life, conducting a comprehensive study which encompasses all age groups is needed for the Iranian population. However, some previous studies were conducted on survival of patients with liver transplantation in Iran which utilized classical methods for analysis (5, 10) with the limited age range.

The current study aimed to model the survival of patients with liver transplant in a wide age range (two years old and above) using ANN and Cox PH regression models in order to compare the performance of these two methods to predict death due to complications of liver transplantation. Accordingly, some variables that influenced survival of the patients with liver transplant were selected based on a few studies conducted on survival of such patients and also the experience of more than two thousand liver transplant surgeries in Shiraz transplant center.

The results of the current study revealed that ANN was better than the Cox PH regression model to predict survival in patients with liver transplant based on the area under the ROC curve (Table 4). However, both of them were large enough to be statistically significant (P < 0.0001). In addition, the prediction rate accuracy was similar in both models (Table 4). Furthermore, the Youden index J, sensitivity and negative predictive values were in favor of ANN while specificity and positive predictive values were higher in Cox PH model. However, the significance of the input variables order should be considered, clinically (Tables 5 and ​and6).6). Based on the clinical experience, the order of variables in Cox PH model may be more consistent with clinical findings. Although, the recipient age can be an important variable but Primary non function (PNF), lung, kidney and vascular complications have more important effect on the patients’ survival. According to the ANN results in Tables 5 and ​and6,6, some variables such as Cold ischemia time (CIT), previous abdominal surgery or transfusion of fresh frozen plasma (FFP), are at the top of the list while clinically they do not seem to be as important as PNF or vascular complications. However, many studies compared these methods for survival analysis in various diseases worldwide (21, 27). All these studies mentioned the superiority of ANN over Cox PH regression in real clinical datasets. In addition, the comparison between these two methods on a simulated dataset confirmed the high ability of ANN method in modeling complex relations compared to that of Cox PH regression model especially for a dataset with high censorship (35).

Generally, comparing these two models, Cox PH model needs to fulfill some theoretical assumptions on data structure. In addition, it uses a subset of variables in the final model (the significant ones). Therefore, its results are easy to interpret and the odds ratio and related confidence intervals can be calculated. In comparison, ANN requires a large data set to learn the relations. In addition, it uses all input variables in modeling process and the absolute value of their weights indicates the importance. Therefore, it cannot distinguish the confounding variables (inconsequential ones) but is a powerful tool to find the complex patterns among the inputs without any assumptions for data structure.

The strength of this study was the possibility of comparison between the two methods in survival analysis of liver transplantation data. In addition, investigating the role of many different factors in survival of patients with liver transplantation, simultaneously among a wide age range and in a large data set, was another advantage of the present research. However, the current study had a potential limitation. Incomplete registration information in the hospital records of patients was problematic. Furthermore, some patients were not available to record their final status (dead/alive). This fact led to lose some subjects. Therefore, the improvement of hospital registration information may be necessary. Moreover, examining other learning algorithms for ANN method or utilizing the hybrid methods between ANN and genetic algorithm are suggested for future studies.

Go to:",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4612564/
Kazemi et al. 2019,Identifying Factors That Affect Patient Survival After Orthotopic Liver Transplant Using Machine-LearningTechniques,"Objectives: Survival after liver transplant depends onpretransplant,  peritransplant,  and  posttransplantfactors. Identifying effective factors for patient survivalafter  transplant  can  help  transplant  centers  makebetter decisions.Materials and Methods: Our study included 902 adultswho received livers from deceased donors from March2011 to March 2014 at  the  Shiraz  Organ TransplantCenter  (Shiraz,  Iran).  In  a  3-step  feature  selectionmethod, effective features of  6-month survival wereextracted by  (1)  F statistics, Pearson chi-square, andlikelihood  ratio  chi-square  and  by  (2)  5  machine-learning techniques. To evaluate the performance ofthe machine-learning techniques, Cox regression wasapplied to the data set. Evaluations were based on thearea under the receiver operating characteristic curveand sensitivity of  models. (3) We also constructed amodel using all factors identified in the previous step.Results: The  model predicted  survival  based  on  26identified effective factors. In the following order, graftfailure, Aspergillus infection, acute  renal failure andvascular complications after transplant, as well as graftfailure diagnosis interval, previous diabetes mellitus,Model  for  End-Stage  Liver  Disease  score,  donorinotropic support, units of packed cell received, andprevious recipient dialysis, were found to be predictivefactors in patient survival. The area under the receiveroperating characteristic curve and model sensitivitywere 0.90 and 0.81, respectively.Conclusions: Data mining analyses can  help identifyeffective  features  of  patient  survival  after  livertransplant  and  build  models  with  equal  or  higherperformance  than  Cox  regression.  The  order  ofinfluential  factors  identified  with  the  machine-learning model was close to clinical experiments.","The results of our study showed that the machine-learning  model  outperformed  the  Cox  regression model. The accuracy of the machine-learning model was higher than the accuracy  of the Cox model, as was its sensitivity to detect effective factors (Table 2).In addition to the increased sensitivity and AUC ofthe model, the machine-learning model reduced the subset  of  informative  features  and  ranked  thepredictive factors appropriately. Furthermore,  this study showed that, in high-dimensional LT datasets, Cox  regression  model  had  lower  sensitivity compared with machine-learning models. Thus, data mining  can be  a beneficial  method  for identifying predictive factors for patient survival after LT in datasets with high dimensionality.Based on our machine-learning results, the order of LT patient survival predictive factors in step 3 ofthe machine-learning model was close to that shownin clinical experience  (Table 4).  The  results  demon -strated  that  graft  failure  is  overwhelmingly  the strongest predictive factor, as we expected. It is notsurprising, but this result revealed the performance of our machine-learning method. As shown in Table4,  some  variables,  including  post-LT  Aspergillusinfection,  post-LT  acute  renal  failure,  vascularcomplications,  GFDI, recipient  history  of diabetesmellitus, MELD score, donor inotropic support in theintensive care unit, number of packed red blood cellunits,  and  pre-LT  dialysis,  have  more  important effects  on  patient  survival  after  LT.  Haseli  and associates36 also reported that patient survival after LT was influenced by graft type, Pediatric End-Stage Liver Disease or MELD score, complications after LT, and  initial  diagnosis.  In  a  study  similar  to  ours, Khosravi and  associates reported  that  the order  ofinfluential factors in the Cox regression model could be close to clinical findings.25 However, we found that the  clinically extracted  factors  in  machine-learning models are consistent with experience. It is not clear why  some  differences exist  in  the  technical  resultsbetween our study and the research from Khosravi andcolleagues; however, a  possible  justification  is thedifference in our approach and the way we filtered irrelevant  variables  in  the  3  steps  and  selectedinfluential variables as inputs for the next phase.Patients who receive ECD livers have outcomessimilar to  those who receive normal livers.37 In our study, we considered recipients of both normal and ECD livers, with investigation of whether ECD is aninfluential factor. In step 2 of feature selection, it wasa predictive factor; however, among the 26 features inthe  last  step,  it could  not be  extracted.  Therefore, ECD may be an essential factor to better understandpatient survival, but it is not more important than theother variables extracted in step 3. Complications are potential causes of LT surgery death; indeed, based on step 3 results, the post-LT predictive factors weremore influential than the other factors. Our extracted discriminative  factors  were  in  line  with  results  of Haseli and colleagues36 and Khosravi and associates.25 Our results showed  that the  number of  packed cell units is a predictive  factor of survival after LT, which  is in  accordance  with  previous  studies.38-40 Furthermore, having a history of diabetes mellitus could have a negative effect on patient survival.41-43 Diabetes  mellitus  after  LT  also  increased  patientmorbidity.44 We found that recipient diabetes mellitus was an important  factor;  however, development of diabetes  mellitus  post-LT  was  removed  from the model. Post-LT acute renal failure was extracted as a predictive  factor  in  our  study;  this  showed  the substantial  impact of this factor on survival. Other studies also revealed that post-LT acute renal failureand  recipient  history  of  diabetes  mellitus  couldpredict survival in LT recipients.45,46 We found thathistory of dialysis could also be as a predictive factorfor patient survival. Furthermore, our study showedthat donor inotropic support in an intensive care unitwas  as  important  as  MELD  and  the  number  ofpacked  red  blood  cell  units  for  LT  survivalprediction. However, Feng and associates47 did notidentify inotropic support to be related to mortality after LT.Most studies on LT survival have had a number oflimitations. A major limitation of our study was thelack of a central data repository; hence, data had to becollected manually from different records at difference places in  the center. Therefore, data  collection wastime  consuming  and  preprocessing  was  difficult. The  second  limitation  of  this  study  was  lack  ofknowledge of nutrition and lifestyle characteristicsof patients. In addition, to investigate other survivaltimes,  such  as  at  12  or  24  months  or  more,  by classification techniques, we required separate runson  separate  data  sets.  This  was  because  some complications may occur in the second 6 months or in second 1 year after LT but do not occur in the first6 months or the first 1 year. Therefore, this situationmay cause some missing value in data set and mayresult in differences in performance of the models, with possible influential factors of patient survival at12 and 24 months or more. Another limitation of thisstudy was its unbalanced data set, which made theanalysis  complex;  however,  we  showed  how machine  learning could correctly  recognize  minor class examples.",https://www.researchgate.net/publication/332314397_Identifying_Factors_That_Affect_Patient_Survival_After_Orthotopic_Liver_Transplant_Using_Machine-Learning_Techniques
Gnanasambandhan et al. 2023,HEL-MCNN: Hybrid extreme learning modified convolutional neural network for allocating suitable donors for patients with minimized waiting time,"Organ transplantation is the ultimate option to treat terminal illness by transplanting the deceased or damaged organs with healthy organs for improving the patient’s life expectancy. The number of organs needed and the organs available for transplantation vary enormously. The tremendous advancements in utilizing big data analytics in the healthcare system make it efficient to explore decision-making information. To make optimal decisions in organ transplantation, this paper proposes a modified convolutional neural network-hybrid extreme learning machine (MCNN-HELM) based prediction model. The proposed MCNN-HELM model utilizes three different real-time datasets as inputs which contain records of liver, heart, and lung transplantation details of the donor and recipient. At first, the missing values and inaccurate data present in real-time datasets are removed via pre-processing. The pre-processed data are then trained using the MCNN-HELM model that efficiently determines the suitable donor for the recipient by minimizing the waiting time of the recipient for the matching organ donor. Moreover, the MCNN-HELM model gives initial preference to patients with high-risk rates to improve their quality of life. The proposed MCNN-HELM model achieves training accuracy of 97.5% with a computational time of 2.2 s, while the precision value of estimated factual outcomes, potential outcomes, and the accuracy of the best donor type are obtained by 16.3582, 16.1401, and 0.6784 which are more efficient than other state-of-the-art methods.","In this section, the MCNN-HELM model is proposed for selecting the donor with minimum waiting time and also providing organ transplantation treatment to high-risk patients. For this performance evaluation, three types of datasets such as PLTSD, UNOS-LU, and UNOS-HR dataset are used. The hyperparameter configuration is used to tune the proposed MCNN-HELM model for attaining better performance compared to other existing techniques. Here, 80% of the data is selected for the training set and the remaining 20% of the data is chosen for the testing set. The performance metrics such as training accuracy, computational time, and 
 are employed for predicting the performance rate. The comparative analysis of the proposed MCNN-HELM model is evaluated by using other state-of-the-art methods such as MINLP, IGBFS-NB, SVM, and FO. In the proposed MCNN-HELM model, the achieved value of precision of estimated factual outcomes is 16.3582, 16.14011 is attained for the precision of estimated potential outcomes, 0.6784 for Accuracy of best donor type, 2.2 s for computational time and 97.5% for training accuracy. The simulation result showed better outcomes compared to other existing methods. In the future, it will concentrate on incorporating additional features linked with family consent. In addition, the proposed system supports approach families by conducting awareness programs to make favorable decisions regarding organ donation to protect one’s life.","https://www.sciencedirect.com/science/article/pii/S0957417423011752#:~:text=At%20first%2C%20the%20missing%20values,for%20the%20matching%20organ%20donor."
Xu et al. 2021,Learning Matching Representations for Individualized Organ Transplantation Allocation,"Organ transplantation is often the last resort for treating end-stage illness, but the probability of a successful transplantation depends greatly on compatibility between donors and recipients. Current medical practice relies on coarse rules for donor-recipient matching, but is short of domain knowledge regarding the complex factors underlying organ compatibility. In this paper, we formulate the problem of learning data-driven rules for organ matching using observational data for organ allocations and transplant outcomes. This problem departs from the standard supervised learning setup in that it involves matching the two feature spaces (i.e., donors and recipients), and requires estimating transplant outcomes under counterfactual matches not observed in the data. To address these problems, we propose a model based on representation learning to predict donor-recipient compatibility; our model learns representations that cluster donor features, and applies donor-invariant transformations to recipient features to predict outcomes for a given donor-recipient feature instance. Experiments on semi-synthetic and real-world datasets show that our model outperforms state-of-art allocation methods and policies executed by human experts.","In this paper, we developed a novel method for learning the compatibility between donors and recipients in
the context of organ transplantation. The key challenge in this problem is that the underlying matching policies, driven by clinical guidelines, creates a
“matching bias”, and hence a co-variate shift in the
joint distribution of donor and recipient features. To
solve this problem, we developed a learning approach
based on matching representations to learn a donorrecipient compatibility function that generalizes well
to the marginal distributions of donors and recipients. Our approach learns feature representations by
jointly clustering donor features, and applying donorinvariant transformations to recipient features to predict outcomes for a given donor-recipient instance. Experiments on multiple datasets show that our model
outperforms state-of-art organ allocation methods.",https://arxiv.org/abs/2101.11769
Martin et al. 2022,Predicting older-donor kidneys' post-transplant renal function using pre-transplant data,"This paper provides a methodology for predicting post-transplant kidney function, that is, the 1-year post-transplant estimated Glomerular Filtration Rate (eGFR-1) for each donor-candidate pair. We apply customized machine-learning algorithms to pre-transplant donor and recipient data to determine the probability of achieving an eGFR-1 of at least 30 ml/min. This threshold was chosen because there is insufficient survival benefit if the kidney fails to generate an eGFR-1 ≥ 30 ml/min. For some donor-candidate pairs, the developed algorithm provides highly accurate predictions. For others, limitations of previous transplants' data results in noisier predictions. However, because the same kidney is offered to many candidates, we identify those pairs for whom the predictions are highly accurate. Out of 6977 discarded older-donor kidneys that were a match with at least one transplanted kidney, 5282 had one or more identified candidate, who were offered that kidney, did not accept any other offer, and would have had ≥80% chance of achieving eGFR-1 ≥ 30 ml/min, had the kidney been transplanted. We also show that transplants with ≥80% chance of achieving eGFR-1 ≥ 30 ml/min and that survive 1 year have higher 10-year death-censored graft survival probabilities than all older-donor transplants that survive 1 year (73.61% vs. 70.48%, respectively).","The distribution of the TRB score for each of the 2 053 007 discarded kidney‐candidate matches is shown in Figure 1 of Online Appendix G. Of the 6977 discarded but matched kidneys in the simulation study cohort, 5971, 5282, and 1500 had at least one matched candidate with risk level = 0.3,0.2,0.1}
, respectively. As expected, the number of discarded kidneys that provide sufficient benefit decreases as 
r decreases. Still, with r = 0.2, over 75% (5282) of the 6977 discarded kidneys under consideration had at least one candidate who could have received significant long‐term benefit.

There are some differences in the characteristics at time of listing of the actual recipients of the 11 527 kidneys and the 5282 candidates with TRB ≥ 0.8 identified in the simulations—see Table 7. Compared to actual recipients, the identified candidates were slightly older (60.74 vs. 57.05 years), more likely to be males (70.2% vs. 61.5%), more likely to be Black and Hispanic (35.8% and 21.8% vs. 30.4% and 14.6%), and more likely to have diabetes (64.4% vs. 43.8%). These results are consistent and reinforce the findings in Tullius and Rabb (2018) and Concepcion et al. (2016) that older candidates with diabetes may be the ones that benefit from currently discarded organs. 

We present a versatile tool that can be used by the OPTN to more judiciously utilize older‐donor kidneys. Using customized machine‐learning techniques and data available at the time of offer, we illustrate how to estimate the risks and benefits of transplantation for each donor‐candidate pair. Additionally, as a proof‐in‐concept of the value of our approach, we performed simulation experiments to quantify the number of older‐donor discarded kidneys that could have provided sufficient benefit to at least one matched candidate. Some of the discarded kidneys could have been used with better outcomes compared to transplanted kidneys if TRB and r = 0.2 were used to select which transplants to perform. The analysis so far has been restricted to data included in the STAR file, leaving out clinically‐relevant data elements that are available to clinicians at the time of kidney offer, for example, the size and mass of the kidney. Finally, anticipated transportation delays are likely to play a role in organ acceptance decisions. Such issues are not included in the current model and present an opportunity for further improving the methodology.",https://europepmc.org/article/MED/37082424#nav22083-sec-0018
Kantidakis et al. 2020,Survival prediction models since liver transplantation - comparisons between Cox models and machine learning techniques,"Background
Predicting survival of
Predicting survival of recipients after liver transplantation is regarded as one of the most important challenges in contemporary medicine. Hence, improving on current prediction models is of great interest.Nowadays, there is a strong discussion in the medical field about machine learning (ML) and whether it has greater potential than traditional regression models when dealing with complex data. Criticism to ML is related to unsuitable performance measures and lack of interpretability which is important for clinicians.

Methods
In this paper, ML techniques such as random forests and neural networks are applied to large data of 62294 patients from the United States with 97 predictors selected on clinical/statistical grounds, over more than 600, to predict survival from transplantation. Of particular interest is also the identification of potential risk factors. A comparison is performed between 3 different Cox models (with all variables, backward selection and LASSO) and 3 machine learning techniques: a random survival forest and 2 partial logistic artificial neural networks (PLANNs). For PLANNs, novel extensions to their original specification are tested. Emphasis is given on the advantages and pitfalls of each method and on the interpretability of the ML techniques.

Results
Well-established predictive measures are employed from the survival field (C-index, Brier score and Integrated Brier Score) and the strongest prognostic factors are identified for each model. Clinical endpoint is overall graft-survival defined as the time between transplantation and the date of graft-failure or death. The random survival forest shows slightly better predictive performance than Cox models based on the C-index. Neural networks show better performance than both Cox models and random survival forest based on the Integrated Brier Score at 10 years.

Conclusion
In this work, it is shown that machine learning techniques can be a useful tool for both prediction and interpretation in the survival context. From the ML techniques examined here, PLANN with 1 hidden layer predicts survival probabilities the most accurately, being as calibrated as the Cox model with all variables.","With the rise of computational power and technology on the 21 st century, more and more data have been collected in the medical field to identify trends and patterns which will allow building better allocation systems for patients, provide more accurate prognosis and diagnosis as well as more accurate identification of risk factors. During the past few years, machine learning (ML) has received increased attention in the medical area. For instance, in the area of LTs graft failure or primary non-function might be predicted at decision time with ML methodology [48]. Briceño et al. created a NN process for donor-recipient matching specifying a binary classification survival output (recipient or graft survival) to predict 3-month graft mortality [49].

In this study statistical and ML models were estimated for patients from the US post-transplantation. Random survival forest performed better than Cox models with respect to the C-index. This shows the ability of the model to discriminate between low and high risk groups of patients. The C-index was not estimated for NN because a natural ordering of subjects is not feasible. Therefore, the Brier score was measured each year for all methods. The RSF showed similar results to the Cox models having slightly smaller total prediction error (in terms of IBS). The NNs performed in general better than the Cox models or the RSF and had very similar performance over time. RSF and survival NN are ML techniques which have a different learning method and model non-linear relationships between variables automatically. Both methods may be used in medical application but should be applied at present as additional analysis for comparison.

Special emphasis was given on the interpretation of the models. An indirect comparison was performed to examine which are the most prognostic variables for a Cox model with all variables, a RSF and NNs. Results showed that Cox model with all variables (via absolute z-score values) and the NNs with one/two hidden layer(s) (via relative importance) identified similar predictors. Both methods identified re-transplantation as the strongest predictor and donor age, diabetes, life support and race as relatively strong predictors. According to RSF, the most prognostic variables were donor age, re-transplantation, life support and serology status of HCV. Aetiology and last serum creatinine were selected as the 7th and the 8th most prognostic. This raises a known concern about the RSF bias towards continuous variables and categorical variables with multiple levels [50] (aetiology has 9 levels: metabolic, acute, alcoholic, cholestatic, HBV, HCV, malignant, other cirrhosis, other unknown). As continuous and multilevel variables incorporate larger amount of information than categorical, they tend to be favoured by the splitting rule of the forest during binary partitioning. Such bias was reflected in the variable importance results.

When comparing statistical models with machine learning techniques with respect to interpretability, Cox models offer a straightforward interpretation through the hazard ratios. On the contrary, for both neural networks and random survival forests the sign of the prediction is not provided (if the effect is positive or negative). Additionally, for NNs interpretation is possible for different variable levels (with the method of Garson and its extension), whereas for RSF only the total effect of a variable is shown. There is no common metric to directly compare Cox models with ML techniques in terms of interpretation. Future research in this direction is needed.

ML techniques are inherently based on mechanisms introducing randomisation and therefore very small changes are expected between different iterations of the same algorithm. To evaluate stability of performance, ML models were run several times under the same parametrisation. RSF were consistently stable after a certain number of trees (300 were selected). This was not the case for the NNs where instability is a common problem. It is challenging to tune a NN due to many hyper-parameter combinations available and the lack of a consistent global performance measure for survival data. IBS was used to tune the novel NNs, which may be the reason of instability for the NN with 2 hidden layers together with the large number of weights. Note also that the NN with 1 hidden layer is well calibrated whereas the NN with 2 hidden layers is less calibrated on the test data.

This is the first study where ML techniques are applied to transplant data where a comparison with the traditional Cox model was investigated. To construct the survival NN, the original problem had to be converted into a classification problem where exact survival times were transformed into (maximum) 10 time intervals denoting years since transplantation. On the other hand, for the Cox models and the RSF exact time to event was used. Recently, a new feed forward NN has been proposed for omics data which calculates directly a proportional hazards model as part of the output node using exact time information [51]. A survival NN with exact times may lead to better predictive performance. For UNOS data, 69.1% of the recipients were alive/censored and 30.9% had the event of interest. Results above were based on these particular percentages for censoring and events (for the NNs the percentages varied because of the reformulation of the problem).

It might be useful to investigate how the number of variables affects the performance of the models. Here 97 variables were pre-selected supported by clinical and statistical reasons (e.g. variables available before or during LT). It might be interesting to repeat the analyses on a smaller group of predictors, implementation time can be drastically reduced as the calculation complexity depends on sample size and predictors multiplicity. Alongside, predictive accuracy might be increased as some noisy factors will be removed from the dataset increasing the signal of potentially prognostic variables.

Both traditional Cox models and PLANNs allow for the inclusion of time-dependent covariates. For PLANNs, each patient is replicated multiple times during the transformation of exact times into a set of k non-overlapping intervals in long format. Thus, different values of a covariate can be naturally incorporated to increase the predictive ability of the networks. It would be interesting to apply and compare the predictive ability of time-dependent Cox models and PLANNs to liver transplantation data including explanatory variables whose values change over time. Such extension to more dynamic methods may increase predictive performance and help in decision making.",https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-020-01153-1#Sec17
Nemati et al. 2023,Predicting kidney transplant survival using multiple feature representations for HLAs,"Kidney transplantation can significantly enhance living standards for people suffering from end-stage renal disease. A significant factor that affects graft survival time (the time until the transplant fails and the patient requires another transplant) for kidney transplantation is the compatibility of the Human Leukocyte Antigens (HLAs) between the donor and recipient. In this paper, we propose 4 new biologically-relevant feature representations for incorporating HLA information into machine learning-based survival analysis algorithms. We evaluate our proposed HLA feature representations on a database of over 100,000 transplants and find that they improve prediction accuracy by about 1%, modest at the patient level but potentially significant at a societal level. Accurate prediction of survival times can improve transplant survival outcomes, enabling better allocation of donors to recipients and reducing the number of re-transplants due to graft failure with poorly matched donors.","A broad group of studies has used data-driven statistical models to
predict graft survival times or measure risk factors’ impact on graft survival. Prior work includes multivariate analysis using Cox proportional
hazards (Cox PH) models with a small number of covariates [24–26].
There has been more recent work on machine learning-based survival analysis applied to kidney transplantation, including an ensemble
model that combines Cox PH models with random survival forests [27]
and a deep learning-based approach [28]. Our results compare favorably to prior studies [24–26,28] using
the same SRTR data we use in this study. Each study differs in inclusion criteria, time duration, and several other factors that prevent
a direct comparison; however, we include their reported results here
for reference. Two older studies [24,25] using Cox PH models without
regularization achieved C-indices of 0.62 and 0.61, respectively. A more
recent study also using a Cox PH model with only pre-transplant covariates [26] including HLA MM achieved a C-index of 0.64; however, their
study included both living and deceased donors while ours considers
only deceased donors. Transplant outcomes with living donors are
much more favorable [26], which may result in easier prediction.
Another recent study [28] used a deep learning approach applied to
both pre- and post-transplant covariates to achieve a C-index of 0.655,
less than the 0.676 we achieved.
Several other recent studies have focused on prediction of patient
survival rather than graft survival, with [27,29] achieving C-indices
of 0.70 and 0.724, respectively. Prediction of patient survival is much
easier than prediction of graft survival, which we focus on in this
paper. For example, [25] considered both patient and graft survival
and achieved a C-index of 0.68 for patient survival compared to 0.61
for graft survival. We also argue that graft survival is the more relevant
clinical endpoint, as a patient who survives a transplant but suffers a
graft failure will require a re-transplant and returns to the waiting list. 

8. Significance and impact
Transplantation outcome prediction is instrumental for clinical decision-making, as well as allocation policy development. The kidney allocation policy by the OPTN was developed to encourage fairness (equal access to treatment) and effectiveness (the longest predicted survival) [30] in transplantation. Informed clinical decision making allows for avoidance of high-risk transplants and thus reduces number of graft losses. However, accurate prediction of transplant outcomes remains a daunting challenge due to the high complexity of human biology.

In addition, failure to account for complexities of HLA results in unintended consequences in transplantation. As such, OPTN’s good intention to promote HLA matching initially resulted in de facto discrimination against African Americans, whose HLA gene locus is highly diverse and who therefore were not selected for transplantation as frequently as Caucasians and other races [31]. The requirement for HLA matching was later relaxed, but the problem of racial disparities in access to high-quality transplants persists until today [32]. By modifying our approach to HLA immunogenicity quantification, adding biologically-relevant representations of HLA, we attempt to build improved models for transplant outcome prediction, which may help address the pressing problem of poor long-term transplantation outcomes.

Addition of HLA features improves our predictive model and presents clinical interest for two reasons. First, physicians are most comfortable making decisions with HLA information at hand. There is a growing consensus in the transplantation field that HLA is a critical consideration for pre-transplant patient evaluation [33]. In the U.S., nationwide sharing of fully HLA-matched kidneys is mandated in certain situations, and transplant centers typically require labs to provide HLA information before a crossmatch (a final pre-transplant test). Therefore, clinicians, governmental entities, and payers who are interested in predicting transplantation outcome are typically interested in making sure HLA compatibility is factored into the model.

Second, due to the large size of the transplant waiting list and exorbitant cost of pre-transplant kidney replacement therapy, even a small improvement in post-transplant outcomes would result in large economic and social impact over time, as was described in simulations by Segev et al. [34]. They showed that as much as $750 million could be saved if transplant rates were to improve by 5.7% in a 4000 patient pool. It would require a separate study to quantify the impact of a 1% increase in prediction accuracy on long-term graft survival, however, it is reasonable to think that implementation of improved predictive models in transplant allocation would result in improvement in transplant survival, with downstream societal impact. Our findings are thus useful for assisting clinical decision making aimed at improving long-term allograft survival.",https://www.sciencedirect.com/science/article/pii/S0933365723001896?casa_token=2v89rFyHeU4AAAAA:079w3c5Yh0QPTa5wZflX248S752BqRj0nkpuXGTT5ALVkjWCOn0FxsBs6qTdcvvbiU7oBluh
Vigia et al. 2023,Pancreas Rejection in the Artificial Intelligence Era: New Tool for Signal Patients at Risk,"Introduction: Pancreas transplantation is currently the only treatment that can re-establish normal endocrine pancreatic function. Despite all efforts, pancreas allograft survival and rejection remain major clinical problems. The purpose of this study was to identify features that could signal patients at risk of pancreas allograft rejection. Methods: We collected 74 features from 79 patients who underwent simultaneous pancreas–kidney transplantation (SPK) and used two widely-applicable classification methods, the Naive Bayesian Classifier and Support Vector Machine, to build predictive models. We used the area under the receiver operating characteristic curve and classification accuracy to evaluate the predictive performance via leave-one-out cross-validation. Results: Rejection events were identified in 13 SPK patients (17.8%). In feature selection approach, it was possible to identify 10 features, namely: previous treatment for diabetes mellitus with long-term Insulin (U/I/day), type of dialysis (peritoneal dialysis, hemodialysis, or pre-emptive), de novo DSA, vPRA_Pre-Transplant (%), donor blood glucose, pancreas donor risk index (pDRI), recipient height, dialysis time (days), warm ischemia (minutes), recipient of intensive care (days). The results showed that the Naive Bayes and Support Vector Machine classifiers prediction performed very well, with an AUROC and classification accuracy of 0.97 and 0.87, respectively, in the first model and 0.96 and 0.94 in the second model. Conclusion: Our results indicated that it is feasible to develop successful classifiers for the prediction of graft rejection. The Naive Bayesian generated nomogram can be used for rejection probability prediction, thus supporting clinical decision making.","Recent advancements in ML have shown promising results in the field of medical decision making. In particular, ML algorithms have the potential to support clinical decisions by identifying patients at risk of rejection after transplantation [25].
However, before the implementation of these new methodologies to support clinical medical decision making, several questions must be answered. Can we, based on clinical data, only identify patients that are more prone to have a rejection event? Are all features contributing to the model? If not, which are the most informative features that better contribute to the model?
To answer the first question, it is possible to use clinical data to identify patients who may be more prone to rejection events. In healthcare, ML algorithms can be trained on large datasets of patient information and outcomes to identify patterns and factors that are associated with several healthcare events and scenarios [26]. In the field of transplantation and in pancreatic transplants, several authors have developed algorithms to make predictions of which patients are at a higher risk for rejection of pancreatic islet grafts [27] or of suffering delayed pancreatic endocrine graft functioning.
Regarding the second question, not all features will necessarily contribute equally to the model. Some characteristics may be less important or even irrelevant or can even result in worse results, since the expanded complexity as a result of the added number of features several times over has no major impact on the prediction values. Many times, it is better to simplify the model to improve its accuracy [28]. Thus, and answering the last question to determine which features have the most information and contribute the most to the model, a feature importance analysis can and should be performed. In our study on information gain, as expected we observed a major gain in terms of AUC and CA [29].
Our improved models, based on pre-transplant recipient, donor, and transplantation features, have shown that they can be practical tools for identifying patients at risk of rejection after transplantation. It is highly desirable to identify this risk early in the immediate post-transplant period so that we can take measures to minimize inflammatory/immune aggression on the pancreatic graft. Therefore, we identified the features that had the greatest impact on the models, some of them already identified in the literature for other solid organs, namely those related to: (1) pre-transplant recipient factors: pre-transplant insulin requirement and virtual panel reactive antibody (VPRA) percentage; traditionally, a high vPRA has been associated with increased risk of rejection [30]; and height, type, and length of dialysis, which are significant factors in determining the onset of graft function and rejection [31]. (2) Donor factors: pancreas donor risk index (PRDI); in the literature for the PRDI, 1.24 or higher donor grafts had significant poorer outcome compared to a PDRI less than 1.24 [32]; blood glucose level. (3) Transplantation factor: warm ischemia time. (4) Immediate post-transplant factors: days in intensive care and de novo donor-specific antibodies (DSA). De novo DSA against both classes I and II HLA conferred poorer graft survival [33].
The provided results discuss the application of machine learning algorithms, specifically Support Vector Machines (SVMs) and Naive Bayes, for the identification of rejection in simultaneous kidney–pancreas transplantation. Initially, when all 74 features were considered, the t-SNE score plot did not show any significant cluster formation, indicating that individual feature patterns were unique to each patient rather than being unique to the analyzed condition. Similarly, the performance of the SVM and Naive Bayes models based on all features was not satisfactory, with lower specificity and overall accuracy. To address this issue and identify the features with the greatest impact on the predictive model, a feature selection algorithm called Info Gain was employed. The application of Info Gain resulted in improved t-SNE plots, forming clusters with the t-SNE model, SVM model, and Naive Bayes model. Although there were some misclassifications, they did not significantly worsen the outcomes. The AUC and overall accuracy values improved significantly for both the SVM and Naive Bayes models after feature selection.
Specifically, the SVM model achieved an AUC of 0.96 and an overall accuracy of 0.94, with a substantial increase in specificity from 0.46 to 0.93 when using the top 10 features identified using Info Gain. The Naive Bayes model also showed improvement, with an AUC of 0.97 and an overall accuracy of 0.87. However, the Naive Bayes model had a higher false classification rate compared to the SVM model.
The analysis of feature importance revealed that the type of kidney replacement treatment and the formation of de novo DSA were among the most influential features for the models’ AUC results. However, the impact of these features on the overall accuracy (CA) differed between the SVM and Naive Bayes models. As a result, the SVM model correctly predicted rejection in 93.9% of true rejection patients and non-rejection in 92.3% of true non-rejection patients, with a 13.8% false classification rate. The Naive Bayes model had a 15.2% false classification rate, but it achieved a 100% prediction rate for all patients with rejection.
Based on the promising performance of the Naive Bayes model, a nomogram plot was constructed to predict pancreas rejection in simultaneous kidney–pancreas transplantation. The nomogram incorporated the top 10 features identified using Info Gain and aimed to provide a means of predicting or risk stratifying pancreas rejection in this transplantation setting and has the potential to serve as a useful tool in supporting medical decision making, particularly for patients for whom undergoing a biopsy can be risky.
In the future, this nomogram could provide valuable assistance in clinical settings, particularly in the clinical decision-making process when taking measures that could preserve the transplanted pancreas, improve patients’ quality of life, and avoid missing the opportunity for transplantation due to organ scarcity and the lower likelihood of retransplantation in case of graft loss. Based on the findings of the study, there are several potential future research directions that can build upon the current work and expand the application of machine learning models in transplantation settings. These directions are as follows: (a) Conducting external validation studies using independent datasets from different transplantation centers and diverse patient populations would be essential to validate the developed machine learning models. External validation helps assess the models’ generalizability and ensures their reliability and effectiveness across different settings. (b) Conducting prospective studies that collect data in real-time would provide more robust evidence for the models’ predictive capabilities. Prospective studies allow for more accurate and standardized data collection, minimizing biases associated with retrospective designs. (c) Assessing the applicability and generalizability of machine learning models in other transplantation settings beyond simultaneous kidney–pancreas transplantation is crucial. The models can be tested and validated in different solid organ transplantations, such as liver, heart, or lung transplants, to determine their utility and potential for personalized rejection prediction in those contexts. (d) Integration with Clinical Decision Support Systems, exploring the integration of machine learning models into clinical decision support systems to facilitate their practical implementation in transplantation clinics. By integrating the models into electronic health record systems, clinicians can receive real-time predictions and recommendations for personalized patient management, aiding in the decision-making process and potentially improving patient outcomes.
By pursuing these future research directions, the field can advance the understanding and application of machine learning models in transplantation, ultimately leading to more personalized and effective patient care, improved outcomes, and enhanced decision-making processes for transplant clinicians.
This study has some limitations and biases; the study focused on a specific set of clinical and laboratory features, resulting in a limited number of variables being considered for analysis. There may be other relevant factors not included in the study that could influence the prediction of rejection in SPK transplantation. Additionally, there may be missing data for some variables, which could introduce bias and affect the robustness of the models. The preferred method for diagnosing and grading pancreas transplant rejection is through biopsy. However, due to the potential risks and complications associated with biopsies, they are not commonly performed. As a result, the reliance on clinical indicators and laboratory testing for rejection monitoring introduces limitations in the accuracy of rejection classification and identification.",https://www.mdpi.com/2075-4426/13/7/1071
Bogucki et al. 2023,‘Be the Match’. Predictors of Decisions Concerning Registration as a Potential Bone Marrow Donor—A Psycho-Socio-Demographic Study,"(1) Background: The study was aimed at a better understanding of the factors determining making a decision to become a potential bone marrow donor, in a Polish research sample; (2) Methods: The data was collected using a self-report questionnaire among persons who voluntarily participated in the study concerning donation, conducted on a sample of the Polish population via Internet. The study included 533 respondents (345 females and 188 males), aged 18–49. Relationships between the decision about registration as potential bone marrow donor and psycho-socio-demographic factors were estimated using the machine learning methods (binary logistic regression and classification & regression tree); (3) Results. The applied methods coherently emphasized the crucial role of personal experiences in making the decision about willingness for potential donation, f.e. familiarity with the potential donor. They also indicated religious issues and negative health state assessment as main decision-making destimulators; (4) Conclusions. The results of the study may contribute to an increase in the effectiveness of recruitment actions by more precise personalization of popularizing-recruitment actions addressed to the potential donors. It was found that selected machine learning methods are interesting set of analyses, increasing the prognostic accuracy and quality of the proposed model.","The presented research is among the most comprehensive studies in which a complex model was developed concerning the relative importance of the psycho-socio-demographic variables in the process of registration as a potential bone marrow donor. It provided evidence that this profile may be an important factor in making the decision about the readiness to make a donation decision of the LADS type and also confirmed that the process of registration of a donor is complex, because the ranging of variables according to importance of their effect exerted on the registration as PBMD was not always the same as that which most differentiated those who registered and those who did not. Among the factors mentioned in the hypothesis as predictors of a readiness to donate in the form of registration as PBMD, not all of them appeared to have an actual effect on making the decision.

Personal experiences. Knowing someone who declared to be a donor or someone who needed and received donation help are factors with a confirmed predictive effect [17,19,33]. This means that here, rather than an upbringing at home or examples of parents’ social activity, were of crucial importance. Such decisions were made clearly under the effect of personal experience, mainly contact with a donor (or an actual recipient). If any family factors played some role here, they were mainly friendly relationships between parents and the social activity of the mother, but these were not factors of great prognostic importance. This means that the ‘creation of a donor’s identity’ using family modelling-identification actions (e.g., by means of the model of intergenerational transmission of parental attitudes towards donation), as suggested by some studies [34], is relatively complex and therefore seems to depend on whether the person has/had acquaintances/friends showing donation behaviors or a personal contact with the environment of ill people. Here, the effect that occurred here was similar to that described by Mocan and Tekin [35], which enhanced the tendency towards donation in people who had contact with ill people (e.g., during a visit to a hospital) or were ill themselves. Therefore, the hypothesis is confirmed that it is mainly the contact with a donor that shapes the donor’s identity [19]. Similarly, the engagement in volunteering seems to pave the way for donation decisions, according to the contemporary understanding of its idea as an instrument of building good. Nevertheless, remaining in a relationship did not exert a predictive effect on the readiness to donate, as was signaled in the research [13,17,36], and this was a satisfying intimate relationship, indicated by CART as more frequent in PBMD groups, that played a prognostic role concerning the decision.

Declaration of religiosity and religious practices/religious objections. The effect of religiosity on attitudes towards transplantation was confirmed [37]. Religious beliefs seem to exert a suppressing effect on the decisions concerning donation. The declaration of religiosity was identified as a factor related with negative perception of organ donation. These results are consistent with previous findings, suggesting the effect of the declared religiosity on the probability of making a decision about potential donation, which means that the religious aspect may block donation decisions [13,15,34,37,38,39,40,41,42] as in other types of donations [12,14,43,44,45], although there are some studies which deny this [46,47]. However, it is noteworthy that the applied analyses were not completely consistent—LR suggests a suppressing effect of participation in religious practices, whereas CART emphasizes the role of religiosity. Thus, religiosity (attitude) and active religious practices (behavioral aspect) are separate categories that are worth more comprehensive investigations in the future.

Social status. People with a higher level of education seem to be more willing to make decisions about donations. The studies to date have demonstrated that PBMDs are better educated and have a more positive attitude towards science than non-donors [13,15], which seems to be a universal pattern, active irrespective of the cultural circle in which the studies are conducted and the type of donation [12,14,44,48,49,50,51]. Here, no effect was observed of a higher material status as a predictor of the decision, mentioned by other researchers with respect to, for example, blood donation or a post-mortem donation [44]. The places of birth and residence were also related to the decisions about making a donation. More frequently, the decisions about donations were made by respondents who live in towns (although the fact of coming from this environment was not the predictor of a donation decision). It is an interesting fact worthy of further studies that these were not the largest cities (usually considered as the centers of cultural or social activity) that released the readiness for donation.

State of health. A negative evaluation of the state of one’s own health appeared to be the factor suppressing readiness for BM donation. This confirms the results of other studies [16], also in relation to organ donation [52]. It is worth noting that diseases reported by the respondents (Table 1) are not always an absolute exclusion criterion, which is emphasized in the literature [53]. Thus, the importance of the education of potential donors should be emphasized concerning the criteria of exclusion from BM donation [16,54].

Gender and age. Age did not exert an effect on the readiness for donating, which differed from the results obtained by other researchers [13,15]. Moreover, no significant differences in the readiness for donation were observed according to gender. Nevertheless, women appeared to be more interested in participating in the study concerning the problem of donation [18], which does not mean, however, that they expressed a greater readiness for donation. This confirms that gender acts differently in the various groups of respondents and the types of donations [16,18,45,55,56,57,58], which requires further studies. Respondents aged 40–50 years constituted a slight percentage of the study group (13 people, less than 2.5% of the study group), while they may still be bone marrow donors.

Limitations. The study has certain limitations inherent mainly in the method of recruitment to the research group (selection) and in the method of analysis. A limitation may be the voluntary character of the group. Studies on volunteers require caution while generalizing the results, as there may exist differences between those who decided to participate in the study and those who did not [27]. It should be remembered that a sample consisting of volunteers may not be entirely representative. Volunteers differ from the rest of the population in that they reported to the study themselves. This may be associated with differences regarding other characteristics which contributed to the fact that the volunteers wanted to participate in the study (for example an interest in the problem, high level of pro-social behavior, etc.). Due to the fact that they differ from the rest of the population, the results obtained cannot always be generalized to the rest of the population, because it may be to some extent biased. Another selection limitation of this study is its geographical context, in which it was conducted. In order to verify the results, they should be replicated in other contexts (increasing external validity can be achieved by replicating studies in other populations). A limitation is also the selection by the Internet because the population of Internet users differs from the populations of individual countries (age, education, place of residence, etc.). This means that relatively rarely may the results of the conducted study be generalized to the population of the whole country or all Internet users [59]. However, very frequently, in the focus of interest of a researcher, are communities other than the ‘whole population’. Many studies concentrate rather on selected groups of individuals, which also concerns the present study.

The second limitation is inherent in the method of analysis. It should be remembered that the last stages of construction of the system is the implementation of algorithm in real-time conditions and the necessity for constant monitoring and maintenance of a set of ageing models (nearly each stochastic predictive model is subject to degradation, as the objects which it describes change), which requires the continuation of studies in the future. Thus, the models should be updated appropriately to suit the environment in the application stage [60]. Finally, although this study explained a significant proportion of the variance in ambivalence, the unexplained variance indicates that the other contributors to ambivalence should be explored.",https://www-ncbi-nlm-nih-gov.myaccess.library.utoronto.ca/pmc/articles/PMC10252417/
Sageshima et al. 2023,Prediction of High-Risk Donors for Kidney Discard and Nonrecovery Using Structured Donor Characteristics and Unstructured Donor Narratives,"Importance  Despite the unmet need, many deceased-donor kidneys are discarded or not recovered. Inefficient allocation and prolonged ischemia time are contributing factors, and early detection of high-risk donors may reduce organ loss.

Objective  To evaluate the feasibility of machine learning (ML) and natural language processing (NLP) classification of donors with kidneys that are used vs not used for organ transplant.

Design, Setting, and Participants  This retrospective cohort study used donor information (structured donor characteristics and unstructured donor narratives) from the United Network for Organ Sharing (UNOS). All donor offers to a single transplant center between January 2015 and December 2020 were used to train and validate ML models to predict donors who had at least 1 kidney transplanted (at our center or another center). The donor data from 2021 were used to test each model.

Exposures  Donor information was provided by UNOS to the transplant centers with potential transplant candidates. Each center evaluated the donor and decided within an allotted time whether to accept the kidney for organ transplant.

Main Outcomes and Measures  Outcome metrics of the test cohort included area under the receiver operating characteristic curve (AUROC), F1 score, accuracy, precision, and recall of each ML classifier. Feature importance and Shapley additive explanation (SHAP) summaries were assessed for model explainability.

Results  The training/validation cohort included 9555 donors (median [IQR] age, 50 [36-58] years; 5571 male [58.3%]), and the test cohort included 2481 donors (median [IQR] age, 52 [40-59] years; 1496 male [60.3%]). Only 20% to 30% of potential donors had at least 1 kidney transplanted. The ML model with a single variable (Kidney Donor Profile Index) showed an AUROC of 0.69, F1 score of 0.42, and accuracy of 0.64. Multivariable ML models based on basic a priori structured donor data showed similar metrics (logistic regression: AUROC = 0.70; F1 score = 0.42; accuracy = 0.62; random forest classifier: AUROC = 0.69; F1 score = 0.42; accuracy = 0.64). The classic NLP model (bag-of-words model) showed its best metrics (AUROC = 0.60; F1 score = 0.35; accuracy = 0.59) by the logistic regression classifier. The advanced Bidirectional Encoder Representations From Transformers model showed comparable metrics (AUROC = 0.62; F1 score = 0.39; accuracy = 0.69) only after appending basic donor information. Feature importance and SHAP detected the variables (and words) that affected the models most.

Conclusions and Relevance  Results of this cohort study suggest that models using ML can be applied to predict donors with high-risk kidneys not used for organ transplant, but the models still need further elaboration. The use of unstructured data is likely to expand the possibilities; further exploration of new approaches will be necessary to develop models with better predictive metrics.","Results of this study suggest that ML models with structured and unstructured data may be applied to classify donors with high-risk kidneys that were not used for organ transplant. ML models with basic structured data showed prediction scores comparable with logistic regression models. In the case of NLP, the classical bag-of-words model and the advanced BERT model were slightly inferior to the models with structured data, probably due to variations in the quality of the free-text data entered in the report.

Several logistic regression models using structured data have been proposed to identify donors at high risk for kidney discards.3-6 Most of them demonstrated better AUROC values (>0.8) than the present study. The study population was the main difference between the previously reported studies and the present study. The current study included all kidney donors regardless of recovery status, whereas the previous studies included only donors with at least 1 kidney recovered for transplant. Consequently, only a small fraction of kidneys were discarded in the prior studies, whereas a sizable portion of kidneys in our study was not transplanted (either not recovered or discarded). The event rate difference likely affected the outcome metrics. In the absence of information on the final recovery status at the time of the kidney offer, it was still reasonable to include such donors in the analysis. Another difference was the sample size. We could only include donor data for offers to our transplant center, whereas prior studies have used more extensive national data. Although the concept of the strategy does not change by sample size, the smaller sample size likely contributed to the poor metrics of the current study.

Massie et al3,22 created a model to predict the probability of discard or delay and later validated their model with another cohort of donors. In contrast to our models, they created a model at the kidney (rather than donor) level, and the model included more detailed donor and kidney information (eg, diabetes duration, insulin use, pump preservation, and biopsy findings). Although the model was highly effective in predicting kidney discard or allocation delays, some of the information (eg, pump preservation, biopsy findings) is not available in the early stage of kidney offers, limiting the application of this model to the later stage of allocation. The model created by Marrero et al4 also included detailed donor information, including heavy alcohol use, cigarette use, tattoos, blood type, and cytomegalovirus antibody status. The model also included biopsy findings and pump use, limiting its application to the postrecovery state. Because the performance of biopsy (plus the degree of glomerulosclerosis) and no pump use (plus the resistance on pump) are associated with increased odds of discards in expanded criteria donor kidneys,23 the inclusion of such factors would likely have improved the outcome metrics in these studies. The present study suggests that when the variables are limited to basic a priori information often available early in the allocation process, prediction models using individual variables were not necessarily superior to a model using only the aggregated KDPI score. It was consistent with the increasing discard rate as KDPI increases in our cohort (Figure 1) and with the high discard rate among high KDPI donors as published by the Organ Procurement and Transplantation Network.1

ML models are becoming increasingly popular in structured data analysis in medicine.7-10 Ensemble models of weak classifiers have been shown to exhibit performance comparable with deep learning (ie, neural network) models when using only tabular data.7-10 Compared with a simple mathematical model (eg, logistic regression), ML models may capture more intricate patterns and relationships that are not obvious to the human eye. However, one drawback of the ML model is its lack of explainability. Even if the predictive performance is adequate, it may not be usable in clinical practice if there is no explanation for why.24-26 The present analysis showed that feature importance and SHAP could be used to improve explainability. As expected, donor age, creatinine level, and history of diabetes and hypertension were highly significant variables in predicting that no organ transplant took place.

Unstructured free-text data have the potential to revolutionize the medical field but is untapped in transplant prediction models. One of the challenges is the availability and quality of data; obtaining the high-quality free text from the patient’s medical records is difficult. On the other hand, the donor’s narrative is included in the organ donation report along with structured data.12 Although we have noticed that the quality of information varies among organ recovery agencies or onsite coordinators, the narrative is readily available and may provide nuanced information that is not captured in the structured fields, eg, “patient found with a needle on his arm.” As expected, the classic bag-of-words model demonstrated relatively low test scores. This model does not capture the context of words and ignores the order of words. It also requires a large amount of data for the model to be accurate.27 Despite our limited data set’s low metrics, feature importance and SHAP summaries were informative in identifying donor conditions that may have affected organ selection, eg, words implying chronic disease such as chronic obstructive pulmonary disease, hypertension, and insulin might work negatively, whereas words implying trauma such as injury might work positively.

The advanced transformer-based BERT model demonstrated somewhat disappointing results. Considering the possibility of an era effect and baseline differences between the 2 data sets, the same analyses (including a BERT model) were also performed on randomly split data sets of the entire cohort. However, no apparent improvement in model performance was observed (data not shown). Because achieving state-of-the-art performance was not the study’s goal, no special preprocessing or feature engineering was applied to the raw data, and hyperparameter tuning was also limited. Nonetheless, we noticed some limitations to the use of donor narratives. Some donor reports provided only general data about donor surgery schedule and hospital and coordinator information with limited medically relevant information. Including such insignificant data might have diluted the notation of important donor conditions, resulting in lower prediction scores. Although transformer-based models can handle longer input sequences and more extensive vocabulary than traditional models, their performance is suboptimal when a long medical summary is used.27 Due to inconsistent free-text input in the different fields, we combined them into a single-text input. Performance could have improved if the model could reliably handle longer sentences (tokens). Because these models are capable of transfer learning, creating a pre-trained model using the national donor data (after excluding irrelevant texts) would be an attractive option. Each organ procurement organization and transplant center can fine tune the model using their own data to reflect their local practices.

Limitations
This study has several limitations. First, the data used to train and validate the models were from a single site with a unique donor composition, which limits the models’ generalizability. Due to resource constraints, we were unable to evaluate national data. However, if the initial model was generated by broader national data and transferred to a local model, the outcome metrics would likely be higher than in the current study. Second, in the conventional ML models, the features included in the modeling are limited, with only limited features outside of the KDPI components. If other potentially influential features (eg, history of cancer or coronary artery disease, heavy alcohol drinking, or tobacco use) or dynamic donor data (eg, blood pressure, urine output, postrecovery ischemia time) were included, the performance of the model could be better than that of the current model. Third, we used only donor data, but if recipient data and transplant center information were added, it is possible that better models could be generated. We were unable to address the question of transplant candidate characteristics that might influence the acceptance of offers. Fourth, the data were not granular enough to distinguish nonrecovery due to absolute contraindications (eg, active infection, malignancy) and nonrecovery due to logistical inefficiencies; ideally, the former should have been excluded from the cohort and the latter analyzed more closely to optimize the donation process. Fifth, we did not combine structured and unstructured data to make predictions. Because multimodal ML has been shown to be effective,28 methods combining different types of data could help create both explainable and accurate models. Finally, the results predicted by these models must be efficiently communicated. Whether these predictions will expedite organ allocation and acceptance decisions and improve organ utilization remains to be seen.",https://jamanetwork-com.myaccess.library.utoronto.ca/journals/jamasurgery/fullarticle/2811385
Hickey et al. 2023,Long-Term outcomes in adult patients with congenital heart disease considered for transplantation: A single center study,"Background

Adult congenital heart disease (ACHD) patients pose unique challenges in identifying the time for transplantation and factors influencing outcomes.

Objective

To identify hemodynamic, functional, and laboratory parameters that correlate with 1- and 10-year outcomes in ACHD patients considered for transplantation.

Methods

A retrospective chart review of long-term outcomes in adult patients with congenital heart disease (CHD) evaluated for heart or heart + additional organ transplant between 2004 and 2014 at our center was performed. A machine learning decision tree model was used to evaluate multiple clinical parameters correlating with 1- and 10-year survival.

Results

We identified 58 patients meeting criteria. D-transposition of the great arteries (D-TGA) with atrial switch operation (20.7%), tetralogy of Fallot/pulmonary atresia (15.5%), and tricuspid atresia (13.8%) were the most common diagnosis for transplant. Single ventricle patients were most likely to be listed for transplantation (39.8% of evaluated patients). Among a comprehensive list of clinical factors, invasive hemodynamic parameters (pulmonary capillary wedge pressure (PCWP), systemic vascular pressure (SVP), and end diastolic pressures (EDP) most correlated with 1- and 10-year outcomes. Transplanted patients with SVP < 14 and non- transplanted patients with PCWP < 15 had 100% survival 1-year post-transplantation.

Conclusion

For the first time, our study identifies that hemodynamic parameters most strongly correlate with 1- and 10-year outcomes in ACHD patients considered for transplantation, using a data-driven machine learning model.","Our study evaluated a comprehensive list of hemodynamic, laboratory, and functional parameters to identify factors that correlate with outcomes in ACHD patients at 1- and 10-year post-decision for transplantation. Transplantation improves survival at 1 and 10 years in ACHD patients. In our cohort, 1-year survival improved 5.15-fold and 10-year survival improved 5.44-fold in ACHD patients who were transplanted as compared to those who were waitlisted and never received transplant.

4.1 Cardiac anatomic predisposition to transplantation

D-TGA and atrial switch operation was the most common CHD diagnosis in our cohort (Table 1). These patients are by far the oldest in the group and have an extensive history of arrhythmia and heart failure (100%) resulting from a systemic right ventricle. As atrial switch operations have been replaced with arterial switch operation, where the left ventricle is systemic, ACHD patients with D-TGA/atrial switch undergoing transplantation is anticipated to decrease in the upcoming years. In the coming decades, this population will be replaced by patients with tetralogy of Fallot, tricuspid atresia (with Fontan physiology) and cc-TGA as the anatomic group most in need of transplantation. TOF is the most common cyanotic CHD and hence reflects the large number of patients with heart failure in need of transplantation. The long-term outcome of repaired TOF patients is largely reassuring with high survival rates, however in patients with significant pulmonary regurgitation, RV dilation, ventricular arrhythmia, biventricular failure and a higher risk of sudden cardiac death7, 8 is likely. The optimal timing for pulmonary valve replacement in these patients is not well understood and likely contributes to their need for transplantation.

Tricuspid atresia is the most common diagnosis in adults with univentricular physiology.9 Patient with Fontan circulation pose unique challenges with transplantation with additional anatomic risk factors including presence of collaterals, pulmonary artery anatomy, and additional comorbidities including PLE contributing to challenges in transplantation. Given the varied anatomies of single ventricle patients, this study was not sufficiently powered to identify factors influencing post-transplant outcomes in single ventricle patients.

4.2 Factors affecting listing parameters and their relationship to survival outcomes

Invasive hemodynamic testing and exercise testing primarily correlated with decision to list. Our model validated criteria that identified patients who were too well to undergo transplantation as they had 100% 1-year survival and 80% 10-year survival. Numerous laboratory parameters including renal function and albumin that influence listing in traditional patients were found to be near normal in our ACHD patients (Table 2). Listing decisions by our multidisciplinary transplant listing team correlated with sternotomies, hemodynamic parameters, and CPET and correlated with outcomes at 1- and 10-years post-decision for transplantation.

4.3 More median sternotomies do not correlate with CHD complexity and outcomes

Surprisingly, patients with ≤2 sternotomies were more likely to die (81.3%) while waitlisted or declined patients - had 100% mortality when placed on advanced therapies. Of these patients 43.8% had a systemic right ventricle either due to atrial switch operation or congenitally corrected-TGA. Multiple factors contribute to this mortality including echocardiographic limitations associated with imaging right ventricles, high predisposition to lethal arrhythmias, and subsequent limitations with using advanced imaging techniques such as cardiac magnetic resonance (cMR) due to presence of devices such as pacemakers and defibrillators. Furthermore, the efficacy of traditional heart failure therapies has limited translation in systemic right ventricles. Additionally, most patients in this class have few functional limitations as children and were discharged from pediatric cardiology services without transition to ACHD providers. Another 31% of patients were single ventricle patients with limited palliation. These patients were notably cyanotic with saturations < 85% and also had pulmonary hypertension as their comorbidities. Both category of patients are the sickest patients in this group with 100% mortality. When advanced heart-failure therapies are not needed their survival improves to 33.3% at 1 year.

Patient with >2 median sternotomies and wedge pressures <15 mmHg have a 100% survival rate at 1 year. Patients in this group have a diagnosis of tetralogy of Fallot or double outlet right ventricle and have a history of surgical revision of conduits and valves. All transplanted patients with SVP < 14 mmHg at the time of listing had a 100% survival at 1-year. These results underscore the importance of continued surveillance of CHD in adulthood with multimodality diagnostic testing with invasive hemodynamic catheterization and CPET.

4.4 Triaging listing status correlates with survival outcomes

As compared to other published studies, a larger percentage of patient were transplanted at our center.10 Crossland et al. represented a European cohort where only 1/3rd of evaluated patients were transplanted. One year outcomes were significantly better as compared to outcomes from the thoracic ISHLT registry which reported a 78.3% survival and comparable to outcomes in patients with ischemic (84.3%) and dilated cardiomyopathy (86.2%).11 Kaplan–Meier survival analysis demonstrated that our post-transplantation group had improved 10-year mortality (69.6%) compared to 59.3% reported by other groups.12 Interestingly, our results showed worse survival (42.1%) in the waitlisted population who awaited transplantation, compared to other studies looking at a similar population which reported a 1-year survival of 89%, 63% at 24 months, and 63% at 48 months.13 These data may reflect a sicker patient population listed for transplantation at our center that is largely driven by single ventricle patients with limited palliation and systemic right ventricle patients with delayed identification for transplantation and late referrals from other centers. As expected, deferred patients who are too well for transplantation have better survival than waitlisted patients; however, patients declined transplantation also have better survival than waitlisted patients. The improved survival in declined patients could be secondary to other criteria such as high allosensitization and social and financial limitations which deems them to be poor transplant candidates.

4.5 One- and 10-year survival in patients largely correlates with intracardiac pressures

Various clinical comorbidities have been shown to predict prognosis in ACHD populations including pulmonary hypertension, restrictive lung disease, anemia, and renal dysfunction.14-17 Exercise capacity testing has also revealed that peak VO2 and VE/VCO2 have been shown to be independent predictors of mortality in ACHD patients.18, 19 While these parameters may help grossly adjust risk quantification, ACHD patients are an extremely heterogenous group and additional parameters are needed to quantify patient specific risk. Right heart catheterization hemodynamic measurements may reflect more individualized parameters to assess short- and long-term prognosis. Unfortunately, there has been a lack of studies specifically analyzing these parameters.

One-year mortality at our center matched national statistics after transplantation in ACHD patients, despite a significantly higher population of single ventricle patients. Listing for heart versus heart +additional organs did not correlate with outcomes. Studies from other centers indicate a multi-organ transplant with increased mortality.20 Instead, ACHD complexity, SVP, and PCWP all were significant predictors of 1 year survival in ACHD patients. In addition, both SVP and wedge pressure were significant predictors of 10-year mortality.

4.6 Study limitation

The small sample size of 58 patients with heterogeneity of diagnosis is a limitation of this study. These parameters must be further validated in larger cohorts. The retrospective nature of our study also limits our ability to validate any prognostic markers. As a single center transplant center, our outcomes may not be reflective of patients presenting at other centers. The study evaluated transplants between 2004 and 2014, which precedes the current UNOS transplantation listing criteria. Hence, intervals between listing and transplantation may be different in the current era. However, as we continue to evaluate the same clinical parameters, the findings of this study are relevant to triaging patients for transplantation. Due to a longer era of atrial switch operations in pediatric patients at our center, we have a higher incidence of patients with Senning/Mustard operations.",https://onlinelibrary-wiley-com.myaccess.library.utoronto.ca/doi/10.1111/ctr.15101
Giannella et al. 2023,Using machine learning to predict antibody response to SARS-CoV-2 vaccination in solid organ transplant recipients: the multicentre ORCHESTRA cohort.,"Objectives
The study aim was to assess predictors of negative antibody response (AbR) in solid organ transplant (SOT) recipients after the first booster of SARS-CoV-2 vaccination.

Methods
Solid organ transplant recipients receiving SARS-CoV-2 vaccination were prospectively enrolled (March 2021–January 2022) at six hospitals in Italy and Spain. AbR was assessed at first dose (t0), second dose (t1), 3 ± 1 month (t2), and 1 month after third dose (t3). Negative AbR at t3 was defined as an anti-receptor binding domain titre <45 BAU/mL. Machine learning models were developed to predict the individual risk of negative (vs. positive) AbR using age, type of transplant, time between transplant and vaccination, immunosuppressive drugs, type of vaccine, and graft function as covariates, subsequently assessed using a validation cohort.

Results
Overall, 1615 SOT recipients (1072 [66.3%] males; mean age±standard deviation [SD], 57.85 ± 13.77) were enrolled, and 1211 received three vaccination doses. Negative AbR rate decreased from 93.66% (886/946) to 21.90% (202/923) from t0 to t3. Univariate analysis showed that older patients (mean age, 60.21 ± 11.51 vs. 58.11 ± 13.08), anti-metabolites (57.9% vs. 35.1%), steroids (52.9% vs. 38.5%), recent transplantation (<3 years) (17.8% vs. 2.3%), and kidney, heart, or lung compared with liver transplantation (25%, 31.8%, 30.4% vs. 5.5%) had a higher likelihood of negative AbR. Machine learning (ML) algorithms showing best prediction performance were logistic regression (precision-recall curve-PRAUC mean 0.37 [95%CI 0.36–0.39]) and k-Nearest Neighbours (PRAUC 0.36 [0.35–0.37]).

Discussion
Almost a quarter of SOT recipients showed negative AbR after first booster dosage. Unfortunately, clinical information cannot efficiently predict negative AbR even with ML algorithms.","Our data confirm the persistence of lack of response in almost one fourth of the patients after booster dose. Using this data, we aimed to develop a prediction models based on easy-to-obtain clinical covariates, such as age, type of transplant, time from transplant to first dosage, types of immunosuppressive drugs, type of mRNA vaccine received, and graft failure. Unfortunately, the best ML model we found only reached a moderate prediction accuracy. This suggests that the clinical covariates provide only limited information.

Our results are consistent with those obtained by Alejo et al. [15], who developed and validated a ML model to predict AbR to two doses of SARS-CoV-2 mRNA vaccines using a nationwide cohort of 1031 SOT recipients, and an external single-centre cohort of 512 SOT recipients in the United States. The authors used 19 clinical factors very similar to those used in our models. Indeed, Alejo et al. found that mycophenolate mofetil use, a shorter time since transplant, and older age were the strongest predictors of a negative AbR. The performance of the model was good in the training set (AUROC, 0.79) and moderate in the external test set (AUROC, 0.67). The main difference between the U.S. cohort and our cohort is the definition of negative AbR (which is < 0.8 U/mL if assessed by Roche and ≤1.1 AU if assessed by EUROIMMUN) used in the U.S. cohort [15]. Alejo et al. used a GBM to predict antibody responses. We found that LR analysis was most accurate in predicting a negative AbR, while tree-based ML models performed worse. A possible explanation for this is that the tree-based methods might have overfitted the training data, as indicated by the Figs. S8–S13, despite being optimized through cross-validation of the hyperparameters (Table S1). This overfitting results in poor generalization performance when applied to the unseen validation cohort in contrast to the not overfitting other models.

Our study has limitations. First, due to the censored structure caused by the detection thresholds of the serology tests, we refrained from reporting and assessing quantitative antibody levels. Second, regarding the type of SARS-CoV2 vaccines (BNT162b2, mRNA-1273, and ChAdOx1), the predictive power of ChAdOx1 could not be assessed due to limited number of subjects exposed. Third, we did not analyse cellular immune response that is an essential component in the clinical protection of SOT recipients from clinically relevant SARS-CoV2 infections. Finally, we developed the model with AbR assessed one month after the first booster dosage, while currently most fragile patients should have received several booster dosages. However, it has been shown that the impact of further booster dosages on AbR may be limited, with lower than 50% of seronegative patients achieving a positive AbR or showing a significant increase in antibody levels [18,19]. Thus, we deem that our model could be valid also in patients exposed to more than one booster dosage.

Although booster dosage in SOT recipients is associated with a progressive increase in AbR, one fourth of this population remains negative or with suboptimal antibody levels. Unfortunately, clinical characteristics are of limited values in developing high performing predictive models of negative AbR.",https://www-sciencedirect-com.myaccess.library.utoronto.ca/science/article/pii/S1198743X2300201X?via%3Dihub#sec4
Cornaby & Weimer 2023,Utilizing principal component analysis in the identification of clinically relevant changes in patient HLA single antigen bead solid phase testing patterns,"Background

HLA antibody testing is essential for successful solid-organ allocation, patient monitoring post-transplant, and risk assessment for both solid-organ and hematopoietic transplant patients. Luminex solid-phase testing is the most common method for identifying HLA antibody specificities, making it one of the most complex immunoassays as each panel contains over 90 specificities for both HLA class I and HLA class II with most of the analysis being performed manually in the vendor-provided software. Principal component analysis (PCA), used in machine learning, is a feature extraction method often utilized to assess data with many variables.

Methods & findings

In our study, solid organ transplant patients who exhibited HLA donor-specific antibodies (DSAs) were used to characterize the utility of PCA-derived analysis when compared to a control group of post-transplant and pre-transplant patients. ROC analysis was utilized to determine a potential threshold for the PCA-derived analysis that would indicate a significant change in a patient’s single antigen bead pattern. To evaluate if the algorithm could identify differences in patterns on HLA class I and HLA class II single antigen bead results using the optimized threshold, HLA antibody test results were analyzed using PCA-derived analysis and compared to the clinical results for each patient sample. The PCA-derived algorithm had a sensitivity of 100% (95% CI, 73.54%-100%), a specificity of 75% (95% CI, 56.30%-92.54%), with a PPV of 65% (95% CI, 52.50%-83.90%) and an NPV of 100%, in identifying new reactivity that differed from the patients historic HLA antibody pattern. Additionally, PCA-derived analysis was utilized to assess the potential over-reactivity of single antigen beads for both HLA class I and HLA class II antibody panels. This assessment of antibody results identified several beads in both the HLA class I and HLA class II antibody panel which exhibit over reactivity from 2018 to the present time.

Conclusions

PCA-derived analysis would be ideal to help automatically identify patient samples that have an HLA antibody pattern of reactivity consistent with their history and those which exhibit changes in their antibody patterns which could include donor-specific antibodies, de novo HLA antibodies, and assay interference. A similar method could also be applied to evaluate the over-reactivity of beads in the HLA solid phase assays which would be beneficial for lot comparisons and instructive for transplant centers to better understand which beads are more prone to exhibiting over-reactivity and impact patient care.","Multiplex solid-phase assays for the detection of HLA antibodies have transformed the field of HLA diagnostics and transplant medicine. Some of the key advantages of this method are the increased sensitivity and specificity for detecting HLA antibodies compared to cytotoxicity and other cell-based assays. As a result of these assays, there has been a sweeping change in tools that highly influence organ allocation, such as the ‘virtual crossmatch’ and ‘calculated panel reactive antibodies’ which have become essential components for center-specific immunological risk stratification for both solid and stem cell transplant, as well as kidney-paired donation programs [16–18]. It is the vital task of the transplant immunology laboratory to identify these HLA antibodies and convey the risk associated with the laboratory findings to the clinical care teams. As the task of identifying HLA antibodies becomes ever more complex, the field would greatly benefit from additional bioinformatic pipelines and software tools to assist laboratory staff in the post-analytical phase of testing [19].

As many solid organs and hematopoietic stem cell transplant patients exhibit a rather complex pattern of HLA antibodies due to various sensitizing events throughout their health care and life experiences, a computer-assisted analysis of these patterns would be ideal. Utilized for variance and pattern analysis by facial recognition software, fiscal evaluation, cyber security risk assessment, and machine learning algorithms, principal component analysis (PCA) presents a possible method for providing analysis of HLA antibody patterns, particularly those with more complex HLA antibody profiles. PCA is a dimensionality reduction technique, which uses feature extraction rather than feature exclusion, making it useful to compare variance among samples across many variables. Our study found that PCA analysis was able to enhance automated HLA antibody analysis for individual patients and assessment of bead over reactivity across a database of patient results.

Several recent manuscripts have been published by a Greece and German research group that utilized PCA to assess HLA single antigen bead data collected across three different histocompatibility centers [20, 21]. In the adult population which they investigated using principal component analysis, they found that this method could identify “patterns in human immune responses with striking similarities with the previously described CREGs” [20]. They demonstrated that anti-DP antigenic responses do not seem to be correlated to anti-DR or -DQ responses, while the latter two do tend to show a correlation based on PCA analysis [20]. In a study published a year later, they describe how through the use of PCA in their machine learning algorithm, they were able to identify several previously identified crossreactive groupings and additional new crossreactive patterns of alloreactivity [21]. A helpful feature of their study gave a detailed analysis of all bead antigenic specificities, represented as a dendrogram, calculating a measure of allelic antigenic distances for these bead-array-defined cross-reactive groups [21].

Our study utilized PCA on a more laboratory application-focused scale to preemptively analyze 24 patient HLA class I and 25 patient HLA class II HLA DSA antibody testing samples and identify samples that had deviant patterns from each patient’s historic antibody profile based on their previous testing results. Using the Euclidean distance ratio derived from principal component scores and the defined threshold which would predict a deviant antibody pattern, we compared the PCA-derived results to the laboratory staff review, testing outcomes, and finalized patient results for the HLA DSA antibody tests performed. PCA-derived analysis was able to identify all samples which had post-transplant DSA (4/4) and all other samples which had noted irregular patterns of reactivity (8/8), including one sample which exhibited a complement-mediated prozone effect. PCA had a sensitivity of 100.00% (95% CI, 73.54%-100.00%) with a negative predictive value of 100.00% across both patient cohorts.

The evaluated specificity was slightly lower at 78.26% (95% CI, 56.30%-92.54%) with a positive predictive value of 70.56% (95% CI, 52.50%-83.90%). This lower positive predictive value was due to five patient samples that PCA-derived analysis identified as deviant from historic samples, but which laboratory staff resulted with no abnormal review remarks, comments, repeat testing, or DSA and which would have been considered consistent with history by staff with no clinical impact. Each of these patient samples was investigated and compared to their historic samples. The results of these comparisons are displayed in Table 2. While there was no large visual difference when looking at the patient antibody histograms and all antibody specificities were in the same order when organized from most reactive to least reactive, there was a difference in the MFI values calculated in these five samples. In each of the five samples, the HLA specificities that historically had been above 1,000 MFIs, all exhibited at least a doubling of their MFI values, with one HLA class II sample being 243% more reactive than the average MFI values measured from the patient’s historic samples. While this increase in sample reactivity might not be clinically impactful in these five cases, cases like these must be identified and investigated as a significant increase in MFI values, as exhibited by these samples, could be clinically impactful in certain instances.

These results demonstrate evidence that PCA-assisted analysis, given the excellent negative predictive value and adequate positive predictive value, could be utilized in assisting laboratory analysis of patient samples. Depending on the laboratory workflow, this analysis could be cumbersome if it added third-party software to the existing workflow. Possibly a software package could be developed that would import the values from the.csv file export from Fusion, enabling PCA analysis of a batched run or individual samples and calculation of the Euclidean distance ratio to determine if the sample results from the run were abnormal compared to the patient’s previous history. However, if the lightweight PCA-based algorithm was added as a feature to existing software that is utilized by HLA laboratories to analyze HLA antibody data, such as HLA Fusion (OneLambda), mTilda (HLA Data Systems), HistoTrac (System Link) and/or MATCH IT (Immucor) among others, it would allow for the automated screening of samples to identify samples with significant variance from their historic antibody pattern and those which are comparable to patient antibody history. This would both simplify analysis for laboratory staff analyzing the testing results and decrease the amount of time necessary to analyze batches of patient samples, likely decreasing sample turnaround time.

An all too common complication in HLA antibody testing is the over-reactivity or false reactivity observed for some HLA single antigen beads [22, 23]. This can be the result of neo-epitope exposure and unspecific binding of serum matrix proteins. As a consequence, some single antigen beads are more prone to false reactivity or over-reactivity compared to other beads in the assays. To evaluate if PCA could provide a method for identifying this increased variance across beads that display a known pattern of over-reactivity, our study analyzed 121 single antigen bead HLA class II sample results which demonstrate overreactive DPA1*02:01-DPB1*01:01 and DPA1*02:02-DPB1*05:01 beads. Results showed that this method could distinguish the two overreactive beads from all of the other beads in the HLA class II single antigen bead panel (S1 Fig). This same method was applied to all single antigen bead HLA class I and HLA class II sample results that had been tested in our laboratory from January 2018 to March 2022 that met the sample inclusion criteria. This was done to investigate if there were beads that consistently exhibit over reactivity across multiple lots.

Our study found that for HLA class I, the beads which exhibited the most over-reactivity included A*29:01, A*29:02, and B*15:12 among others (Table 3). While the most overreactive beads for HLA class II included DRB3*02:02, DPA1*02:01-DPB1*01:01, DPA1*02.02-DPB1*05:01, DQA1*05:05-DQB1*03:01, DRB1*04:01, and DRB1*04:03 beads among other beads (Table 4). These results demonstrate that these beads have a higher variance across the thousands of patient samples tested compared to all of the other beads for the HLA class I and HLA class II single antigen bead assays. This type of analysis is beneficial for the clinical evaluation of patient results. Using this method allows the laboratory to understand which beads have the propensity to demonstrate false reactivity. Once identified, these beads merit closer scrutiny when they may have an impact on patient care.

This method can be applied to each laboratory antibody testing database once antibody MFI values have been exported from the analysis software provided by the vendor. This would provide a customized assessment of what beads display over-reactivity given the laboratory patient population and testing protocol, as certain procedures can reduce bead over-reactivity by using additional washes or treatments. This method could also be utilized to assess bead lots. If there are historic beads with a new recombinant or native HLA protein, HLA complex attachment to the beads using a different chemistry or any other production-related procedure change that could potentially impact bead reactivity upon testing, this method could be utilized to assess the bead lot(s) across one or multiple laboratory test sites.

There is a potential limitation to our study. In identifying differences in patients’ current HLA antibody patterns from historic HLA antibody patterns, we only thoroughly investigated reactivity above 1,000 MFI, since that is the cut-off for reactivity established for the assay and part of our inclusion criteria. However, it is well known that real HLA antibodies can be present and represented in these assays with MFI values below 1,000 MFI due to the spreading of antibodies over multiple beads or as a result of low concentrations in patient serum. The efficacy of this method on patient samples with only low MFI values, below 1,00 MFI, would need to be investigated with further rigor utilizing additional samples with characteristics that would best assess the performance of this method. However, it is the author’s opinion that this approach could yield similar results on samples with real HLA reactivity below 1,000 MFI. Another potential limitation is that our proof of concept study utilized only a single HLA class I and HLA class II single antigen bead run, including only a small number of patients. If the process were built and more automated so that it could be time-saving, it would be beneficial to perform a similar study and include many more patients to more stringently assess the sensitivity and specificity of the method on a larger number of patient samples.

For the implementation of these methods in the clinical laboratory, a package specific for its utility in R or incorporation of this type of method in new or existing HLA antibody analysis software would need to be completed. This would allow for practical testing of a large number of patient samples to further characterize its utility. It would also provide valuable data to demonstrate and delineate the time saved in assessing patient samples as a screening mechanism.

In summary, our study results demonstrate the utility of using PCA-derived analysis to facilitate the assessment of Luminex solid phase HLA antibody testing results. Integrating PCA-derived analysis would enable automated screening of patient samples for HLA antibody testing, identifying samples suspected of deviant HLA antibody patterns compared to previous test results. Such deviances from historic patterns could include resurgent DSA, dnDSA, increased antibody reactivity, and prozone activity. This method could also contribute to providing evidence for, and the identification of, over and/or false reactive single antigen beads in both the HLA class I and HLA class II assays.",https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0288743
Shaw et al. 2023,Multicenter analysis of immunosuppressive medications on the risk of malignancy following adult solid organ transplantation,"Objective: This study aimed to assess the risk of maintenance immunosuppression on the post-transplant risk of malignancy across all solid organ transplant types.
Methods: This is a retrospective cohort study from a multicenter hospital system in the United States. The electronic health record was queried from 2000 to 2021 for cases of solid organ transplant, immunosuppressive medications, and post-transplant malignancy.
Results: A total of 5,591 patients, 6,142 transplanted organs, and 517 post-transplant malignancies were identified. Skin cancer was the most common type of malignancy at 52.8%, whereas liver cancer was the first malignancy to present at a median time of 351 days post-transplant. Heart and lung transplant recipients had the highest rate of malignancy, but this finding was not significant upon adjusting for immunosuppressive medications (heart HR 0.96, 95% CI 0.72 – 1.3, p = 0.88; lung HR 1.01, 95% CI 0.77 – 1.33, p = 0.94). Random forest variable importance calculations and time-dependent multivariate cox proportional hazard analysis identified an increased risk of cancer in patients receiving immunosuppressive therapy with sirolimus (HR 1.41, 95% CI 1.05 – 1.9, p = 0.04), azathioprine (HR 2.1, 95% CI 1.58 – 2.79, p < 0.001), and cyclosporine (HR 1.59, 95% CI 1.17 – 2.17, p = 0.007), while tacrolimus (HR 0.59, 95% CI 0.44 – 0.81, p < 0.001) was associated with low rates of post-transplant neoplasia.
Conclusion: Our results show varying risks of immunosuppressive medications associated with the development of post-transplant malignancy, demonstrating the importance of cancer detection and surveillance strategies in solid organ transplant recipients.","In this multicenter cohort study of 6,142 transplanted organs followed for a median follow-up of 1,903 days, we found significant variations in the risk of malignancy amongst maintenance immunosuppressive medications. Our study highlights several important observations about longitudinal immunosuppression and the risk of malignancy across all types of SOT. The risk of malignancy did not appear to be dependent on the transplanted organ. Additionally, our statistical modeling and machine learning algorithms demonstrate that the risk of subsequent malignancy was segregated by immunosuppressive agents. We observed the highest rate of subsequent malignancy in patients who received azathioprine, followed by cyclosporine and sirolimus. Tacrolimus was associated with the lowest risk of malignancy among the immunosuppressant agents considered.

Prior research has identified higher rates of post-SOT malignancy in heart and lung recipients compared to those receiving kidney or liver transplants (46, 47). However, upon adjusting for immunosuppressive medications, we found no difference in the rates of malignancy between SOT groups, suggesting a potential contribution to neoplasia imparted by the biological effects of the post-transplant immunosuppression (48, 49).

Due to tissue rejection and medication toxicity, a patient’s immunosuppressive regimen often changes (50). By treating immunosuppressive medications as time-varying covariates, we temporally assessed the risk of immunosuppression (24). In our study, azathioprine, cyclosporine, and sirolimus were associated with the highest risk of cancer development. Azathioprine, an antagonist of purine metabolism, has long been associated with the development of cancer in SOT, inflammatory bowel disease, multiple sclerosis, and rheumatoid arthritis (51–55). Within our study, we observed the highest rates of skin cancer with the use of azathioprine. In addition to the direct immunosuppressive effects of azathioprine, this finding is likely magnified due to the role azathioprine plays in DNA synthesis and repair – a key mechanism in the pathogenesis of skin cancer (14, 56).

Consistent with previous studies, cyclosporine was also associated with high rates of skin cancer (57). This finding is partially due to the role cyclosporine plays in the inhibition of ultraviolet-B-induced apoptosis and DNA repair (58). However, higher rates of kidney cancer were also seen with cyclosporine use. Previous studies have demonstrated a cyclosporine dose-dependent risk of malignancy in kidney transplant recipients (59). The higher rates of kidney cancer that we observe may be due to the induction of transforming growth factor-beta by cyclosporine, increasing cellular proliferation, and decreasing differentiation (60–62).

The increased risk of cancer with sirolimus was unexpected as prior studies have generally demonstrated decreased risk with sirolimus use (63–66). However, in concordance with a randomized trial that investigated the risk of malignancy in kidney transplant recipients treated with sirolimus, we found a decreased risk of the development of skin cancer with sirolimus use across all SOT recipients (67, 68). In prior studies of kidney and heart transplant recipients, the transition from a calcineurin inhibitor to sirolimus was associated with a lower risk of malignancy (69, 70), likely due in part to the role mTOR plays in cell proliferation (71). In liver transplant recipients, cumulative exposure to tacrolimus increased the risk of cancer (72). This finding was not unsurprising and not in opposition to our data as we did not assess serum levels of tacrolimus. However, our data demonstrated a lower risk of cancer in individuals who received tacrolimus compared to other immunosuppressive medications.

ML algorithms are now being applied to a variety of SOT research questions (73). To our knowledge, our study represents the first time that a ML model has been used to assess variable importance in determining which SOT recipients developed a malignancy. Random forest classification, a form of decision trees, is a highly flexible, interpretable, and accurate method of estimating non-linear relationships – an area where traditional statistics struggle (74). ML models can be applied to feature selection and outcome prediction. In contrast to traditional statistical methods used in this analysis, our ML model identified age at transplant as a highly predictive marker of post-transplant malignancy diagnosis. However, not all results were dissimilar, as the ML model also identified sirolimus, tacrolimus, azathioprine, and cyclosporine as highly predictive variables.

We consider the internal validity of this investigation to be high as our data are consistent with other landmark studies. For example, our data demonstrated that the majority of liver cancer diagnoses occur within the first year of transplantation – likely due to hepatocellular carcinoma recurrence following transplantation (5, 75). In addition, skin cancer was the most common post-transplant malignancy in our cohort, followed by lymphoma and kidney cancer (46). However, there are several limitations to this study. First, there are likely discrepancies in data entry, collection, and classification that may exist as this was a retrospective cohort study based on ICD codes. We chose to omit a formal control group with the calculation of standardized incidence ratios, focusing instead on the comparison between medications. The rates of malignancy are likely underestimated as we included individuals who received transplants up to the study endpoint. In addition, patients may be lost to follow-up, and subsequent cancer diagnoses may be made outside of the queried hospital system. In addition, the centers in this study are located in the greater Chicago area and thus may not represent results from distinct geographical regions across the United States or in other countries. Compared to national database studies, our sample size is small, preventing the stratification of individual malignancies and association with particular immunosuppression regimens. Furthermore, we did not account for the dose of immunosuppression or blood plasma level of these medications. We also did not control for any pre-transplant-related criteria, including organ ischemic time, viral studies, donor information, screening tests, or education level. This may confound some of the findings that we attribute to sociodemographic factors and immunosuppressive medications. Lastly, our findings do not establish causality but provide further data to underscore the importance of cancer detection and surveillance strategies in organ transplant recipients.",https://www-frontiersin-org.myaccess.library.utoronto.ca/journals/oncology/articles/10.3389/fonc.2023.1146002/full#h5
Killian et al. 2021,Machine learning-based prediction of health outcomes in pediatric organ transplantation recipients,"Objectives
Prediction of post-transplant health outcomes and identification of key factors remain important issues for pediatric transplant teams and researchers. Outcomes research has generally relied on general linear modeling or similar techniques offering limited predictive validity. Thus far, data-driven modeling and machine learning (ML) approaches have had limited application and success in pediatric transplant outcomes research. The purpose of the current study was to examine ML models predicting post-transplant hospitalization in a sample of pediatric kidney, liver, and heart transplant recipients from a large solid organ transplant program.
Materials and Methods
Various logistic regression, naive Bayes, support vector machine, and deep learning (DL) methods were used to predict 1-, 3-, and 5-year post-transplant hospitalization using patient and administrative data from a large pediatric organ transplant center.
Results
DL models did not outperform traditional ML models across organ types and prediction windows with area under the receiver operating characteristic curve values ranging from 0.50 to 0.593. Shapley additive explanations (SHAP) were used to increase the interpretability of DL model results. Various medical, patient, and social variables were identified as salient predictors across organ types.
Discussion
Results showed that deep learning models did not yield superior performance in comparison to models using traditional machine learning methods. However, the potential utility of deep learning modeling for health outcome prediction with pediatric patients in the presence of large number of samples warrants further examination.
Conclusion
Results point to DL models as potentially useful tools in decision-support systems assisting physicians and transplant teams in identifying patients at a greater risk for poor post-transplant outcomes.","Based on the current analyses, although deep learning did not outperform traditional methods, its potential predictive utility should not be underestimated. The data sets in this study were small with a mixed of numeric and categorical variables. Deep learning is known to be “data-hungry” and might not be the best approach for all predictive modeling problems. In a follow-up study on whole UNOS data, we are re-examining the performance of deep learning models in comparison to traditional ML methods in the presence of larger number of samples. Results also represent an important development in the use of ML models with pediatric transplant data as prior research found only poor to fair predictive utility and sensitivity [16]. Similar studies in adult populations have similarly reported lower predictive utility[29–31, 52–54]. Accuracy of DL models here outperformed a recent examination of pediatric liver transplantation using the Studies of Pediatric Liver Transplantation data and a RF decision tree approach to ML [15]. Importantly, DL models offer increased predictive utility through the examination and evaluation of complex relationships among variables as important pathways (e.g., neurons activation) for improving outcome prediction [42]. With gains in the complexity and predictive accuracy, DL models become increasingly complex and challenging to interpret the influence of individual variables, for example. In the current study, SHAP values [48] were examined to aid in the interpretability of DL models and the influence of individual variables within the models.

Medical variables

Primary pretransplant diagnosis and functional status of the patient at TCR and TRR were consistently the most predictive medical factors across prediction windows and organ types. Functional status across organ types and UNOS data collection forms is a measure of the health–related quality of life of the patient. This variable is recorded as a percentage ranging from 10% to 100% with responses related to inpatient status, ability to carry out daily living activities, and evidence of the impact of disease symptoms on daily activities. Primary pretransplant diagnosis includes hundreds of possible diagnoses across the three organ types.

Predictive medical variables for kidney recipients included dialysis status, creatinine levels, received treatment for hypertension at TCR, Kidney Donor Risk Index, and treatment for rejection episodes within 1-year post-transplant. Hospitalization for liver transplant recipients was predicted by the initial or final model for end-stage liver disease (patients age 12 years and older) or pediatric end-stage liver disease (younger than 12 years of age) scores, ascites at TCR, albumin levels at TCR, and treatment of rejection within 6 and 12 months post-transplant. Body mass index, days status 1A (most urgent status on transplant waiting list), use of inotropes with acute decompensated heart failure, and other forms of life support including ventricular assist devices during the pretransplant period predicted hospitalization for heart transplant patients. Most of these variables indicate severity of disease at the time of listing and/or transplantation however use of ML brings a unique set of variables out of the many which demonstrate good predictive ability and can help prognosticate the risk of complications in the post-transplant period.

Importantly, physician- or transplant team-reported noncompliance (UNOS variable of PX_NCOMPLIANT, “Recipient Noncompliant During this Follow-Up Period”) was a variable included on UNOS TRF forms from late 1999 to 2007. Within our data, this variable did not have >50% values and therefore had missing values imputed through MissForest, consistent with bestpractices in ML approaches [37–39]. The removal of this variable despite apparent predictive utility in this data and prior studies [11], and inclusion of adherence in other bigdata studies of pediatric post-transplant outcomes [17], points to the need for inclusion of an adherence measure within UNOS data. Nonadherence is associated with increased number and frequency of hospitalizations, the need for biopsies testing for organ rejection, and even mortality [4, 6–10, 55]. Although in need of continued validation across organ types, the Medication Level Variability Index (MLVI) has shown promise as a measure of nonadherence as higher MLVI scores have predicted poor post-transplant outcomes in samples of pediatric heart, liver, and kidney transplant recipients [4, 56–60]. Inclusion of measures of nonadherence in UNOS TRF forms, such as yearly patient MLVI and other measures of medication adherence, would provide a leap forward in post-transplant monitoring, care, and identification of high-risk patients.

Psychosocial variables

Numerous social, familial, and patient variables were found to be predictive of outcomes through DL models, across prediction windows, and within organ types. Across organ types and prediction windows, patient age, gender, race, and ethnicity each provided predictive utility to the DL models. Age and gender were both identified in numerous post-transplant models as predictive of hospitalizations. Older age at transplant and current patient age have been found to predict lower post-transplant health–related quality of life [61], more rejection episodes (both acuteand chronic) [62–64], and greater mortality riskwhen compared toyounger children across organ type [55, 65]. Female patients have reported poor post-transplant outcomes in studies of pediatric kidney [55, 66, 67] and heart [68] recipients.

Academic progress and level were important predictors in DL models. While these variables are likely highly correlated with functional status, their inclusion in the models suggests additional predictive utility beyond a proxymeasure of health–related quality of life. Social factors identified as predictive of hospitalizations in the current study included race, ethnicity, and socio-economic status (SES). Race/ethnicity and SES have been among the most studied factors predicting post-transplant outcomes and medication adherence [11], and numerous studies have identified significant associations between these social factors and medical outcomes in pediatric kidney, liver, and heart patients [13, 66–68]. Correlates of lower SES such as single-parent households, lower parental education, and receipt of public insurance have been reported as associated with poorer post-transplant outcomes [13, 22, 66, 69].

Limitations and future direction

One of the most important limitations of this study was the sample size. After preprocessing the data and only including the features with lower rates of missing values in the models in this study, the number of remained records in each prediction window of each organ type is pretty limited, especially in the case of heart data sets. DL models usually have more trainable parameters in comparison to the number of samples they are trained on. Thus, in theory, it is suggested to use some form of regularization to control the generalization error and reduce complexity [70]. However, building DL models on larger number of data points can potentially enhance the reliability and generalizability of the models. Also, the majority of features included in the predictive models in this study were categorical features. For the means of predictive modeling, we converted these categorical variables to numerical ones by assigning numbers to each category of each feature. Although the one-hot coding scheme might have been a better choice for the goal of this study, especially in terms of interpretations, the small sample size, the number of categorical variables, and the number of categories in each of these variables would not allow for inducing more sparsity. This would in turn lower the predictive accuracy of the models. Also, for some of the patients a complete record of follow-up data for a specific prediction window might have not been available. Thus, we assumed the outcome will not change in the subsequent years. While the current study serves as a proof of concept and informs the potential advantages of predictive modeling for future works, to address the aforementioned limitations, we are extending this research by modeling the complete UNOS data where we have more data points available. This will allow us to exclude those patients with incomplete follow-up data in each prediction window and have separate training, validation, and test sets for the learning process, hyperparameter tuning, and performance analysis.",https://academic.oup.com/jamiaopen/article/4/1/ooab008/6168494?login=true#284526030
Hong et al. 2022,Personalized Prediction of Kidney Function Decline and Network Analysis of the Risk Factors after Kidney Transplantation Using Nationwide Cohort Data,"We developed a machine-learning-based model that could predict a decrease in one-year graft function after kidney transplantation, and investigated the risk factors of the decreased function. A total of 4317 cases were included from the Korean Organ Transplant Registry (2014–2019). An XGBoost model was trained to predict the recipient’s one-year estimated glomerular filtration rate (eGFR) below 45 mL/min/1.73 m2 using 112 pre- and peri-transplantation variables. The network of model factors was drawn using inter-factor partial correlations and the statistical significance of each factor. The model with seven features achieved an area under the curve of 0.82, sensitivity of 0.73, and specificity of 0.79. The model prediction was associated with five-year graft and rejection-free survival. Post-transplantation hospitalization >25 days and eGFR ≥ 88.0 were the prominent risk and preventive factors, respectively. Donor age and post-transplantation eGFR < 59.8 were connected to multiple risk factors on the network. Therefore, careful donor–recipient matching in older donors, and avoiding pre-transplantation risk factors, would reduce the risk of graft dysfunction. The model might improve long-term graft outcomes by supporting early detection of graft dysfunction, and proactive risk factor control.","4.1. Principal Findings
One-year eGFR < 45 mL/min/1.73 m2 was successfully predicted using seven factors (donor and recipient ages, low and high eGFR levels after transplantation, high serum creatinine after transplantation, height difference between the donor and recipient, and post-transplantation stay), and the predicted decline in one-year eGFR was related to long-term allograft outcomes. Two risk factors, donor age and low eGFR after transplantation, were noticeable on the network because of their multiple positive connections to other risk factors.
4.2. Improving the Performance of the XGBoost Model
A higher performance (AUC = 0.8) of the model was obtained when predicting the one-year eGFR < 45 mL/min/1.73 m2 than in the previous report (AUC = 0.7) [13]; however, the current and previous studies could not be directly compared because of the difference in development data size (4317 vs. 707), transplantation period (2014–2019 vs. 1998–2008), ethnicity (Asian vs. European), donor type (living and deceased vs. deceased only), and machine learning type (classifier vs. regressor). However, the following techniques were used to improve the model performance.
4.2.1. Automated Machine Learning
A developer configures the XGBoost hyperparameters to obtain the best performance, and the results can vary according to the developer’s experience [17]. This study used the GPU-accelerated TPOT, one of the most effective automated machine learning tools. TPOT uses genetic algorithms to find the best combination of hyperparameters, and GPUs increase the chance that TPOT finds more advanced combinations by accelerating its computation time [23].
4.2.2. Addressing the Imbalanced Classification Problem
The minority class is more difficult to predict because a model may focus on learning the characteristics of the abundant cases from the majority class [24]. This study had a moderate imbalance problem of 11% of the one-year eGFR < 45 mL/min/1.73 m2 (positive class), which was addressed using class-weighted XGBoost [24]. In detail, the model was eight times more likely to correct errors in the positive class, and successfully achieved balanced sensitivity and specificity.
4.3. Clinical Relevance of Discretized Factors
Some numerical variables were discretized to categorical variables, and the data-driven discretization criteria had clinical relevance. For example, eGFR < 59.8 and ≥ 88.0 mL/min/1.73 m2 correspond to GFR categories in chronic kidney disease, that is, “mildly to moderately decreased” (GFR 45–59, G3 or higher category) and “normal or high” (GFR ≥ 90, G1 category), respectively [25]. For post-transplantation stay >25 days, longer than 14 days is a risk factor of five-year mortality, and longer than 30 days was reported in 1% of recipients [26].
4.4. Factors Associated with a Decline in One-Year Renal Allograft Function
4.4.1. Donor Age, the Most Influential Factor in the Model Prediction
Donor age has been reported as the most critical factor governing graft survival and patient mortality after kidney transplantation [1]. “Donor age” had long tails in low and high SHAP values on the summary plot, which meant that the probability varied considerably with age (Figure 2). Age > 70 years required special attention because of a dramatic increase in the probability of one-year eGFR decrease in this group (Figure 2), and this finding has been consistently demonstrated [27,28,29].
4.4.2. eGFR at Discharge, the Most Statistically Significant Factor
The factor eGFR at discharge ≥88.0 mL/min/1.73 m2 showed the strongest association with a one-year eGFR decrease, and eGFR at discharge <59.8 mL/min/1.73 m2 was a potent risk factor (Table 2 and Figure 2). The KDIGO guideline recommends daily measurement of serum creatinine, and calculation of eGFR until hospital discharge after kidney transplantation to detect acute changes in renal allograft function, such as acute rejection, obstruction, urine leak, vascular compromise, and recurrent infection diseases, which are more common early after transplantation [30].
4.4.3. Other Significant Model Factors
“Post-transplantation stay >25” was a significant risk node, and positively connected to “eGFR at discharge <59.8” on the factor network, which means that hospitalization length might be associated with early renal allograft function (Figure 2 and Figure 4). Prolonged hospitalization can be caused by early renal allograft dysfunction, and patients with poor fitness or health for transplantation tend to have an extended length of stay during transplant admission [1,30].
A tall recipient for a short donor was another significant risk factor (Table 2 and Figure 2). Since body height is correlated with kidney length, the results imply that a relatively small donor kidney may not be sufficient for a tall recipient [31,32]. An explanation for the kidney size mismatch problem is nephron dosing, suggesting that the graft size should match the recipient’s physiologic demand to prevent renal damage by glomerular hyperfiltration.
A recipient age under 29 years or over 57 years was significantly related to a decline in one-year eGFR (Table 2). A recipient age ≥ 60 years is a risk factor for one-year eGFR decline, and affects the five-year eGFR by lowering one-year eGFR [10]. In addition, previous studies have demonstrated that renal allograft survival is the worst in young recipients between 10 and 20 years of age [33,34]. It should also be noted that the proposed network showed that the recipient age < 29 years was associated with high eGFR at discharge, whereas the recipient age > 57 years was related to low eGFR at discharge (Figure 4). Accordingly, low and high recipient ages would have different reasons for a one-year eGFR drop. The increased risk of graft failure in young adults is partly explained by non-adherence to immunosuppressive medications, and vulnerability to graft rejection [35,36].
4.4.4. Significant Non-Model Factors
Two significant non-model factors, the female recipient and the number of HLA mismatches, were included as a covariate and known factor, respectively. Female recipients show higher graft failure risks than male recipients, especially when their donors are male, presumably because of sex-determined minor histocompatibility antigens, or the influence of sex hormones on immune activation [37,38]. HLA mismatch is a known risk factor for low one-year eGFR [10] and graft loss one-year post-transplantation [39].
4.5. Application of Factor Network for Finding Control Targets and Confounders
Two significant risk nodes, “eGFR at discharge <59.8” and “Donor age”, had multiple positive connections to other risk factors in the network (Figure 4). These connections explained the increase in the factors’ unadjusted odds ratios to 2.2 and 1.4 times their adjusted odds ratios, respectively. Therefore, controlling these two risk factors might considerably reduce the risk of declining one-year eGFR.
For “eGFR at discharge <59.8”, there might be a causal relationship with pre-transplantation risk factors, such as “Donor age”, “Female recipient”, and “Recipient age > 57”, which occurred temporally before “eGFR at discharge <59.8” (Figure 4). For example, a graft from an older donor reduced the number of viable nephrons, and led to early graft dysfunction [40].
“Donor age” was positively connected to “Female recipient”, “Recipient age > 57”, “Height(R)−Height(D)”, and “Number of HLA mismatches”, which means that care should be taken when matching a recipient with an older donor (Figure 4). For example, better HLA matching offsets the disadvantages of the older donor in deceased donor kidney transplantation [41]. Lastly, the positive connection between “Donor age” and “eGFR at discharge <59.8” should be emphasized because donor age could affect the one-year eGFR and long-term outcome through low eGFR at discharge (Figure 3 and Figure 4).
The factor network revealed that two known risk nodes, “Deceased donor” and “Delayed graft function”, could be confounded by “eGFR at discharge <59.8” and “Post-transplantation stay >25” through their positive connections (Figure 4). For a deceased donor, a prolonged ischemia–reperfusion injury is usually an inevitable event affecting early kidney allograft function [42]. The injury also causes delayed graft function (see the positive connection between nodes 9 and 12 in Figure 4), which is reported to be associated with a decreased eGFR at discharge, and increased length of stay after transplantation [26,43].
4.6. Limitations
First, the relationships among the factors on the network should be considered associative, not causal. However, causal relationships between factors can be suggested if they had temporality and plausibility [44]. Second, the results may change because of the different ethnicities of the recipients and donors, and the discharge policies after transplantation. Third, there might be unfound pre-, peri-, and post-transplantation factors, such as medication adherence, that would significantly alter the one-year eGFR [13].",https://www.mdpi.com/2077-0383/11/5/1259
Pettit et al. 2023,The utility of machine learning for predicting donor discard in abdominal transplantation,"Background

Increasing access and better allocation of organs in the field of transplantation is a critical problem in clinical care. Limitations exist in accurately predicting allograft discard. Potential exists for machine learning to provide a balanced assessment of the potential for an organ to be used in a transplantation procedure.

Methods

We accessed and utilized all available deceased donor United Network for Organ Sharing data from 1987 to 2020. With these data, we evaluated the performance of multiple machine learning methods for predicting organ use. The machine learning methods trialed included XGBoost, random forest, Naïve Bayes (NB), logistic regression, and fully connected feedforward neural network classifier methods. The top two methods, XGBoost and random forest, were fully developed using 10-fold cross-validation and Bayesian optimization of hyperparameters.

Results

The top performing model at predicting liver organ use was an XGBoost model which achieved an AUC-ROC of .925, an AUC-PR of .868, and an F1 statistic of .756. The top performing model for predicting kidney organ use classification was an XGBoost model which achieved an AUC-ROC of .952, and AUC-PR of .883, and an F1 statistic of .786.

Conclusions

The XGBoost method demonstrated a significant improvement in predicting donor allograft discard for both kidney and livers in solid organ transplantation procedures. Machine learning methods are well suited to be incorporated into the clinical workflow; they can provide robust quantitative predictions and meaningful data insights for clinician consideration and transplantation decision-making.","In this study, we set out to test and develop the capabilities of machine learning to accurately predict organ use and discard in liver and kidney transplantation procedures. There is no current standard for defining organ quality. The ability to accurately predict whether an organ is fit for transplantation could serve to increase utilization and benefit patients. This can include using marginal quality allografts, that are likely to be discarded, for desperate immediate need patients. Using the UNOS data on all deceased donors from 1987 to the present, we tested the capabilities of six machine learning methods for predicting kidney and liver utilization versus discard from donor offers. Ultimately the random forest machine learning and XGBoost machine learning methods performed the best on the metrics of AUC-ROC compared against a NB model, a feedforward neural network, and logistic ridge regression methods. This comparison was conducted with five-fold cross validation. The two top performing methods, while similar on a smaller subset of the data, differentiated after undergoing a full ten-fold cross validation on all available data and using a Bayesian optimizer of model hyperparameters. The top performing model for predicting kidney organ use was the XGBoost model which achieved an AUC-ROC of .952. The XGBoost method also delivered the top performing model in predicting liver organ use, achieving an AUC of .925.

With this performance, these models represent the best available predictive models of organ use for kidney and liver transplantation to date. Prior to this study, the abilities of machine learning methods for predicting organ utilization were limited. In 2021, authors Barah et al. conducted a comparison of machine learning methods for predicting kidney discard from the UNOS data from 2014-2019. They found a random forest method to be the top performing model, with an AUC-ROC of .888.44 They did not trial the XGBoost method, and our finding of an overall AUC-ROC on all deceased donor data of .952 using XGBoost is a significant improvement. While many metrics related to liver utilization have been modeled with machine learning methods, including assessing macrosteatosis45 and drug-induced liver injury,46 we identified only one directly similar machine learning study assessing liver allograft utilization.47 In the 2021 study by authors Bishara et al.47 machine learning was used to predict allograft utilization from the national donor management goals registry48 and found that a gradient-boosted machine learning classifier, similar to XGBoost, achieved an AUC-ROC of .84 for this task. In our study, the XGBoost method was able to achieve an AUC-ROC of .925 overall for predicting organ discard using these UNOS data. Pushing AUC-ROC above the .9 threshold to now represents excellent classifier performance,49 and we present it to be definitively useful as a quantitative marker to consider in the clinical decision-making process.

Smaller, simpler, stand-alone metrics to evaluate organ utilization and procedural risk are established and have been validated.50 The kidney donor risk index (KDRI51) and liver donor risk index (LDRI52) are organ-specific donor risk indices which are utilized to asses post-transplantation graft survival.50 The KDRI51 has undergone iteration, and currently predicts by combining 21 donor variables including age, race, height, weight, creatinine, cause of death, HCV+ diabetes, hypertension, and smoking duration. In 2020 it was evaluated and determined to have an independent c-statistic (AUC-ROC) of .652 for predicting organ use.53 The LDRI52 was created to similarly assess liver graft failure and risk, and uses the parameters of age, race, donor height, cause of death, and type (partial vs. whole liver) of donation. The AUC-ROC of the LDRI has been estimated to be.54 and, despite this limited overall discriminative ability, it has been useful for examining and refining the organ allocation process.55, 56 While larger and currently more difficult to implement, there is a clear advantage to using machine learning over these risk indices given their significantly greater predictive capabilities.

In this study, we report the top 25 most important features for consideration in assessing liver and kidney use, as identified by our top performing XGBoost models. Interestingly, the KDRI calculated score for each donor was an independent highly important feature for predicting both liver and kidney organ use. While the KDRI score is calculated on donors to assess kidney risk, it is calculated for all deceased donors and available at the time of liver harvesting and transplantation decision making as well. It is interesting to observe that it is the fourth most important feature as identified by the XGBoost liver discard model and the first for the kidney model. It is a suitable candidate for consideration as a standalone metric for liver use and could be incorporated into future predictive models. Other features of interest for liver use include the cc's per hour of IV fluids the donor was receiving at procurement, as well as several blood and metabolic markers. Blood, organ sizing, and several metabolic markers were similarly present as top features and were useful for predicting kidney discard. Notably, using only these top 25 features to predict organ discard resulted in useful ML models. The advantage of using all features increased the liver use XGBoost model to an AUC-ROC of .925, compared to .852 with the 25-feature XGBoost model. Similarly for predicting kidney use the full feature XGBoost model achieved an AUC-ROC of .952, while the 25 feature only XGBoost model .913. An argument can be made that, while slightly inferior in predictive performance, the 25-feature model could serve for real time organ evaluation via manual clinical team input of donor values, in an online version of the tool. Further minimal feature models should be evaluated, as they could be easy to implement, interpretable, and be of near immediate clinical utility. In addition to the full feature model being included within the electronic health record, a condensed model could change day to day practice with manual input during the clinical decision-making window.

In the five-fold cross validation comparison of machine learning methods, it is interesting to see the performance achieved by models compared to the NB model. A NB model treats all variables independently and combines their independent predictive powers towards creating a single prediction of the outcome of interest. The capability of this model serves to represent how well we can predict an outcome simply by understanding all its independent components. A significant increase in predictive capabilities is observed using the XGBoost or random forest models over the NB model, highlighting the improvement in modeling for our specific task through nonlinear associations and multi parameter co-appreciation utilized with these machine learning methods. Greater insight exists than can be gained by appreciating independent variables alone. Nonlinear associations exist in these deceased donor UNOS data which are most appropriately modeled by methods which capture this nuance.

This study has limitations. One major limitation relates to data recording. The deceased donor data available from UNOS only includes donor-specific metrics for donors with at least one organ ultimately recovered. Potential donors where no organ was recovered are not present in these data. This partially explains the imbalance ratio observed in the data set where the positive class organ use doubles or quadruples the organ discard class for our liver and kidney training datasets, respectively. It can be expected that if any organ was recovered from each of our donors, then our data sample may be from an overall healthier cohort than what would be observed in the general population. Further validation of these models at a single institution, where all organ offers are tested for prediction versus actual results, would be a useful next step in machine learning prediction accuracy validation. Unfortunately, such validation data is not publicly available at the time of this study.

In our study we perform an initial comparison of machine learning methods to assess their general appropriateness for addressing our problem of interest: predicting organ discard. We do not claim the investigation of the seven machine learning methods to be comprehensive or complete, but rather an initial estimate which allowed us to move forward with top performing models. For example, we used logistic lasso57 and ridge regression due to its advantage of shrinking but not eliminating features of interest; however,57 “elastic net58” regression could also be trialed. We only tested one fully connected feedforward neural network architecture, which does not represent the full array of various architectures, regularization, techniques, and optimizers which could be utilized. However, given machine learning historic performance of equivalency or superiority on tabular datasets59 such as the one that we have, as well as the significant improvement observed initially by machine learning models in our results, we felt comfortable moving forward with developing the random forest and XGBoost models and not pursuing further neural network architectures. Finally, decision tree-based models are generally more interpretable than deep learning models, as they maintain input feature representations,59 which would add value for this clinically oriented classification task.

This study also has some practical limitations. In our models, we use all available deceased donor UNOS features. We do not, however, perform manual feature construction combining variables of clinical relevance which could afford room for improvement. Further, we do not remove highly correlated variables, such as height, weight, and BMI, and allow these variables to exist concurrently within these data. These models were only generated using deceased donor data and did not incorporate living donor instances. The logic behind the decision to exclude the living donor data from this analysis included a desire to avoid additional “healthy patient bias” as well as minimize trend dilution that may exist from tailored recipient selection which occurs with living donor allocations. An additional limitation stems from our generation of our outcome target variables. Per organ type, if the organ disposition variable was had a value of 1 or 2, these instances were excluded. This removes instances where the patient was not consented, or patient consent was requested but ultimately not obtained. We viewed such instances to be either clerical omissions or less reproducible occurrences. However, this may be an overly conservative assumption. We also note the lack of a suitable external validation data set from these UNOS deceased donor data to be present at the time of these analyses to validate our model performance externally. Finally, we note that these models are retrospective in analysis, and the tools have not yet been applied prospectively at the point of care to evaluate their success at predicting discard, or to evaluate their utility for marginal allograft decision making.

Despite these limitations, we do present the notable performance of the XGBoost method at predicting organ use allocation from the deceased donor data available in the UNOS database. We advocate for machine learning methods to be incorporated as software to provide both robust quantitative predictions as well as meaningful data insights in the form of feature importance's. These results can be available for clinician consideration at the point of transplantation decision making. Further research is warranted to optimize and perhaps even simplify these models to facilitate this incorporation. Directions for future work include removing correlated or low feature importance variables to attain a minimalist XGBoost model to discern how much tradeoff in performance is observed. If minimal, perhaps an interpretable model could be generated that would further facilitate ease of clinical adoption and use. Research should also move forward with developing models or validating models of organ use which also utilizes living donor data.",https://onlinelibrary-wiley-com.myaccess.library.utoronto.ca/doi/10.1111/ctr.14951
Bae et al. 2022,"mTOR inhibitors, mycophenolates, and other immunosuppression regimens on antibody response to SARS-CoV-2 mRNA vaccines in solid organ transplant recipients","A recent study concluded that SARS-CoV-2 mRNA vaccine responses were improved among transplant patients taking mTOR inhibitors (mTORi). This could have profound implications for vaccine strategies in transplant patients; however, limitations in the study design raise concerns about the conclusions. To address this issue more robustly, in a large cohort with appropriate adjustment for confounders, we conducted various regression- and machine learning-based analyses to compare antibody responses by immunosuppressive agents in a national cohort (n = 1037). MMF was associated with significantly lower odds of positive antibody response (aOR = 0.090.130.18). Consistent with the recent mTORi study, the odds tended to be higher with mTORi (aOR = 1.001.452.13); however, importantly, this seemingly protective tendency disappeared (aOR = 0.470.731.12) after adjusting for MMF. We repeated this comparison by combinations of immunosuppression agents. Compared to MMF + tacrolimus, MMF-free regimens were associated with higher odds of positive antibody response (aOR = 2.394.267.92 for mTORi+tacrolimus; 2.345.5415.32 for mTORi-only; and 6.7810.2515.93 for tacrolimus-only), whereas MMF-including regimens were not, regardless of mTORi use (aOR = 0.811.542.98 for MMF + mTORi; and 0.811.512.87 for MMF-only). We repeated these analyses in an independent cohort (n = 512) and found similar results. Our study demonstrates that the recently reported findings were confounded by MMF, and that mTORi is not independently associated with improved vaccine responses.","In this analysis of 1549 solid organ transplant recipients, MMF showed a strong association with lower odds of positive antibody response after SARS-CoV-2 mRNA vaccines, which far outweighed the association of mTORi or other immunosuppressive agents with antibody response. While mTORi showed a tendency toward higher odds of positive antibody response (aOR = 1.45), this protective tendency disappeared after adjusting for MMF (aOR = 0.73). When compared by regimens (i.e., combinations of the agents), MMF-free regimens were associated with higher odds of positive antibody response, whereas MMF-including regimens were not, regardless of mTORi use. Lastly, MMF accounted for 37.4% of a machine learning model’s ability to predict antibody response, whereas no other immunosuppressive agents did for >3%. Our findings demonstrate that MMF avoidance, but not mTORi use, is independently associated with improved antibody responses to SARS-CoV-2 mRNA vaccines in transplant recipients.

Netti and colleagues10 recently concluded that mTORi was associated with improved antibody responses to BNT16b2. In this study, the authors compared 28 kidney transplant recipients who received everolimus + tacrolimus + prednisolon to 108 kidney transplant recipients who received MMF + tacrolimus + prednisolon and observed that the everolimus group was associated with a positive antibody response compared to the MMF group (e.g., aHR = 1.5314.25411.816). Based on these findings, the authors concluded that their result “underlines the potential beneficial role of mTOR inhibitors to enhance the immunogenicity of mRNA BNT162b2 vaccine in kidney transplant recipients”.

On the one hand, our findings are highly congruent with those of Netti and colleagues.10 In our study, the recipients who received mTORi+tacrolimus had significantly higher odds of positive antibody response compared to the recipients who received MMF + tacrolimus (Table 3; aOR = 2.394.267.92). However, more importantly, we also found that1 the beneficial association of mTORi with antibody responses disappeared after adjusting for MMF,2 MMF-free regimens showed superior antibody responses to MMF-including regimens regardless of mTORi use, and3 MMF had substantially greater variable importance than mTORi or any other agents had in a machine learning-based prediction model. These results were replicated in an independent secondary cohort. Our findings support that MMF avoidance, rather than mTORi use, would have been the primary factor that led to Netti and colleague’s results.

Due to the observational design of this study, the associations observed in our study do not warrant causal effects. Clinical factors that influence both the exposure (the selection of immunosuppressive regimen) and the outcome (vaccine efficacy) might have confounded the associations observed in our study. To mitigate this concern, we adjusted for age, time since transplant, and vaccine type, which were the major risk factors for negative antibody response.

In conclusion, our three analytical approaches to two independent cohorts of 1549 solid organ transplant recipients have consistently indicated that MMF avoidance, rather than mTORi use, would be the key determinant of the immunogenicity of SARS-CoV-2 mRNA vaccines in this population. As sustaining sufficiently high antibody titer is a central part of protection against SARS-CoV-2 and its variants, continued research on this topic is warranted.",https://www-sciencedirect-com.myaccess.library.utoronto.ca/science/article/pii/S1600613523000667?via%3Dihub#s0080
van Baardwijk et al. 2022,A Decentralized Kidney Transplant Biopsy Classifier for Transplant Rejection Developed Using Genes of the Banff-Human Organ Transplant Panel,"Introduction: A decentralized and multi-platform-compatible molecular diagnostic tool for kidney transplant biopsies could improve the dissemination and exploitation of this technology, increasing its clinical impact. As a first step towards this molecular diagnostic tool, we developed and validated a classifier using the genes of the Banff-Human Organ Transplant (B-HOT) panel extracted from a historical Molecular Microscope® Diagnostic system microarray dataset. Furthermore, we evaluated the discriminative power of the B-HOT panel in a clinical scenario.
Materials and Methods: Gene expression data from 1,181 kidney transplant biopsies were used as training data for three random forest models to predict kidney transplant biopsy Banff categories, including non-rejection (NR), antibody-mediated rejection (ABMR), and T-cell-mediated rejection (TCMR). Performance was evaluated using nested cross-validation. The three models used different sets of input features: the first model (B-HOT Model) was trained on only the genes included in the B-HOT panel, the second model (Feature Selection Model) was based on sequential forward feature selection from all available genes, and the third model (B-HOT+ Model) was based on the combination of the two models, i.e. B-HOT panel genes plus highly predictive genes from the sequential forward feature selection. After performance assessment on cross-validation, the best-performing model was validated on an external independent dataset based on a different microarray version.
Results: The best performances were achieved by the B-HOT+ Model, a multilabel random forest model trained on B-HOT panel genes with the addition of the 6 most predictive genes of the Feature Selection Model (ST7, KLRC4-KLRK1, TRBC1, TRBV6-5, TRBV19, and ZFX), with a mean accuracy of 92.1% during cross-validation. On the validation set, the same model achieved Area Under the ROC Curve (AUC) of 0.965 and 0.982 for NR and ABMR respectively.
Discussion: This kidney transplant biopsy classifier is one step closer to the development of a decentralized kidney transplant biopsy classifier that is effective on data derived from different gene expression platforms. The B-HOT panel proved to be a reliable highly-predictive panel for kidney transplant rejection classification. Furthermore, we propose to include the aforementioned 6 genes in the B-HOT panel for further optimization of this commercially available panel.","In this study, we developed a decentralized molecular kidney transplant biopsy classifier to improve the diagnosis of transplant rejection and subsequently improve the classification of the type of rejection, namely ABMR and TCMR. Moreover, we compared the discriminating power of the B-HOT panel to that of forward sequential feature selection applied to the whole microarray gene set and found 6 additional predictive genes that increase the B-HOT panel performances in a clinical scenario. The resulting B-HOT+ model achieved an average accuracy of 0.921 during cross-validation and AUCs of 0.965 and 0.982 for NR and ABMR respectively on an external validation set.

To our knowledge, we developed the first kidney transplant biopsy classifier that is able to classify NR and ABMR samples within a validation set derived from a different analysis platform, in this case, a different microarray version. This is a large step towards the development of a thoroughly validated and multi-platform compatible model that will require a coordinated and organic effort from multiple members of the research community. The MMDx is the main currently available molecular diagnostic system for transplant rejection. Its strength relies on its thoroughly validated gene-set and centralized analysis pipeline. This system strongly advanced research in the seek for a reliable molecular diagnostic system for allograft pathology, but its centralized nature limits its availability and clinical impact. Further research is needed for the development of a decentralized molecular diagnostic system for allograft pathology and general agreements over specific methodologies, such as predefined gene-sets or analysis pipelines, might be of benefit to produce more coherent and comparable literature, speeding up the development process.

In our study, the B-HOT panel appeared to be a high-quality and reliable gene-set for the classification of kidney transplant biopsies, supporting the findings of other research groups investigating the discriminatory value of this knowledge-based panel for kidney transplant diagnostics (15). An alternative model built with feature selection performed over the whole gene-set failed to outperform the B-HOT panel-based model. The superior performances of the B-HOT panel compared to this technique reinforce our will to support and implement its use. We consider the B-HOT panel an important milestone of the Banff Foundation and the whole transplantation community, and further studies are needed to optimize its composition and clinical impact.

The six additional genes (CST7, KLRC4-KLRK1, TRBC1, TRBV6-5, TRBV19, and ZFX) derived from the Feature Selection model improved the predictive power of the B-HOT panel. These genes appeared to be involved in the regulation and action of the immune function, suggesting that their additional predictive power is unlikely to be due to chance. CST7 and ZFX act on hematopoietic cell precursors, regulating the immune function (16, 17). KLRC4-KLRK1 takes part in the innate immune function and regulates natural killer cells (18). TRBC1, TRBV6-5, and TRBV19 are all genes coding for T cell receptor structures (19). All things considered, performances improved when these genes were added to the current B-HOT panel, suggesting that they should be further investigated and possibly considered for the optimization of the B-HOT panel itself.

This study has several limitations. First of all, the retrospective nature of this study limits the strength of its findings. Furthermore, the GSE98320 training set is composed of 1,181 biopsy samples with corresponding expression levels and MMDx archetype diagnoses. However, our validation set GSE129166 reports histological diagnoses (Banff diagnostic categories), displaying the validity of the MMDx archetypes and again raising the question about what we should consider as ground truth considering the high interobserver variability that characterizes histological renal allograft diagnoses. Finally, our validation set presents only 2 TCMR samples, preventing us from evaluating our model’s performances concerning that diagnostic category. However, gene expression analysis support for ABMR diagnosis has been long-awaited by the community, considering the drawbacks and complexities of ABMR histopathological diagnosis and subtyping. For this reason, our main focus was on the validation of performances for the diagnostic category of ABMR.

The development of the commercially available B-HOT panel is a large achievement of the Banff Foundation for Allograft Pathology in collaboration with NanoString®. Subsequently, the integration of a multi-platform compatible diagnostic system to diagnose kidney allograft pathology with this technology should be a priority. Multi-platform compatibility is vital as single-platform models could limit the availability of newly developed diagnostic tools. Although the price for a NanoString® sample run is lower than the ones of many other gene expression analysis technologies, the use of NanoString® could be demanding for what concerns set-up and use. RNA in formalin-fixed paraffin-embedded biopsies is prone to degradation. Therefore, the NanoString® technology is very suitable for this type of sample and can drastically increase the possibility of obtaining large datasets. Furthermore, gene expression data from different technologies already showed to be highly correlated (20, 21). A multi-platform compatible system will allow different centers to use already available gene expression analysis systems. For this to be possible, specific normalization pipelines for different gene analysis technologies are vital to combine different datasets, virtually expanding the available data for the development of the diagnostic system itself.

In conclusion, we developed a molecular diagnostic model for renal allograft pathology that is able to work on different microarray versions, taking a large step in the final development of a decentralized multi-platform compatible system that could strongly influence clinical practice and outcome. Further research is needed, especially focusing on the complex normalization pipelines that are required to compare gene expression data generated by different technologies. The development of this tool must combine the efforts of the whole transplantation community for the validation of the B-HOT panel, making sure that optimal performances are achieved through the use of this technology. The current B-HOT panel proved to be an extensive and reliable gene-set for kidney transplant biopsy classification, however, we found that the addition of 6 genes could lead to a superior B-HOT+ model to classify transplant rejection.",https://www-frontiersin-org.myaccess.library.utoronto.ca/articles/10.3389/fimmu.2022.841519/full#h5
Min et al. 2022,An Integrated Clinical and Genetic Prediction Model for Tacrolimus Levels in Pediatric Solid Organ Transplant Recipients.,"Background. 
There are challenges in achieving and maintaining therapeutic tacrolimus levels after solid organ transplantation (SOT). The purpose of this genome-wide association study was to generate an integrated clinical and genetic prediction model for tacrolimus levels in pediatric SOT.

Methods. 
In a multicenter prospective observational cohort study (2015–2018), children <18 years old at their first SOT receiving tacrolimus as maintenance immunosuppression were included (455 as discovery cohort; 322 as validation cohort). Genotyping was performed using a genome-wide single nucleotide polymorphism (SNP) array and analyzed for association with tacrolimus trough levels during 1-y follow-up.

Results. 
Genome-wide association study adjusted for clinical factors identified 25 SNPs associated with tacrolimus levels; 8 were significant at a genome-wide level (P < 1.025 × 10−7). Nineteen SNPs were replicated in the validation cohort. After removing SNPs in strong linkage disequilibrium, 14 SNPs remained independently associated with tacrolimus levels. Both traditional and machine learning approaches selected organ type, age at transplant, rs776746, rs12333983, and rs12957142 SNPs as the top predictor variables for dose-adjusted 36- to 48-h posttacrolimus initiation (T1) levels. There was a significant interaction between age and organ type with rs776476*1 SNP (P < 0.05). The combined clinical and genetic model had lower prediction error and explained 30% of the variation in dose-adjusted T1 levels compared with 18% by the clinical and 12% by the genetic only model.

Conclusions. 
Our study highlights the importance of incorporating age, organ type, and genotype in predicting tacrolimus levels and lays the groundwork for developing an individualized age and organ-specific genotype-guided tacrolimus dosing algorithm.","Current practice guidelines for genotype-guided tacrolimus starting dose after organ transplant do not take into consideration interacting clinical factors that can influence tacrolimus levels. Our study used an unbiased approach to identify and validate known and novel SNPs that were independently associated with tacrolimus levels posttransplant after adjusting for clinical variables. The top-ranked clinical and genetic variables explained 30% of the variability in tacrolimus starting levels (T1). A combined model incorporating clinical and genetic variables had lower prediction error for dose-adjusted T1 levels compared with the clinical or SNP only models, and there were performance differences by organ subtype.

Overall, our study has several important findings. Unlike previous studies that were mostly in adults and in kidney recipients, our study used a GWAS approach to pharmacogenetic discovery that included all pediatric solid organ transplant recipients. This allowed the identification of additional SNPs on chromosome 7 (on CYP3A4, CYP3A5, CYP3A7, and CYP3A7-CYP3AP1 genes) beyond rs776746 (CYP3A5) and also allowed the development of an integrated prediction model that included organ-specific differences. It highlights the importance of moving beyond only a genotype-guided dosing to using an integrated clinical and genetic prediction model to inform individualized tacrolimus dosing.

Besides confirming an association of previously reported factors with tacrolimus levels including organ type, age, tacrolimus dosage, and concomitant use of a CYP3A4 inhibitor drug,1,3,21,22 a GWAS adjusted for these factors identified known and new variants associated with tacrolimus levels with a cluster located on chromosome 7 that mapped to the CYP3A family of pharmacogenes.7,23 The most significant SNP was rs776746, an established pharmacogenetic SNP associated with tacrolimus levels and for which CPIC guidelines recommend a higher starting dose in CYP3A5 expressors.8 Of the remaining independent SNPs, 5 SNPs (rs2257401, rs2242480, rs12333983, rs4646450, rs4646458) mapped to pharmacogenes CYP3A4, CYP3A5, and CYP3A7.24-27 The relatively high proportion of Asians (20%) in our study cohort may explain the significant finding with rs2257401, a SNP in LD with rs776746, that has been associated with tacrolimus levels in a Korean kidney transplant population.24,28 There is limited data on the SNP, rs12333983 (3'-UTR 27674A>T), known to be associated with hepatic CYP3A4 expression,29 rs17161780 that maps to CYP3A5-ZKSCAN5, and rs12957142 an intergenic SNP. Two SNPs, CYP3A5*6 and CYPA5*7, for which CPIC guidelines exist, did not reach significance at a genome-wide level in our study, possibly because these SNPs are associated with tacrolimus levels mainly in an African-American population,16 an ethnic group that was underrepresented in our cohort. CYP3A4*22 (rs35599367) loss of function variant, that has been associated with dosage-adjusted tacrolimus levels in other studies,23,30 was also not significant in our study likely because only 1 patient was homozygous for this variant. The ABCB1 3435C>T SNP (rs1045642)31 previously reported to be associated with tacrolimus metabolism did not reach significance in our study probably because the effect of ABCB1 polymorphism on tacrolimus pharmacokinetics is small.32 Six SNPs did not map to pharmacogenetics.

We used a machine learning approach to build a prediction model for dose-adjusted tacrolimus T1 levels that combined clinical and genetic factors. We focused primarily on T1 levels since the ability to predict accurate starting dose of tacrolimus can improve the achievement and maintenance of on-target tacrolimus concentrations.10 Both linear regression and lasso models selected organ type, age at transplant, rs776746, rs12333983, and rs12957142 as important predictors of dose-adjusted T1 levels. The machine learning model had a lower prediction error than the linear regression model in the validation cohort, which is consistent with other studies that have reported greater prediction accuracy with machine learning compared with a logistic regression approach.20,33 The performance of the combined clinical and SNP model was superior to that of the clinical or SNP only models. The combined model explained 30% of the variability in tacrolimus dose-adjusted T1 levels across the cohort. The variability in dose-adjusted T1 levels explained by the model was highest in heart and lower in liver and kidney recipients.

To further define how age and organ type influenced the association of genotype with T1 levels, we performed an interaction analysis using the top-ranked SNP, rs776746*1, and log-transformed dose-adjusted T1 levels and found a significant interaction of age and organ type with this association. The difference in T1 levels between nonexpressors and expressors was highest in infant and adolescent age groups and in heart recipients. While CYP3A5 expression appears to be independent of age,34,35 tacrolimus levels are influenced by several other factors that are age-dependent. For example, infants have lower dose-adjusted T1 levels likely due to a relatively large liver size with high plasma clearance of the drug.36 Also, age influences the expression of other enzymes like CYP3A4 (active in adults) and CYP3A7 (active in infants) that CYP3A5 nonexpressors have to rely on for tacrolimus clearance.35,37 Finally, changes in growth hormone and sex hormone levels in children between 5 and 15 years may influence hepatic CYP3A4 expression as these hormones enhance CYP3A4 expression.38 All these factors likely explain the age-related variability in the association of CYP3A5 genotype with dose-adjusted T1 levels. Further studies are needed to explore the basis of higher levels and greater SNP effect in heart transplants compared with other organ types. Regardless, these findings reinforce the importance of including both clinical and genetic factors when developing tacrolimus dosing algorithms and that an age and organ-specific approach will be needed to optimize prediction models in different organ types.

Clinical Significance

Several trials have evaluated CYP3A5-guided tacrolimus starting dose with mixed results.39,40 The DeKAF study group incorporated 4 clinical factors (time posttransplant, age, steroid use, and calcium channel blocker use) in addition to CYP3A5 genotype,41 but the ability to predict tacrolimus clearance could not be replicated in an independent validation cohort42 highlighting the challenges with modeling a complex phenotype and possibly an unaccounted effect of other genetic variants.43 A pediatric trial of solid organ transplants at our center showed the importance of incorporating age into CYP3A5-guided dosing to improve on-target tacrolimus concentrations.10 The findings of the present study underscore the importance of incorporating not just age and CYP3A5 genotype but also additional SNPs and clinical predictors into individualized dosing. Achieving on-target tacrolimus concentrations through individualized dosing has the potential to reduce the need for therapeutic drug monitoring, reduce costs and hospital length of stay, as well as reduce complications related to tacrolimus over- or under-dosing.44

Limitations

Although there were some differences between the discovery and validation cohort characteristics, the genotype findings were independent of clinical confounders and therefore remained significant in the validation cohorts. The study was underpowered for analysis of donor genotype, especially in liver recipients, for analysis of other factors, for example, hemoglobin, albumin, liver function as potential covariates of tacrolimus levels, and for association with clinical outcomes.3,45,46 The study was underpowered to detect an association of 2 SNPs that are included in CPIC guidelines due to underrepresentation of African-American population in our cohort. Our study was also underpowered to detect an association between donor genotype and tacrolimus levels in liver transplants because of a small number of paired liver donor-recipient genotypes. Future work is needed to include a larger sample size with a more diverse population to address these limitations.

In summary, using GWAS, we identified pharmacogenetic SNPs beyond previously known SNPs that were associated with tacrolimus levels independent of age, organ type, ethnicity and concomitant medications in a pediatric transplant cohort. Using machine learning, we demonstrated the superiority of a combined genetic and clinical prediction model for starting tacrolimus levels compared with a clinical or genetic only model, highlighting the importance of a precision medicine approach to tailored medical therapy in this population. While further refinement of the model is needed, the findings of our study pave the way for future development of individualized tacrolimus dosing.",https://journals-lww-com.myaccess.library.utoronto.ca/transplantjournal/fulltext/2022/03000/an_integrated_clinical_and_genetic_prediction.28.aspx
Vittoraki et al. 2021,Hidden Patterns of Anti-HLA Class I Alloreactivity Revealed Through Machine Learning,"Detection of alloreactive anti-HLA antibodies is a frequent and mandatory test before and after organ transplantation to determine the antigenic targets of the antibodies. Nowadays, this test involves the measurement of fluorescent signals generated through antibody–antigen reactions on multi-beads flow cytometers. In this study, in a cohort of 1,066 patients from one country, anti-HLA class I responses were analyzed on a panel of 98 different antigens. Knowing that the immune system responds typically to “shared” antigenic targets, we studied the clustering patterns of antibody responses against HLA class I antigens without any a priori hypothesis, applying two unsupervised machine learning approaches. At first, the principal component analysis (PCA) projections of intra-locus specific responses showed that anti-HLA-A and anti-HLA-C were the most distantly projected responses in the population with the anti-HLA-B responses to be projected between them. When PCA was applied on the responses against antigens belonging to a single locus, some already known groupings were confirmed while several new cross-reactive patterns of alloreactivity were detected. Anti-HLA-A responses projected through PCA suggested that three cross-reactive groups accounted for about 70% of the variance observed in the population, while anti-HLA-B responses were mainly characterized by a distinction between previously described Bw4 and Bw6 cross-reactive groups followed by several yet undocumented or poorly described ones. Furthermore, anti-HLA-C responses could be explained by two major cross-reactive groups completely overlapping with previously described C1 and C2 allelic groups. A second feature-based analysis of all antigenic specificities, projected as a dendrogram, generated a robust measure of allelic antigenic distances depicting bead-array defined cross reactive groups. Finally, amino acid combinations explaining major population specific cross-reactive groups were described. The interpretation of the results was based on the current knowledge of the antigenic targets of the antibodies as they have been characterized either experimentally or computationally and appear at the HLA epitope registry.","Knowledge about cross-reactivity of anti-HLA responses has been gained over several years usually in the setting of solid organ transplantation and has provided a rationale for common observations such as the development of antibodies against non-donor antigens which are shared with the polymorphic regions of the donor antigens interacting with antibodies. These have been defined as eplets and represent a part of a B cell epitope.

Experimental definition of eplets can be of great value to improve risk stratification tools and hopefully further push to the direction of improved computational tools and their potential use as prognostic markers. Eplets were defined experimentally after adsorption of poly-specific antibodies on appropriate targets (homozygote cell line or recombinant HLA molecule) followed by an elution step. The eluate was then tested for reactivity on all other HLA molecules followed by the definition of amino acid similarities between the recognized antigens which defined the eplet (27). Therefore, the definition of eplets depends on the sera tested and several eplets could be ignored depending on the selection criteria for the tested sera. However, a high number of incompatible eplets on donor antigens increase the likelihood that some of these epitopes will be immunogenic, increasing the risk of developing donor-specific antibodies (12). Hence, Eurotransplant has used an eplet based allocation scheme for highly sensitized patients with excellent results (28, 29). This led to the idea that HLA eplet mismatch differences or other HLA molecular mismatch (mMM) scores are promising approaches for risk stratification although the definition of cut-off values defining a positive reaction is still an object of debate (30). Therefore, experimental definition of eplets can be of great value to improve these risk stratification tools and hopefully further push to the direction of improved computational tools and their potential use as prognostic markers (31). Moreover, there is no acceptable score of the immunogenic value of each eplet mismatch.

In the present study a method of “reading” the products of the humoral alloresponse in terms of their antigenic targets is proposed. Modeling the immune response with dimensionality reduction approaches produced very simple but robust unsupervised views of cross-reactivity, and thus an indirect eplet definition, as seen on Luminex analyzed immune responses. We preferred using unsupervised dimensionality reduction algorithms in order to let the data “speak” without any a priori immunological hypothesis. For this reason, we did not follow the instructions provided by the manufacturers to use arbitrary cut-off values to determine positive and negative reactions. The interpretation of the results was based on the current knowledge of the antigenic targets of the antibodies as they have been characterized experimentally or computationally and as they appear at the HLA epitope registry.

Previously, we undertook a machine learning approach in a homogeneous cohort coming from a single country in order to “revisit” cross-reactivities observed for anti-HLA class II responses in patients with anti-HLA class II antibodies either before or after renal transplantation (13). Now, we reasoned that extracting this type of information from an equivalent HLA class I specific data table could be also useful for the generation of concepts that could be helpful for bead-array eplet definition and give indirect information about antigenic distances of HLA alleles that could be useful for patient monitoring.

The first analysis presented in this manuscript showed that 90% of the variance of the immune response seen as cumulative MFI values for each locus can be captured on a single plane PCA biplot. This finding suggests that using the PCA as a dimensionality reduction algorithm is highly suitable for the analysis in question.

Furthermore, other indirect conclusions can be easily drawn from this biplot. Indeed, the fact that anti-HLA-A and anti-HLA-C responses show an almost orthogonal pattern suggests that these responses are weakly correlated at a population level and thus the antigenic distance between the products of these two loci are the biggest among the products of class I loci analyzed nowadays on Luminex devices. Furthermore, when we performed the same type of PCA with single bead fluorescence measures we also observed a similar orthogonal pattern between the alleles encoded by A and C loci although responses against 47 different alleles were analyzed (data not shown). These findings suggest that shared eplets among these two loci do not play an important role when seen from a population perspective. This is also in accordance with the number of common eplets published in the epitope registry.

On the other hand, total anti-HLA-B and total anti-HLA class I responses are colinear and lie along the first eigenvector of the PCA biplot thus suggesting that the major variance observed in anti-HLA class I response comes from anti-HLA-B response and has a strong correlation with the total anti-HLA class I response. The indirect message of this finding is that an increase in anti-HLA-B response correlates with an increase in total anti-HLA class I response and is thus strongly correlated with a high sensitization status. This finding is also in accordance with the number of proposed eplets in the epitope registry but also with the high degree of polymorphism of the HLA-B locus (23). The PCA decomposition of the antibody response highlights the polyclonal nature the response.

Furthermore, when we looked at the PCA biplot defined by the second and third eigenvectors of the covariance matrix, we observed a clear pattern of three 120° angled arrows suggesting that locus-specific reactions are rare when seen from a population perspective.

When all allele-specific MFIs for HLA-A locus on a PCA biplot are projected, 69.1% of the variance is captured on a single plane. Again, this fact argues that PCA is suitable for modeling the anti-HLA-A response. However, reactions against all antigenic specificities are not well captured on this single biplot. Indeed, anti-HLA-A2 and HLA-A28 bead-array defined group 1 responses are well represented on this biplot and almost orthogonal to group 2 allele responses which are also very well represented. Indirectly, this finding highlights that the anti-HLA-A response is more frequently directed against two major loosely bead-array defined groups of eplets.

The most interesting pattern of this projection is that there is almost no correlation between these two types of response suggesting that two major but independent types of immune responses are observable in the Greek population. A similar result is observed with a cohort analyzed with an Immucor bead-array (Supplementary Figure 1A).

There have been described 115 polymorphic residues accessible to antibodies on the 31 HLA-A alleles that were tested for antibodies in this study (23). According to the projections of the anti-A responses on PCA biplots, 69.1% of the response variance is explained by a strong clustering tendency, associated with only 45 of these polymorphisms. However according to these, only the antibody verified eplets 144TKH and 145KHA, which were found to be different between the two major bead array defined allelic groups, are HLA-A specific suggesting that their concomitant presence or absence associates with the two major groups of observable clusters in the Greek population. The fact that both eplets have amino acids which are predicted as being on the surface of the molecules suggests that a PCA analyzed anti-HLA-A response in a large population highlights the most important cluster associated with antibody verified eplets, but also may give experimental credit for eplets found only through computational approaches. In addition, the clear orthogonality observed in Figure 2B between the anti-A2C1 CREG allele responses and the responses against the HLA-A23, A24, A25, and A32 third group alleles, associates with the polymorphisms at position 82, 83 (RG/LR). These polymorphisms correspond to the 82LR eplet (high ElliPro score and antibody verified), expressed on the HLA- A23, A24, A25, A32, and all Bw4 alleles.

HLA-B specific responses show a much more complex pattern and the first plane PCA projection captures 66% of the variance. Indeed, the arrows representing the different antigens analyzed through this Luminex assay are more loosely grouped, though Bw4 associated antigens (variables) are grouped below the 1st eigenvector and Bw6 associated antigens are grouped above. This finding underlines the fact that the Bw4 and Bw6 CREG distinction of HLA-B response is the main feature clearly seen on Luminex-based assays. A similar result is observed with a cohort analyzed with an Immucor bead-array (Supplementary Figure 1B).

In order to capture at least 70% of the variance per locus, we also used a second plane projection by adding another 8.9% of explained variance for the B locus. Again, the Bw4 and Bw6 variable grouping is preserved. Some other characteristics of variable co-linearity are also noteworthy if the projections of the two planes are combined. Indeed, B*07:02, B*27:08, and B*81:01 (Bw6 alleles) specific arrows remain grouped whatever the plane is; thus, these antigens belong to one major bead-array defined group observed in the Greek population. Another bead-array defined group is composed of B*15:13, B*51:01, B*51:02, B*52:01, and B*53:01 (Bw4 alleles). These two groups are represented by two beams of arrows orthogonal to each other on the first PCA plane, and although still grouped, they are pointing in opposite directions on the second plane. Taken together these data suggest that antibody responses against these two groups are uncorrelated when one looks at the first PCA plane and for a minority of patients are negatively correlated (second PCA plane). Although this is not really surprising, it is interesting to note that these two Bw4 and Bw6 belonging groups represent the most distantly related and very well explained antibody responses against the B locus.

The second plane variance decomposition adds supplementary information represented as angular distances between the arrows representing each antigen related variable of the Bw4 CREG. Indeed, the B*27:05 and B*47:01 arrows are still grouped and almost orthogonal to the most distantly related B*15:13, B*51:01, B*51:02, B*52:01, and B*53:01 group. This suggests that these two groups show the maximum antigenic distance within the Bw4 CREG.

Moreover, on the second plane PCA biplot for the Bw6 CREG, a B*07:02, B*27:08, and B*81:01 bead-array defined sub-CREG shows a negative correlation with a bead-array defined sub-CREG formed by B*15:02, B*15:10, B*15:11, B*35:01, and B*78:01. One can thus argue that aa differences between distantly related groups may be seen as a data-based approach to define eplets with a strong impact on the population. Similarly, aa which differs between the alleles of each “tightly packed” group points to aa changes defining eplets with a limited “weight” in the studied population.

We then tried to see whether sequential alignments of the most distantly related Bw4 and Bw6 belonging alleles provide additional information about eplet weights.

We first performed these alignments for the most antigenically distant Bw4 alleles, and we found that only three (80I and 163EW and 163LW) of the eplets found to be different between these Bw4 alleles have been antibody verified. This fact suggests that several computationally defined eplets associate with a frequent cluster of an immune response and are specific of an antigenically similar PCA defined allelic group of foremost importance for the studied population.

For the most distantly related Bw6 aa differences at positions 11, 12 (SV/AM), 69, 71 (AA/TT), and 163 (E/L) between these sub-Bw6 groups are found and corresponded to eight eplets. Even though only three of the eight eplets have been experimentally verified, again a group of five computationally defined eplets associate with a frequent cluster of an immune response.

Taken into consideration these data, it is suggested that a sequential analysis of PCA either proximal or distant allelic groups may give new information about the importance (weight) of computationally derived eplets needing special attention. This might be important for a better understanding of anti-HLA-B responses where many computationally eplets are described.

Anti-HLA-C responses that are divided into two groups occupying the upper and lower quadrants of the PCA biplot, respectively represent 76.9% of the variance of the antibody response in the Greek population. Interestingly, the two groups of antibodies recognize the two groups of HLA-C alleles that express either the C1 or the C2 epitope. Both epitopes are known as the dominant inhibitory KIR ligands (32). These epitopes are characterized by different aas at position 80, where the presence of an asparagine (N) defines the C1 group while the presence of a lysine (K) defines the C2 group (33). There have been described 77 polymorphic residues on the 15 HLA-C alleles (excluding C*07:02) that were used as antibody targets in the Luminex assay. According to the projections of the anti-C responses by PCA, 82.5% of the response variance is explained by a strong clustering tendency associated with only 35 of the known polymorphisms. A similar pattern of anti-HLA-C responses is seen in a French and a German population (M. Carmagnat and C. Lehmann personal communications). Moreover, a similar pattern was observed in a cohort of patients analyzed on an Immucor bead array (Supplementary Figure 1C). This relatively tight biphasic pattern is surprising, and one should be cautious about potential confounding factors such as the presence of different proportions of denatured HLA molecules on the bead array leading to false groupings. However, the fact that a similar pattern is found with a different bead array assay from a second vendor (Immucor) in an independent population strongly argues against the impact of the bead manufacturing process and the importance of denatured antigens for this finding. Furthermore, stratification of the HLA-C response according to the HLA-C genetic background of the host showed that self-directed antibody responses are rare (Supplementary Figure 2). Indeed, antibody responses against denatured HLA molecules are one of the major causes of self-directed anti-HLA responses and are usually directed against a limited number of epitopes which are common between different HLA-A, -B, -C antigens and HLA–E molecules (34).

Since HLA-C alleles are inhibitory ligands protecting cellular targets from NK cell lysis, antibody responses against C1/NK2 and C2/NK1 alleles may play an important role in NK cell function in the transplantation context. Although the presence of C1 and C2 alleles is part of algorithms used for better allocation of bone marrow donors, the role of NK cells in organ transplantation is not studied so much (35, 36). However, very recent studies underlined a prominent role of NK1 versus NK2 responses for graft tolerance (32). It would be interesting to analyze the effect of anti-HLA-C antibody responses as a possible NK1 versus NK2 balance modifying factor.

Finally, we used a second more simple algorithmic approach to the whole reactions leading to a phylogenetic tree representation based on a distance matrix. Probably the most useful property of this representation is that it provides a simple way to define roots of main branches from where “speciation like” divergences emerge. It seems that a common antigenic structure forms the root of the branch from where subsequent sequence variations induce longer antigenic distances. Another interesting feature of this procedure resulting in a “species-like” projection of anti-HLA responses is the fact that these projections come from real data provided by Luminex platforms which are the main devices used to study anti-HLA responses nowadays. To this aspect, it parallels previous experimental methods used to define CREGs but can be used in existing data coming for numerous samples at no cost. In a certain way, the method provides a robust procedure to illustrate bead-array defined CREGs and then go into subsequent branches of the tree to define triplets, eplets or important physicochemical differences that form the basis of current algorithms used for the prediction of adverse immune responses against the graft.

The dendrogram of the antibody responses produced with this algorithmic approach put almost all the alleles of the same locus to the same branch of a tree. However, some distinct exceptions to this rule are illustrated. Anti-HLA-A*25:01, A*32:01 responses clustered among anti-HLA-Bw4 specific responses. A similar pattern was observed for anti-HLA-A*23:01, A*24:02, and A*24:03 responses that clustered in a branch containing Bw4 alleles (B*44:02, B*44:03) and Bw6 alleles (B*45:01 and B*15:12) whatever the distance measure (Minkowski or Manhattan, data not shown) used for the computation. These HLA-A alleles belonged to the Bw4 CREG and interestingly they were not well represented on the corresponding HLA-A PCA biplot, due probably to the strong influence of the response to shared polymorphic regions between HLA-Bw4 and HLA-A alleles. Respectively anti-HLA-B*46:01 responses clustered with anti HLA-C*03:02, C*03:03, C*03:04, C*07:02, C*12:03, C*01:02, C*16:01, C*14:02, and C*08:01 (C1 group), possibly indicating strong influence of the response to “shared” polymorphic regions between HLA-B46 (Bw6 allele) and C1 group.

However, one could argue that these speciation-like representations could be simply due to the presence of antibodies against denatured HLA which are in different proportions among different sets of bead arrays and produce groupings of reactions not related to epitope recognition. To investigate this possibility, we compared the dendrograms with the exact same settings for the HLA-A locus where the antigens presented on an OL, and an Immucor bead array was the most common, and the resulting tanglegram showed that the groupings of the corresponding antigens were very similar (Supplementary Figure 3). Moreover, a tanglegram of HLA-B and HLA-C responses in both dendrograms still classifies0 the B*46:01 allele within the HLA-C alleles although the B*46:01 allele is very rare in the Greek population. These findings further suggest that the proportion of denatured HLA antigens bound on the bead arrays does not influence clusters of the observable antigenic proximities.

Interestingly, neighbor-joining phylogenetic trees of gene region sequences of HLA-C genes show major differences with the antigenic distances shown by the immune response based dendrograms (37).

One of the limitations of our approach is that sera used in the study were considered positive either after a screening test or previous information of anti-HLA class I sensitization. This limits the conclusions that are only applicable to a population of pre/post-transplant patients. Moreover, one could argue that bead-arrays contain a proportion of denatured HLA molecules that tend to be recognized even in non-sensitized healthy individuals albeit at low levels. The presence of antibodies against denatured HLA which are in different proportions among different sets of bead arrays could produce groupings of reactions not related to epitope recognition. To investigate this possibility, concerning PCA projections, a similar approach in another smaller cohort selected on the same criteria and analyzed on an Immucor bead array showed a similar PCA pattern for anti-HLA-A, -B and -C responses (Supplementary Figure 1). The bead-arrays from Immucor and OL contain almost the same alleles, but the proportion of denatured antigens is not the same. Moreover, we compared dendrograms with the exact same settings for the HLA-A locus where the antigens present on an OL and an Immucor bead array were the most common, and the resulting tanglegram showed that the groupings of the corresponding antigens were very similar (Supplementary Figure 3). Moreover, a tanglegram of HLA-B and HLA-C responses in both dendrograms still classifies the B*46:01 allele within the HLA-C alleles although the B*46:01 allele is very rare in the Greek population. These findings further suggest that the proportion of denatured HLA antigens bound on the bead arrays does not influence clusters of the observable antigenic proximities.

Overall, we suggest that unsupervised modeling of the immune response as measured by Luminex platforms gives valuable information regarding antigenic distances among HLA alleles, and it is relatively easy to obtain from data stored in recent years in histocompatibility laboratories. Indeed, the use of PCA calculated angles and/or measures of phylogenetic distances among antigens could be another complementary approach used as a “weighing tool” for the improvement of predictions based on eplet load differences, EpMS, and PIRCHE scores. However, we recognize that more studies in different populations are necessary to provide more insights and support this mixed numerical and experimental approach.",https://www-frontiersin-org.myaccess.library.utoronto.ca/articles/10.3389/fimmu.2021.670956/full
Woillard et al. 2021,Mycophenolic Acid Exposure Prediction Using Machine Learning,"Therapeutic drug monitoring of mycophenolic acid (MPA) based on area under the curve (AUC) is well-established and machine learning (ML) approaches could help to estimate AUC. The aim of this work is to estimate the AUC of MPA in organ transplant patients using extreme gradient boosting (Xgboost R package) ML models. A total of 12,877 MPA AUC from 0 to 12 hours (AUC0–12 h) requests from 6,884 patients sent to our Immunosuppressant Bayesian Dose Adjustment expert system (https://abis.chu-limoges.fr) for AUC estimation and dose recommendation based on MPA concentrations measured at least at three sampling times (~ 20 minutes, 1 and 3 hours after dosing) were used to develop two ML models based on two or three concentrations. Data were split into a training set (75%) and a test set (25%) and the Xgboost models in the training set with the lowest root mean squared error (RMSE) in a 10-fold cross-validation experiment were evaluated in the test set and in 4 independent full-pharmacokinetic (PK) datasets from renal or heart transplant recipients. ML models based on two or three concentrations, differences between these concentrations, relative deviations from theoretical times of sampling, presence of a delayed absorption peak, and five covariates (dose, type of transplantation, associated immunosuppressant, age, and time between transplantation and sampling) yielded accurate AUC estimation performances in the test datasets (relative bias < 5% and relative RMSE < 20%) and better performance than MAP Bayesian estimation in the four independent full-PK datasets. The Xgboost ML models described allow accurate estimation of MPA AUC0–12 h and can be used for routine exposure estimation and dose adjustment and will soon be implemented in a dedicated web interface.","In this work, we developed Xgboost ML models based on two or three samples to estimate the AUC0–12 h of MPA in transplant or auto-immune indications. The models were developed using the standard ML procedure with a development in a training set using cross-validation for tuning of the parameters and estimation of the performances and once the best model was selected, validation in the test set. The results were then compared on an external dataset with those obtained by extensive-sampling AUC0–12 h estimated using MAP-BE using all the available samples (reference AUC0–12 h) and to MAP-BE estimates based on a three-point LSS, as used by the ISBA expert system3 in renal and heart transplant recipients. The performances of the ML were excellent as compared with the reference AUC0–12 h when using three concentrations, and numerically better than those of MAP-BE based on the three-point LSS.

The performances obtained with the 3-point ML model were better in all the external validation datasets, in terms of imprecision and number of profiles out of the ± 20% acceptability interval, than the 3-point MAP-BE. Additionally, the MAP-BEs available in ISBA and used in the present study for comparison to ML were initially developed using the datasets used here for validation, probably advantaging them. However, in line with our previous results for tacrolimus, ML yields better results. The ML model using only 2 samples showed acceptable bias values but imprecision around 25%, which is not so high in comparison to literature results but very close to the threshold of 25% considered as the limit for accurate estimation in the last MPA consensus.2 This two-point ML estimator might still be used in cases where only two samples are available, but the prescriber has to be informed of the degraded performance in comparison to the three-point model.

In addition to better performances than the three samples MAP-BE used in ISBA, the ML approach has the advantage of practicability, as our single ML model can accommodate all associated immunosuppressants or MMF indications, when different MAP-BEs are necessary for each subgroup in ISBA.

There are some differences between this study and our previous one with tacrolimus.19 Indeed, tacrolimus PopPK is not easy to model, but it has obvious associations with covariates that explain part of the interindividual variability, the dose-exposure relationship is good, and the AUC is fairly correlated with single timepoints (the first being the trough level, used in routine practice).30 All these elements may explain why our first ML algorithms were successful. The situation is very different with MMF, which can be regarded as the worst-case scenario: MPA exhibits very complex PKs, the very diverse (and sometimes chaotic) individual profiles are much more difficult to model and there is only a weak correlation between the dose and the AUC (as shown here), and between the AUC and any single timepoint.2 Therefore, it was much more challenging to develop accurate ML algorithms for MPA, and this achievement suggests it should be possible for all the other drugs for which TDM is performed. The progress when compared with MAP-BE looks less impressive than with tacrolimus, but this highlights the fact that learning from a less precise reference (here, MAP-BE imprecision is ~ 20% for MMF) results in somewhat less precise prediction. If the ML models were trained in a large, full-profile dataset, they would have been more accurate, but we did not have such a database. In our hands, trading rich profiles for a lower number of patients (i.e., training the ML algorithms in the validation datasets) did not result in a better performance either.

The principle of ML is to build algorithms that estimate individual values with the lowest imprecision, which is perfectly adapted to TDM. However, ML algorithms cannot study the determinants of interindividual or intra-individual variability or perform simulations to estimate a priori the first dose, or calculate the probability of target attainment for different dosing regimens, for instance. Therefore, we think that ML may replace PopPK for individual exposure estimation but that PopPK will remain very useful for other purposes. Additionally, although being very accurate, ML algorithms are not as flexible as PopPK models. For instance, our ML models cannot take into account more than three samples per patient and the sampling times have to be in the same ranges as those of the training set (because the validation for times outside these ranges has not yet been done).

Imprecision values observed in the present study (19%, 14.7%, and 22.5%, in renal and 14.6% in heart transplantation) are slightly better than those reported in the literature, in the range of 20%–25%.5, 10, 31-35 These values are higher in comparison to other immunosuppressants (e.g., tacrolimus, with imprecision ~ 10%) but they reflect the complexity of MPA PKs. Some papers reported slightly better precision, but these PopPK models were either not validated in independent datasets7, 9, 36 or they compared three-points MAP-BE AUCs to reference AUCs obtained using six samples and the same PopPK model, leading probably to overestimation of the MAP-BE results.11

The randomized, comparative APOMYGRE study in 137 adult kidney transplant recipients on cyclosporin A showed that concentration-controlled MMF dosing using the MAP-BE implemented in ISBA had a significantly lower incidence of acute rejection than patients with standard-dose MMF.37 In their meta-analysis, Metz et al.38 concluded that when concentration-controlled dosing is performed based on PK calculation to a target concentration, MPA exposure is well-controlled and clinical outcomes are improved, which has been stressed in the very recent consensus conference paper.2 Based on the better performance of our ML estimator in comparison to ISBA MAP-BEs (among which those used in the APOMYGRE study), we propose to use it for routine calculation of MPA AUC and will make it available to the medical community through a web-interface. In the meantime, a temporary shiny application (https://jbwoillard.shinyapps.io/App-7_mmf_ml/) has been built to test the model (for research purposes only).

The added value of AUC-based dose adjustment of MMF has currently been demonstrated for the very early post-transplant phase (especially the first weeks after transplantation).2 The performances of our ML models in patients transplanted for < 1 month (only one study in kidney and one in heart transplant recipients had data collected within the first month) were very similar to those in the whole validation population.

The use of our ML estimator is submitted to some restrictions, due to the datasets used to design it: it was developed using MPA concentrations measured using high-performance liquid chromatography and in which the interdose was 12 hours. Additionally, the external dataset used for external validation comprised only renal and heart transplant recipients. Its performances in the other MMF indications for which the model was developed (nephrotic syndrome, liver or lung transplantation, lupus, pediatric lupus, hematopoietic stem cell transplantation, and “other” mostly corresponding to auto-immune disease) are still to be evaluated.

We believe that the ML models we developed are insensitive to the achievement of the PK steady-state, because they do not rely on a PK model and they gave excellent results in the very large training and validation datasets, where part of the profiles were certainly not at steady-state (e.g., profiles collected on day 7 post-transplantation where the dose had already been adjusted based on adverse effects).

However, these ML models have some limitations. To cope with deviations from the theoretical times, we included them as new features in the model but we have no idea about the accuracy of the algorithms when samples are drawn outside the ranges actually observed in our datasets. This has to be investigated in further studies. In the meantime, we do not recommend using the algorithms when samples are collected outside the predefined ranges (20 minutes: from 10 to 30 minutes; 60 minutes: from 45 to 75 minutes; and 180 minutes: from 160 to 200 minutes). In addition, our ML models are less flexible than MAP-BE and cannot manage more than three samples. These ML models are perfectly adapted for routine use but not really for research projects or specific cases in which more than three samples are available.

In conclusion, an Xgboost ML model providing accurate estimation of MPA AUC0–12 h has been developed for many MMF indications, and validated in independent groups of kidney and heart transplant recipients. It will soon be implemented in a dedicated web interface for these two indications. A patent has been filed.",https://ascpt-onlinelibrary-wiley-com.myaccess.library.utoronto.ca/doi/10.1002/cpt.2216
Vittoraki et al. 2020,"Patterns of 1,748 Unique Human Alloimmune Responses Seen by Simple Machine Learning Algorithms","Allele specific antibody response against the polymorphic system of HLA is the allogeneic response marker determining the immunological risk for graft acceptance before and after organ transplantation and therefore routinely studied during the patient's workup. Experimentally, bead bound antigen- antibody reactions are detected using a special multicolor flow cytometer (Luminex). Routinely for each sample, antibody responses against 96 different HLA antigen groups are measured simultaneously and a 96-dimensional immune response vector is created. Under a common experimental protocol, using unsupervised clustering algorithms, we analyzed these immune intensity vectors of anti HLA class II responses from a dataset of 1,748 patients before or after renal transplantation residing in a single country. Each patient contributes only one serum sample in the analysis. A population view of linear correlations of hierarchically ordered fluorescence intensities reveals patterns in human immune responses with striking similarities with the previously described CREGs but also brings new information on the antigenic properties of class II HLA molecules. The same analysis affirms that “public” anti-DP antigenic responses are not correlated to anti DR and anti DQ responses which tend to cluster together. Principal Component Analysis (PCA) projections also demonstrate ordering patterns clearly differentiating anti DP responses from anti DR and DQ on several orthogonal planes. We conclude that a computer vision of human alloresponse by use of several dimensionality reduction algorithms rediscovers proven patterns of immune reactivity without any a priori assumption and might prove helpful for a more accurate definition of public immunogenic antigenic structures of HLA molecules. Furthermore, the use of Eigen decomposition on the Immune Response generates new hypotheses that may guide the design of more effective patient monitoring tests.","Alloimmune response against HLA class II molecules is a frequent laboratory measurement either before or after transplantation and is associated with graft injury and impaired graft function (20, 21). Although the HLA polymorphism and the complexity of the immune response determine the type of antibody production, shared-public functional antigenic determinants and close genetic linkage between antigens of different loci lead to the production of antibodies against specific antigenic groups. The definition of the reactivity patterns of these antibodies is useful in order to understand the principles of the alloresponse, to define the HLA immunogenic polymorphic residues, to design more effective laboratory tests and to improve allocation.

Agnostic methods play an important role in today's science. Epitope definition of HLA molecules with very few à priori assumptions was not attempted until very recently (22, 23). The field of hypothesis-free learning of the immune response starts growing. Simmons et al. (24) shows the advantages of searching patterns of reactivity in anti HLA class II responses. In this context we analyzed anti HLA Class II responses measured on the Luminex platform in a cohort of 1,748 patients monitored by the three major histocompatibility laboratories in Greece in order to identify reactivity patterns of these antibodies. In order to get the procedure as agnostic as possible we did not use cut off limits thus avoiding rendering each individual response strictly quantitative.

The observations in this study are based on “automatic” data driven classification of the alloimmune response performed with solid state fixed antigens. In order to generate the correlograms presented herein we tried several hierarchical ordering formulae. Only the application of general and/or Mc Quitty distance-based agglomerating models produced distinguishable locus specific responses, not only confirming known inter and intra locus cross reactivity among HLA antigens but also illustrating new features and providing important information regarding known or less studied loci.

All this information, coming from patients of a single country and tested under the same methodology, may become a hypothesis generating tool that can be used for formal modeling of the immune response. The example of anti HLA-DPB responses showing numerous undocumented but experimentally proven correlations both by Correlation and PCA analyses is an example of the utility of such tools.

Similar antibody reactivity patterns were observed between the patients awaiting an organ and those post transplantation. This observation indicates that beneath the complexity, the immune system reacts under a set of rules while not being affected by the immunosuppressive treatment.

On the contrary, a marking difference was observed at the uni-dimensional correlation pattern of anti HLA class II immune response when one focuses on pre or post transplantation patients. Indeed, the agglomerative hierarchical clustering of the correlograms differs in terms of inter locus proximity as DR and DQ responses are always ordered one next to the other while DP responses are considered as more proximal to DR or to DQ depending on the clinical condition. These results parallel our previous finding regarding a hierarchy in specific locus antigen recognition of the graft. Indeed, anti DQ graft specific antibodies have been described as the earliest de novo DSA usually seen during the follow up in renal transplant recipients in Greece (25, 26).

In order to get more insight on this “computer vision” of anti HLA class II response we further performed a PCA at the same population data. This unsupervised learning method transforms original data and visualizes them in subsequent orthogonal plains explaining the maximum of variance in terms of antibody specificity (variables) and responses in each serum (patients).

In this immune response setting, a three-dimension PCA explains about 55% of the population's total variance which is considered to be relatively limited in comparison with the classical thumb rule demanding an 80% variance explanation for a “good” PCA. However, this very preliminary analysis points to several hypotheses to be tested in the upcoming years. According to the PCA generated view, the first observation that comes from the Dim 1 and Dim2 variance plane, depicted in both patients' groups, is a differential clustering of DPB responses as compared to DRB and DQB. These responses appear to co-exist, confirming the analogous observation with the previously described correlograms. As a consequence, the overall picture of both types of analyses point to the same message in terms of no correlation between DPB and DR/DQ responses in the Greek population. Interestingly a similar finding was reported in a North American population (24). Therefore, a more careful analysis of DP specific responses could provide new markers for a safer organ allocation given that in Greece, but also in most European countries, anti DP responses are not taken into account for patient inscription on waiting lists. The second best explaining variance plane shows that another 12% of the variance is a clear mono locus immune response seen in both groups of patients. This can be interpreted as the second main characteristic of anti HLA class II immune response.

An important finding uncovered by this Machine Learning approach concerns the amount of immunologically important information that can be gained by the bead bound antigens proposed by the industry. Although the choice of these alloantigens is based on the relative frequencies of the alleles found in human populations, both the correlogram and the PCA results suggest that there is a lot of information redundancy in terms of antigen coupled beads. For instance, DRB1*15:01 and DRB1*15:02 specific reactions show a very strong correlation pattern and a quasi-identical PCA explained correlation and variance in several planes. This finding suggests that the information gained from the various DR15 coated beads appears equivalent. Therefore, if one measures the intensity of one DR15 coated bead, is then able to positively predict the values of the rest of the beads with a simple linear regression. Noticeably, an evidently similar pattern is also seen in reactions against DRB1*16:01 and 16:02, but also DRB1*14:01 and DRB1*14:54 coated beads. Probably, a formal regression analysis driven by PCA and/or correlogram clustering patterns could prove useful to indicate beads with redundant information. Vice versa antigens not incorporated into the Luminex display can be predicted by a cross reactive bead providing information for more informative HLA panel to monitor the allogeneic Immune response.

The information that emerges from this approach might prove useful for updating successful but still evolving matching algorithms. Several studies have shown that HLA alloantigen immunogenicity can be more accurately assessed by evaluating differences in the number and location of amino acid mismatches at continuous and discontinuous (eplet) positions, as well as their physicochemical properties. Actually, the HLA Matchmaker is widely used for prediction of harmful anti-graft immune responses and relies on the correct definition of eplets and eplet load differences between donor and recipient (27). More recently another approach based on differences in inter and intra locus donor-recipient HLA amino acid sequence along with an electrostatic mismatch score, enables prediction of allosensitization to HLA and also prediction of the risk of an individual donor-recipient HLA mismatch to induce DSA. However, the same study suggests that HLA Class II DSA responses are independently associated with these two HLA immunogenicity scores (12). An alternative approach for epitope based matching is the identification of the mismatched HLA derived epitopes that can be recognized by T cells via the indirect pathway using the PIRCHE-II algorithm. A higher PIRCHE-II score was related to both de novo donor specific HLA formation and allograft rejection (28). Our study proposes for an algorithm aiming to clarify the targets of the anti-HLA response after generation of effector alloantibodies. However, the generation of the antibodies requires triggering of a heterogeneous population of allopeptide specific CD4 T helper subsets that recognize different alloantigens and generate Ig-class switched alloantibody responses through the indirect CD4 T cells/allospecific B cells pathway. CD4 T cell alloresponse is a key determinant of transplant outcome provided that the indirect allorecognition pathway is activated in early acute rejection but also operates at late times points after transplantation sustaining chronic alloimmune responses. Therefore, the antibody specificity generated at different stages after transplantation is affected from the different epitopes that trigger different CD4 T helper subsets at varying stages of effector and memory differentiation. We consider that defining the targets of alloantibodies we may confirm the immunogenic T cell epitopes predicted in algorithms and study the dynamic of the phenomenon of intra and inter molecular epitope diversification or spreading (29).

Our approach may be used to enrich these algorithms and especially those based on eplet definitions. Indeed, empirically defined eplets can be experimentally validated or not by using the raw MFI values produced by the Luminex platforms and analyzed either through correlation or PCA based algorithms. However, all algorithms should always be considered in the context of other factors. We acknowledge that the results generated by the proposed algorithm are affected by immunological factors besides the HLA structures and the DSA such as hemodialysis—related immunosuppressive factors, heterologous immunity, immunological memory, immunosuppressive treatment and the development of regulatory mechanisms under immunosuppressive regiments. Under all these parameters the alloantibody profile of the patients is generated. This profile is essential for graft allocation and signals activation of anti-graft alloresponse post transplantation.

Having under consideration that a more accurate definition of the immunogenic HLA epitopes in different population groups will become a valuable tool for epitope matching, an important parameter for donor selection, here we propose a new approach to study the alloimmune response. From a transplantation perspective the use of a priori defined epitopes is currently widely discussed as a tool for allocation of organs as reviewed by Kramer et al. (30). We propose that a descriptive statistics approach can be another enlightening way to address antigenic proximities of HLA and helpful for the definition of frequent public epitopes present in a population. Furthermore, it has an enormous advantage since one can use data usually stored by transplantation laboratories and easily analyzable. It remains to be proven whether this type of analysis can delve deeper in the alloimmune study and define minimal regions of Immunogenicity based on experimental data to be used for organ allocation.",https://www-frontiersin-org.myaccess.library.utoronto.ca/articles/10.3389/fimmu.2020.01667/full
Pineda et al. 2017,Novel Non-Histocompatibility Antigen Mismatched Variants Improve the Ability to Predict Antibody-Mediated Rejection Risk in Kidney Transplant,"Transplant rejection is the critical clinical end-point limiting indefinite survival after histocompatibility antigen (HLA) mismatched organ transplantation. The predominant cause of late graft loss is antibody-mediated rejection (AMR), a process whereby injury to the organ is caused by donor-specific antibodies, which bind to HLA and non-HLA (nHLA) antigens. AMR is incompletely diagnosed as donor/recipient (D/R) matching is only limited to the HLA locus and critical nHLA immunogenic antigens remain to be identified. We have developed an integrative computational approach leveraging D/R exome sequencing and gene expression to predict clinical post-transplant outcome. We performed a rigorous statistical analysis of 28 highly annotated D/R kidney transplant pairs with biopsy-confirmed clinical outcomes of rejection [either AMR or T-cell-mediated rejection (CMR)] and no-rejection (NoRej), identifying a significantly higher number of mismatched nHLA variants in AMR (ANOVA—p-value = 0.02). Using Fisher’s exact test, we identified 123 variants associated mainly with risk of AMR (p-value < 0.001). In addition, we applied a machine-learning technique to circumvent the issue of statistical power and we found a subset of 65 variants using random forest, that are predictive of post-tx AMR showing a very low error rate. These variants are functionally relevant to the rejection process in the kidney and AMR as they relate to genes and/or expression quantitative trait loci (eQTLs) that are enriched in genes expressed in kidney and vascular endothelium and underlie the immunobiology of graft rejection. In addition to current D/R HLA mismatch evaluation, additional mismatch nHLA D/R variants will enhance the stratification of post-tx AMR risk even before engraftment of the organ. This innovative study design is applicable in all solid organ transplants, where the impact of mitigating AMR on graft survival may be greater, with considerable benefits on improving human morbidity and mortality and opens the door to precision immunosuppression and extended tx survival.","Antibody-mediated rejection is a major cause of allograft dysfunction and graft loss as a result of the development of de novo DSA to donor-specific HLA antigen mismatches with the recipient after tx (44). The principal targets of the AMR response are the highly polymorphic HLA antigens, but the rejection process has also been observed in HLA-identical siblings (10), suggesting a critical role for D/R nHLA antigen mismatches that may also drive pathogenic antibodies to these mismatched nHLA antigens in AMR (45). Developing methodologies to detect genetic differences between D/R prior to transplant that drive increased AMR risk after tx, will be highly relevant for influencing the long-term outcomes of graft life expectancy after organ tx. This pilot study with carefully selected clinical phenotypes shows a significant increase in the number of mismatched variants prior to tx, which significantly correlate with the development of biopsy-confirmed acute rejection in the recipient after tx.

By developing and applying custom statistical methodologies to exomeSeq data on donor and recipient samples prior to tx, we confirmed our hypothesis that the total number of variants that mismatch by D/R pairs is higher when the recipient goes on to develop AMR after tx. In addition, we also found a highly refined set of variants that can accurately predict immune risk stratification of patients before tx, into those that develop different clinical endpoints after tx of either biopsy-confirmed AMR, biopsy-confirmed CMR or stable function and no rejection. None of these newly identified variants were located in the HLA region, even though the patients involved in this study were sensitized to various HLA antigens, suggesting the possible role of nHLA antigens. Importantly, the AMR group is enriched in race mismatch while NoRej is enriched in relatedness. In all the analysis performed here, these differences have been considered, showing that our findings are independent for both, race mismatch and relatedness.

Further analysis of the 94 variants significantly associated with an increased risk of post-tx AMR located in 72 unique genes are enriched in immune-related function, supporting their role in the rejection process; in addition, these variants also map to genes that are more likely to be expressed on the cell-surface, suggesting that changes in the expression/function of these genes are more likely to be recognized by the recipients’ immune system, and support the possible generation of antibody responses to nHLA targets. These results are supported by a previous study (15), where a cohort with a small number of acute rejections was used to generate an allogenomic mismatch score that associated with transmembrane proteins predicted long-term graft function in kidney transplantation. In this study, we examine a larger number of acute rejections and also stratify risk further by considering both types of acute rejection, AMR and CMR.

We also observe here that specific nHLA variant mismatches impact the development of CMR, as the remaining 25 variants associated exclusively with post-tx CMR. These 25 variants map to 22 unique genes and are highly enriched in immune-related function involving CD4+ T-cells and CD8+ T-cells. This study, thus, also highlights the existence of key intrinsic differences between the triggers and mechanisms of injury in AMR and CMR.

The genes associated with rejection in this study are biologically relevant; specifically those that also have multiple associated variants (AP3D1, CDC123, CDYL2, CSMD3, FAM129B, MUC3A, MYOM2, OR51F1, OR8G1, OR8G5, PNPLA6, PSEN2, RASA3, ZNF280D, AIM1L, CHRNA10 and KIAA1755 and SLC-family). 15 out of 18 of these genes associate with risk of post-transplant AMR, and the majority (74%) of non-synonymous variants are located in these genes and in three other genes that associate with the risk of post-tx CMR (AIM1L, CHRNA10, and KIAA1755). These variants are likely to be biologically significant for their impact on post-tx rejection. In the follow-up studies we plan to evaluate post-tx nHLA antibody responses to identify the proteins, as we hypothesize that non-synonymous mutated variants produce mutated D/R mRNAs and proteins that can trigger allospecific antibody responses in the recipient and drive AMR. In addition, biological relevance in the context of AMR can be ascribed to many of the identified variants as eQTLs (DNA sequence variants that can influence the expression level of one or more genes) that are significantly enriched in blood vessels and kidney, the target organs of injury in AMR. We observed many hotspots in the endothelial eQTLs where more than one variant is related to one gene and vice versa. For example, the two variants (rs2251409 and rs2243558) located in the FAM129B gene are associated with three different genes (SLC2A8, ZNF79, and RPL12) enriched in the vascular tissues. On the other hand, other genes are associated with multiple variants, e.g., AP3D1 is associated with five different variants located in the same gene. These eQTLs may be immunologically relevant as they can influence the mRNA and protein expression of multiple genes differentially between donor and recipient pairs and contribute to AMR. Interestingly, all identified eQTLs belong only to variants associated with risk of AMR (and not with risk of CMR) highlighting the importance of detecting these variant differences in D/R pairs prior to engraftment as a means of risk stratification for risk of developing post-tx AMR. The eQTL analysis provide us with an important tool to ascribe relevant functional associations to genes for understanding the process of AMR in organ tx. For example, we highlight that we observe variants in many olfactory transduction factor genes, OR51F1, OR8G1, and OR8G5, which on initial review, should have no impact on kidney tx outcomes, but more in depth analysis reveals that all these variants also map to an eQTL in blood vessels for a common gene, VWA5A (von Willebrand factor A domain-containing protein 5A), which has been shown in a recent study (46), to cause variations in the levels of circulating VWF protein and significantly impact survival after organ tx. Thus, functionally relevant variant differences between donor and recipient may not just relate to mismatched variants in specific genes between the pairs, but may also relate to other downstream genes that these variants may modify.

Though a limitation of this pilot study is the small sample size, we highlight that our study still provides for robust discovery as it benefits from stringent clinical selection criteria for patient selection in each cohort, uses biopsy-confirmed diagnosis for each patient and has extensive statistical data modeling that limits the rate of false positive results. An important analytical caveat in statistical genetics is to control for false positives results, without being too restrictive so as to lose valuable information (false negatives results). Fisher’s exact test is a classic test that does not have enough power to deal with a large number of variables when the sample size is very small and is not ideal for a multi-class problem. To provide for stringent statistical analysis on small sample numbers with many data points, our approach of RF applies a multivariate model (all variables are introduced in the model at the same time) avoiding the correction for MT. The application of VSURF, a strategy that uses the OOB error estimate and the variable importance measures from RF to build an algorithm that performs a variable selection method for each clinical endpoints (AMR, CMR, and NoRej), detects 65 variants, a subset of the 123 variants found with the Fisher’s exact test, that classify all AMR, CMR, and NoRej samples perfectly in regards to patient outcomes after tx. To make our analysis even more stringent, we show that even though RF does not need to correct for MT since it is built using several subsets of random variables, permutation testing further validates our results, as the average OOB error rate from the permuted datasets was 25%, which is quite large in comparison to the one from the discovery set (OOB error = 3%). Thus, a combination of the statistical approaches gives us high confidence in our conclusions that patients who develop AMR after tx have the highest rate of mismatched D/R variants that can be detected before tx. Patients who develop CMR after tx have some shared variants with patients who develop post-tx AMR, which may highlight overlapping mechanisms in both kinds of injury, as previously described (47). Patients who develop CMR post-tx also have unique variants that relate primarily to gene function in CD4/CD8+ T cells, the prime cellular player in CMR. The NoRej group is well classified because these patients mostly lack any of the mismatches in the variants in the rejection groups. We recognize that additional independent validation of some of the mutations in the most significant variants is needed.

In conclusion, we have identified a finite and novel set of D/R specific mismatched variants that associate with high risk of rejection after tx and can discern between different histological and prognostic groups of either AMR or CMR after tx. We believe that these variants are functionally relevant as they relate to genes and/or eQTLs that control one or multiple genes that are enriched in the kidney (the organ undergoing injury), are involved in immune function and more likely to be displayed on the surface of the kidney cells, where they can trigger a destructive immune response in the recipient. The current sequencing and custom analytical methodologies can catalog HLA as well as, hitherto undiscovered, nHLA genetic differences between the donor and recipient before tx, that impact clinical outcomes after tx. This critical information can be obtained prior to tx surgery to select an optimal donor when more than one donor is being considered, or to assess post-tx rejection risk of AMR and CMR and personalize induction and maintenance immunosuppression to mitigate immune risk. Preventing rejection, specifically AMR, by optimizing donor selection, would have a significant positive on improving long-term tx outcomes. We believe that the inclusion of a minimal nHLA variant list should be added to current HLA testing to enhance our ability to predict AMR risk, and will fit an unmet clinical need for comprehensive prediction of tx immune risk prior to organ engraftment, opening the door to precision immunosuppression and extended tx survival.",https://www-frontiersin-org.myaccess.library.utoronto.ca/articles/10.3389/fimmu.2017.01687/full
Khan & Tutun 2021,Understanding and Predicting Organ Donation Outcomes Using Network-based Predictive Analytics,"There is an ever-increasing disparity between the number of organs needed for transplantation and the number available for donation. As a result, thousands of people die every year while waiting for an organ transplant. Therefore, it is now more critical than ever to study the factors associated with organ donation. A better understanding of such factors will help immeasurably in formulating data-driven strategies for improving familial consent for organ donation. This research combines machine learning methods and network science to accurately predict organ donation consent outcomes. In this study, six years of patient data from an organ procurement organization (OPO) in New York City were obtained and used to propose the consent prediction model. A comparison of the various prediction models was also conducted. OPOs can now use the best models to develop strategies for optimizing the consent rate, thereby saving more lives. The experimental results show that our approach outperformed in terms of detection because we combined network and machine learning algorithms to obtain clearer insights. The proposed approach can be used as an expert system to increase the organ donation consent rate, thereby bridging the gap between organ demand and supply.","Detecting organ donation outcomes by combining networks and predictive models is critical to saving the lives of
individuals on the organ transplant waitlist. Thus, it is very significant to identify important organ donation factors
that correlate directly with family consent. The proposed predictive model was built to predict if a family will be
willing to give consent, provided all the related factors. Understanding the different factors related to organ donation
will help OPO and hospital staff develop better ways of approaching family members to obtain consent for donation,
focussing more time on important factors that actually yield better outcomes. Therefore, this model can be used as an
expert system to increase the consent rate for organ donation, thereby bridging the gap between organ demand and
supply. This work also compared the various prediction models available. OPOs can now use the best models to
develop strategies for optimizing the consent rate, thereby saving more lives. In future work, network science and
machine learning could be combined to build an even more robust model to predict organ donation outcomes. Based
on these networks, interactions between the various factors affecting donation could then be explored and the bridge
events and hubs most useful in developing new strategies defined. ",https://www.sciencedirect.com/science/article/pii/S1877050921011017?ref=pdf_download&fr=RR-7&rr=82dcb633cdce5497